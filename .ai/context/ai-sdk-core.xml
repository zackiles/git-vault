<repomix>This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, content has been formatted for parsing in xml style, content has been compressed (code blocks are separated by ⋮---- delimiter).<file_summary>This section contains a summary of this file.<purpose>This file contains a packed representation of the entire repository&apos;s contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.</purpose><file_format>The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file</file_format><usage_guidelines>- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.</usage_guidelines><notes>- Some files may have been excluded based on .gitignore rules and Repomix&apos;s configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: packages/ai/core/**/*.ts
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Content has been formatted for parsing in xml style
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)</notes><additional_info></additional_info></file_summary><directory_structure>packages/
  ai/
    core/
      data-stream/
        create-data-stream-response.test.ts
        create-data-stream-response.ts
        create-data-stream.test.ts
        create-data-stream.ts
        data-stream-writer.ts
        index.ts
        pipe-data-stream-to-response.test.ts
        pipe-data-stream-to-response.ts
      embed/
        embed-many-result.ts
        embed-many.test.ts
        embed-many.ts
        embed-result.ts
        embed.test.ts
        embed.ts
        index.ts
      generate-image/
        generate-image-result.ts
        generate-image.test.ts
        generate-image.ts
        index.ts
      generate-object/
        generate-object-result.ts
        generate-object.test.ts
        generate-object.ts
        index.ts
        inject-json-instruction.test.ts
        inject-json-instruction.ts
        output-strategy.ts
        stream-object-result.ts
        stream-object.test.ts
        stream-object.ts
        validate-object-generation-input.ts
      generate-speech/
        generate-speech-result.ts
        generate-speech.test.ts
        generate-speech.ts
        generated-audio-file.ts
        index.ts
      generate-text/
        generate-text-result.ts
        generate-text.test.ts
        generate-text.ts
        generated-file.ts
        index.ts
        output.test.ts
        output.ts
        parse-tool-call.test.ts
        parse-tool-call.ts
        reasoning-detail.ts
        run-tools-transformation.test.ts
        run-tools-transformation.ts
        smooth-stream.test.ts
        smooth-stream.ts
        step-result.ts
        stream-text-result.ts
        stream-text.test.ts
        stream-text.ts
        to-response-messages.test.ts
        to-response-messages.ts
        tool-call-repair.ts
        tool-call.ts
        tool-result.ts
        tool-set.ts
      middleware/
        default-settings-middleware.test.ts
        default-settings-middleware.ts
        extract-reasoning-middleware.test.ts
        extract-reasoning-middleware.ts
        index.ts
        language-model-v1-middleware.ts
        simulate-streaming-middleware.test.ts
        simulate-streaming-middleware.ts
        wrap-language-model.test.ts
        wrap-language-model.ts
      prompt/
        append-client-message.test.ts
        append-client-message.ts
        append-response-messages.test.ts
        append-response-messages.ts
        attachments-to-parts.ts
        call-settings.ts
        content-part.ts
        convert-to-core-messages.test.ts
        convert-to-core-messages.ts
        convert-to-language-model-prompt.test.ts
        convert-to-language-model-prompt.ts
        data-content.test.ts
        data-content.ts
        index.ts
        invalid-data-content-error.ts
        invalid-message-role-error.ts
        message-conversion-error.ts
        message.ts
        prepare-call-settings.test.ts
        prepare-call-settings.ts
        prepare-retries.test.ts
        prepare-retries.ts
        prepare-tools-and-tool-choice.test.ts
        prepare-tools-and-tool-choice.ts
        prompt.ts
        split-data-url.ts
        standardize-prompt.test.ts
        standardize-prompt.ts
        stringify-for-telemetry.test.ts
        stringify-for-telemetry.ts
        tool-result-content.ts
      registry/
        custom-provider.test.ts
        custom-provider.ts
        index.ts
        no-such-provider-error.ts
        provider-registry.test.ts
        provider-registry.ts
      telemetry/
        assemble-operation-name.ts
        get-base-telemetry-attributes.ts
        get-tracer.ts
        noop-tracer.ts
        record-span.ts
        select-telemetry-attributes.ts
        select-temetry-attributes.test.ts
        telemetry-settings.ts
      test/
        mock-embedding-model-v1.ts
        mock-image-model-v1.ts
        mock-language-model-v1.ts
        mock-server-response.ts
        mock-speech-model-v1.ts
        mock-tracer.ts
        mock-transcription-model-v1.ts
        mock-values.ts
        not-implemented.ts
      tool/
        mcp/
          json-rpc-message.ts
          mcp-client.test.ts
          mcp-client.ts
          mcp-sse-transport.test.ts
          mcp-sse-transport.ts
          mcp-transport.ts
          mock-mcp-transport.ts
          types.ts
        index.ts
        tool.ts
      transcribe/
        index.ts
        transcribe-result.ts
        transcribe.test.ts
        transcribe.ts
      types/
        embedding-model.ts
        image-model-response-metadata.ts
        image-model.ts
        index.ts
        json-value.ts
        language-model-request-metadata.ts
        language-model-response-metadata.ts
        language-model.ts
        provider-metadata.ts
        provider.ts
        speech-model-response-metadata.ts
        speech-model.ts
        transcription-model-response-metadata.ts
        transcription-model.ts
        usage.ts
      util/
        async-iterable-stream.test.ts
        async-iterable-stream.ts
        cosine-similarity.test.ts
        cosine-similarity.ts
        create-stitchable-stream.test.ts
        create-stitchable-stream.ts
        detect-mimetype.test.ts
        detect-mimetype.ts
        get-potential-start-index.test.ts
        get-potential-start-index.ts
        is-non-empty-object.ts
        merge-objects.test.ts
        merge-objects.ts
        merge-streams.test.ts
        merge-streams.ts
        now.ts
        prepare-outgoing-http-headers.test.ts
        prepare-outgoing-http-headers.ts
        prepare-response-headers.test.ts
        prepare-response-headers.ts
        remove-text-after-last-whitespace.test.ts
        remove-text-after-last-whitespace.ts
        simulate-readable-stream.test.ts
        simulate-readable-stream.ts
        split-array.test.ts
        split-array.ts
        split-on-last-whitespace.test.ts
        split-on-last-whitespace.ts
        value-of.ts
        write-to-server-response.ts
      index.ts</directory_structure><files>This section contains the contents of the repository&apos;s files.<file path="packages/ai/core/data-stream/create-data-stream-response.test.ts">import { expect, it, describe } from &apos;vitest&apos;;
import { createDataStreamResponse } from &apos;./create-data-stream-response&apos;;
import { convertReadableStreamToArray } from &apos;@ai-sdk/provider-utils/test&apos;;
import { formatDataStreamPart } from &apos;@ai-sdk/ui-utils&apos;;
⋮----
// Verify response properties
⋮----
// Verify headers
⋮----
// Verify encoded stream content</file><file path="packages/ai/core/data-stream/create-data-stream-response.ts">import { prepareResponseHeaders } from &apos;../util/prepare-response-headers&apos;;
import { createDataStream } from &apos;./create-data-stream&apos;;
import { DataStreamWriter } from &apos;./data-stream-writer&apos;;
export function createDataStreamResponse({
  status,
  statusText,
  headers,
  execute,
  onError,
}: ResponseInit &amp; {
execute: (dataStream: DataStreamWriter)</file><file path="packages/ai/core/data-stream/create-data-stream.test.ts">import { delay } from &apos;@ai-sdk/provider-utils&apos;;
import { convertReadableStreamToArray } from &apos;@ai-sdk/provider-utils/test&apos;;
import { formatDataStreamPart } from &apos;@ai-sdk/ui-utils&apos;;
import { expect, it } from &apos;vitest&apos;;
import { DelayedPromise } from &apos;../../util/delayed-promise&apos;;
import { Source } from &apos;../types/language-model&apos;;
import { createDataStream } from &apos;./create-data-stream&apos;;
import { DataStreamWriter } from &apos;./data-stream-writer&apos;;
⋮----
start(controller)
⋮----
start(controllerArg)
⋮----
async function pull()
// function is finished
⋮----
// controller1 is still open, create 2nd stream
⋮----
// close controller1
⋮----
await delay(); // relinquish control
// it should still be able to write to controller2</file><file path="packages/ai/core/data-stream/create-data-stream.ts">import { DataStreamString, formatDataStreamPart } from &apos;@ai-sdk/ui-utils&apos;;
import { DataStreamWriter } from &apos;./data-stream-writer&apos;;
export function createDataStream({
  execute,
  onError = () =&gt; &apos;An error occurred.&apos;, // mask error messages for safety by default
}: {
execute: (dataStream: DataStreamWriter)
⋮----
onError = () =&gt; &apos;An error occurred.&apos;, // mask error messages for safety by default
⋮----
start(controllerArg)
⋮----
function safeEnqueue(data: DataStreamString)
⋮----
// suppress errors when the stream has been closed
⋮----
write(data: DataStreamString)
writeData(data)
writeMessageAnnotation(annotation)
writeSource(source)
merge(streamArg)
⋮----
// Wait until all ongoing streams are done. This approach enables merging
// streams even after execute has returned, as long as there is still an
// open merged stream. This is important to e.g. forward new streams and
// from callbacks.
⋮----
// suppress errors when the stream has been closed</file><file path="packages/ai/core/data-stream/data-stream-writer.ts">import { JSONValue } from &apos;@ai-sdk/provider&apos;;
import { DataStreamString } from &apos;@ai-sdk/ui-utils&apos;;
import { Source } from &apos;../types/language-model&apos;;
export interface DataStreamWriter {
  /**
   * Appends a data part to the stream.
   */
  write(data: DataStreamString): void;
  /**
   * Appends a data part to the stream.
   */
  writeData(value: JSONValue): void;
  /**
   * Appends a message annotation to the stream.
   */
  writeMessageAnnotation(value: JSONValue): void;
  /**
   * Appends a source part to the stream.
   */
  writeSource(source: Source): void;
  /**
   * Merges the contents of another stream to this stream.
   */
  merge(stream: ReadableStream&lt;DataStreamString&gt;): void;
  /**
   * Error handler that is used by the data stream writer.
   * This is intended for forwarding when merging streams
   * to prevent duplicated error masking.
   */
  onError: ((error: unknown) =&gt; string) | undefined;
}
⋮----
/**
   * Appends a data part to the stream.
   */
write(data: DataStreamString): void;
/**
   * Appends a data part to the stream.
   */
writeData(value: JSONValue): void;
/**
   * Appends a message annotation to the stream.
   */
writeMessageAnnotation(value: JSONValue): void;
/**
   * Appends a source part to the stream.
   */
writeSource(source: Source): void;
/**
   * Merges the contents of another stream to this stream.
   */
merge(stream: ReadableStream&lt;DataStreamString&gt;): void;
/**
   * Error handler that is used by the data stream writer.
   * This is intended for forwarding when merging streams
   * to prevent duplicated error masking.
   */</file><file path="packages/ai/core/data-stream/index.ts"></file><file path="packages/ai/core/data-stream/pipe-data-stream-to-response.test.ts">import { expect, it, describe } from &apos;vitest&apos;;
import { pipeDataStreamToResponse } from &apos;./pipe-data-stream-to-response&apos;;
import { formatDataStreamPart } from &apos;@ai-sdk/ui-utils&apos;;
import { createMockServerResponse } from &apos;../test/mock-server-response&apos;;
⋮----
// Wait for the stream to finish writing
⋮----
// Verify response properties
⋮----
// Verify headers
⋮----
// Verify written data using decoded chunks
⋮----
// Wait for the stream to finish writing
⋮----
// Verify error handling using decoded chunks</file><file path="packages/ai/core/data-stream/pipe-data-stream-to-response.ts">import { ServerResponse } from &apos;node:http&apos;;
import { prepareOutgoingHttpHeaders } from &apos;../util/prepare-outgoing-http-headers&apos;;
import { writeToServerResponse } from &apos;../util/write-to-server-response&apos;;
import { createDataStream } from &apos;./create-data-stream&apos;;
import { DataStreamWriter } from &apos;./data-stream-writer&apos;;
export function pipeDataStreamToResponse(
  response: ServerResponse,
  {
    status,
    statusText,
    headers,
    execute,
    onError,
  }: ResponseInit &amp; {
execute: (writer: DataStreamWriter)</file><file path="packages/ai/core/embed/embed-many-result.ts">import { Embedding } from &apos;../types&apos;;
import { EmbeddingModelUsage } from &apos;../types/usage&apos;;
/**
The result of a `embedMany` call.
It contains the embeddings, the values, and additional information.
 */
export interface EmbedManyResult&lt;VALUE&gt; {
  /**
  The values that were embedded.
     */
  readonly values: Array&lt;VALUE&gt;;
  /**
  The embeddings. They are in the same order as the values.
    */
  readonly embeddings: Array&lt;Embedding&gt;;
  /**
  The embedding token usage.
    */
  readonly usage: EmbeddingModelUsage;
}
⋮----
/**
  The values that were embedded.
     */
⋮----
/**
  The embeddings. They are in the same order as the values.
    */
⋮----
/**
  The embedding token usage.
    */</file><file path="packages/ai/core/embed/embed-many.test.ts">import assert from &apos;node:assert&apos;;
import {
  MockEmbeddingModelV1,
  mockEmbed,
} from &apos;../test/mock-embedding-model-v1&apos;;
import { MockTracer } from &apos;../test/mock-tracer&apos;;
import { embedMany } from &apos;./embed-many&apos;;</file><file path="packages/ai/core/embed/embed-many.ts">import { prepareRetries } from &apos;../prompt/prepare-retries&apos;;
import { assembleOperationName } from &apos;../telemetry/assemble-operation-name&apos;;
import { getBaseTelemetryAttributes } from &apos;../telemetry/get-base-telemetry-attributes&apos;;
import { getTracer } from &apos;../telemetry/get-tracer&apos;;
import { recordSpan } from &apos;../telemetry/record-span&apos;;
import { selectTelemetryAttributes } from &apos;../telemetry/select-telemetry-attributes&apos;;
import { TelemetrySettings } from &apos;../telemetry/telemetry-settings&apos;;
import { Embedding, EmbeddingModel } from &apos;../types&apos;;
import { splitArray } from &apos;../util/split-array&apos;;
import { EmbedManyResult } from &apos;./embed-many-result&apos;;
/**
Embed several values using an embedding model. The type of the value is defined
by the embedding model.
`embedMany` automatically splits large requests into smaller chunks if the model
has a limit on how many embeddings can be generated in a single call.
@param model - The embedding model to use.
@param values - The values that should be embedded.
@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.
@param abortSignal - An optional abort signal that can be used to cancel the call.
@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.
@returns A result object that contains the embeddings, the value, and additional information.
 */
export async function embedMany&lt;VALUE&gt;({
  model,
  values,
  maxRetries: maxRetriesArg,
  abortSignal,
  headers,
  experimental_telemetry: telemetry,
}: {
  /**
The embedding model to use.
     */
  model: EmbeddingModel&lt;VALUE&gt;;
  /**
The values that should be embedded.
   */
  values: Array&lt;VALUE&gt;;
  /**
Maximum number of retries per embedding model call. Set to 0 to disable retries.
@default 2
   */
  maxRetries?: number;
  /**
Abort signal.
 */
  abortSignal?: AbortSignal;
  /**
Additional headers to include in the request.
Only applicable for HTTP-based providers.
 */
  headers?: Record&lt;string, string&gt;;
  /**
   * Optional telemetry configuration (experimental).
   */
  experimental_telemetry?: TelemetrySettings;
}): Promise&lt;EmbedManyResult&lt;VALUE&gt;&gt;
⋮----
/**
The embedding model to use.
     */
⋮----
/**
The values that should be embedded.
   */
⋮----
/**
Maximum number of retries per embedding model call. Set to 0 to disable retries.
@default 2
   */
⋮----
/**
Abort signal.
 */
⋮----
/**
Additional headers to include in the request.
Only applicable for HTTP-based providers.
 */
⋮----
/**
   * Optional telemetry configuration (experimental).
   */
⋮----
// specific settings that only make sense on the outer level:
⋮----
// the model has not specified limits on
// how many embeddings can be generated in a single call
⋮----
// nested spans to align with the embedMany telemetry data:
⋮----
// specific settings that only make sense on the outer level:
⋮----
// split the values into chunks that are small enough for the model:
⋮----
// serially embed the chunks:
⋮----
// nested spans to align with the embedMany telemetry data:
⋮----
// specific settings that only make sense on the outer level:
⋮----
class DefaultEmbedManyResult&lt;VALUE&gt; implements EmbedManyResult&lt;VALUE&gt;
⋮----
constructor(options: {
    values: EmbedManyResult&lt;VALUE&gt;[&apos;values&apos;];
    embeddings: EmbedManyResult&lt;VALUE&gt;[&apos;embeddings&apos;];
    usage: EmbedManyResult&lt;VALUE&gt;[&apos;usage&apos;];
})</file><file path="packages/ai/core/embed/embed-result.ts">import { Embedding } from &apos;../types&apos;;
import { EmbeddingModelUsage } from &apos;../types/usage&apos;;
/**
The result of an `embed` call.
It contains the embedding, the value, and additional information.
 */
export interface EmbedResult&lt;VALUE&gt; {
  /**
  The value that was embedded.
     */
  readonly value: VALUE;
  /**
  The embedding of the value.
    */
  readonly embedding: Embedding;
  /**
  The embedding token usage.
    */
  readonly usage: EmbeddingModelUsage;
  /**
  Optional raw response data.
     */
  readonly rawResponse?: {
    /**
  Response headers.
       */
    headers?: Record&lt;string, string&gt;;
  };
}
⋮----
/**
  The value that was embedded.
     */
⋮----
/**
  The embedding of the value.
    */
⋮----
/**
  The embedding token usage.
    */
⋮----
/**
  Optional raw response data.
     */
⋮----
/**
  Response headers.
       */</file><file path="packages/ai/core/embed/embed.test.ts">import assert from &apos;node:assert&apos;;
import {
  MockEmbeddingModelV1,
  mockEmbed,
} from &apos;../test/mock-embedding-model-v1&apos;;
import { MockTracer } from &apos;../test/mock-tracer&apos;;
import { embed } from &apos;./embed&apos;;</file><file path="packages/ai/core/embed/embed.ts">import { prepareRetries } from &apos;../prompt/prepare-retries&apos;;
import { assembleOperationName } from &apos;../telemetry/assemble-operation-name&apos;;
import { getBaseTelemetryAttributes } from &apos;../telemetry/get-base-telemetry-attributes&apos;;
import { getTracer } from &apos;../telemetry/get-tracer&apos;;
import { recordSpan } from &apos;../telemetry/record-span&apos;;
import { selectTelemetryAttributes } from &apos;../telemetry/select-telemetry-attributes&apos;;
import { TelemetrySettings } from &apos;../telemetry/telemetry-settings&apos;;
import { EmbeddingModel } from &apos;../types&apos;;
import { EmbedResult } from &apos;./embed-result&apos;;
/**
Embed a value using an embedding model. The type of the value is defined by the embedding model.
@param model - The embedding model to use.
@param value - The value that should be embedded.
@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.
@param abortSignal - An optional abort signal that can be used to cancel the call.
@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.
@returns A result object that contains the embedding, the value, and additional information.
 */
export async function embed&lt;VALUE&gt;({
  model,
  value,
  maxRetries: maxRetriesArg,
  abortSignal,
  headers,
  experimental_telemetry: telemetry,
}: {
  /**
The embedding model to use.
     */
  model: EmbeddingModel&lt;VALUE&gt;;
  /**
The value that should be embedded.
   */
  value: VALUE;
  /**
Maximum number of retries per embedding model call. Set to 0 to disable retries.
@default 2
   */
  maxRetries?: number;
  /**
Abort signal.
 */
  abortSignal?: AbortSignal;
  /**
Additional headers to include in the request.
Only applicable for HTTP-based providers.
 */
  headers?: Record&lt;string, string&gt;;
  /**
   * Optional telemetry configuration (experimental).
   */
  experimental_telemetry?: TelemetrySettings;
}): Promise&lt;EmbedResult&lt;VALUE&gt;&gt;
⋮----
/**
The embedding model to use.
     */
⋮----
/**
The value that should be embedded.
   */
⋮----
/**
Maximum number of retries per embedding model call. Set to 0 to disable retries.
@default 2
   */
⋮----
/**
Abort signal.
 */
⋮----
/**
Additional headers to include in the request.
Only applicable for HTTP-based providers.
 */
⋮----
/**
   * Optional telemetry configuration (experimental).
   */
⋮----
// nested spans to align with the embedMany telemetry data:
⋮----
// specific settings that only make sense on the outer level:
⋮----
class DefaultEmbedResult&lt;VALUE&gt; implements EmbedResult&lt;VALUE&gt;
⋮----
constructor(options: {
    value: EmbedResult&lt;VALUE&gt;[&apos;value&apos;];
    embedding: EmbedResult&lt;VALUE&gt;[&apos;embedding&apos;];
    usage: EmbedResult&lt;VALUE&gt;[&apos;usage&apos;];
    rawResponse?: EmbedResult&lt;VALUE&gt;[&apos;rawResponse&apos;];
})</file><file path="packages/ai/core/embed/index.ts"></file><file path="packages/ai/core/generate-image/generate-image-result.ts">import { GeneratedFile } from &apos;../generate-text&apos;;
import { ImageGenerationWarning } from &apos;../types/image-model&apos;;
import { ImageModelResponseMetadata } from &apos;../types/image-model-response-metadata&apos;;
/**
The result of a `generateImage` call.
It contains the images and additional information.
 */
export interface GenerateImageResult {
  /**
The first image that was generated.
   */
  readonly image: GeneratedFile;
  /**
The images that were generated.
     */
  readonly images: Array&lt;GeneratedFile&gt;;
  /**
Warnings for the call, e.g. unsupported settings.
     */
  readonly warnings: Array&lt;ImageGenerationWarning&gt;;
  /**
Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.
   */
  readonly responses: Array&lt;ImageModelResponseMetadata&gt;;
}
⋮----
/**
The first image that was generated.
   */
⋮----
/**
The images that were generated.
     */
⋮----
/**
Warnings for the call, e.g. unsupported settings.
     */
⋮----
/**
Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.
   */</file><file path="packages/ai/core/generate-image/generate-image.test.ts">import { ImageModelV1, ImageModelV1CallWarning } from &apos;@ai-sdk/provider&apos;;
import { MockImageModelV1 } from &apos;../test/mock-image-model-v1&apos;;
import { generateImage } from &apos;./generate-image&apos;;
import {
  convertBase64ToUint8Array,
  convertUint8ArrayToBase64,
} from &apos;@ai-sdk/provider-utils&apos;;
⋮----
&apos;iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAACklEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==&apos;; // 1x1 transparent PNG
⋮----
&apos;/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAABAAEDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAb/xAAUEAEAAAAAAAAAAAAAAAAAAAAA/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAX/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwCdABmX/9k=&apos;; // 1x1 black JPEG
const gifBase64 = &apos;R0lGODlhAQABAIAAAAUEBAAAACwAAAAAAQABAAACAkQBADs=&apos;; // 1x1 transparent GIF
const createMockResponse = (options: {
  images: string[] | Uint8Array[];
  warnings?: ImageModelV1CallWarning[];
  timestamp?: Date;
  modelId?: string;
  headers?: Record&lt;string, string&gt;;
}) =&gt; (</file><file path="packages/ai/core/generate-image/generate-image.ts">import { AISDKError, ImageModelV1, JSONValue } from &apos;@ai-sdk/provider&apos;;
import { NoImageGeneratedError } from &apos;../../errors/no-image-generated-error&apos;;
import {
  DefaultGeneratedFile,
  GeneratedFile,
} from &apos;../generate-text/generated-file&apos;;
import { prepareRetries } from &apos;../prompt/prepare-retries&apos;;
import { ImageGenerationWarning } from &apos;../types/image-model&apos;;
import { ImageModelResponseMetadata } from &apos;../types/image-model-response-metadata&apos;;
import { GenerateImageResult } from &apos;./generate-image-result&apos;;
import {
  detectMimeType,
  imageMimeTypeSignatures,
} from &apos;../util/detect-mimetype&apos;;
/**
Generates images using an image model.
@param model - The image model to use.
@param prompt - The prompt that should be used to generate the image.
@param n - Number of images to generate. Default: 1.
@param size - Size of the images to generate. Must have the format `{width}x{height}`.
@param aspectRatio - Aspect ratio of the images to generate. Must have the format `{width}:{height}`.
@param seed - Seed for the image generation.
@param providerOptions - Additional provider-specific options that are passed through to the provider
as body parameters.
@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.
@param abortSignal - An optional abort signal that can be used to cancel the call.
@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.
@returns A result object that contains the generated images.
 */
export async function generateImage({
  model,
  prompt,
  n = 1,
  size,
  aspectRatio,
  seed,
  providerOptions,
  maxRetries: maxRetriesArg,
  abortSignal,
  headers,
}: {
  /**
The image model to use.
     */
  model: ImageModelV1;
  /**
The prompt that should be used to generate the image.
   */
  prompt: string;
  /**
Number of images to generate.
   */
  n?: number;
  /**
Size of the images to generate. Must have the format `{width}x{height}`. If not provided, the default size will be used.
   */
  size?: `${number}x${number}`;
  /**
Aspect ratio of the images to generate. Must have the format `{width}:{height}`. If not provided, the default aspect ratio will be used.
   */
  aspectRatio?: `${number}:${number}`;
  /**
Seed for the image generation. If not provided, the default seed will be used.
   */
  seed?: number;
  /**
Additional provider-specific options that are passed through to the provider
as body parameters.
The outer record is keyed by the provider name, and the inner
record is keyed by the provider-specific metadata key.
```ts
{
  &quot;openai&quot;: {
    &quot;style&quot;: &quot;vivid&quot;
  }
}
```
     */
  providerOptions?: Record&lt;string, Record&lt;string, JSONValue&gt;&gt;;
  /**
Maximum number of retries per embedding model call. Set to 0 to disable retries.
@default 2
   */
  maxRetries?: number;
  /**
Abort signal.
 */
  abortSignal?: AbortSignal;
  /**
Additional headers to include in the request.
Only applicable for HTTP-based providers.
 */
  headers?: Record&lt;string, string&gt;;
}): Promise&lt;GenerateImageResult&gt;
⋮----
/**
The image model to use.
     */
⋮----
/**
The prompt that should be used to generate the image.
   */
⋮----
/**
Number of images to generate.
   */
⋮----
/**
Size of the images to generate. Must have the format `{width}x{height}`. If not provided, the default size will be used.
   */
⋮----
/**
Aspect ratio of the images to generate. Must have the format `{width}:{height}`. If not provided, the default aspect ratio will be used.
   */
⋮----
/**
Seed for the image generation. If not provided, the default seed will be used.
   */
⋮----
/**
Additional provider-specific options that are passed through to the provider
as body parameters.
The outer record is keyed by the provider name, and the inner
record is keyed by the provider-specific metadata key.
```ts
{
  &quot;openai&quot;: {
    &quot;style&quot;: &quot;vivid&quot;
  }
}
```
     */
⋮----
/**
Maximum number of retries per embedding model call. Set to 0 to disable retries.
@default 2
   */
⋮----
/**
Abort signal.
 */
⋮----
/**
Additional headers to include in the request.
Only applicable for HTTP-based providers.
 */
⋮----
// default to 1 if the model has not specified limits on
// how many images can be generated in a single call
⋮----
// parallelize calls to the model:
⋮----
// collect result images, warnings, and response metadata
⋮----
class DefaultGenerateImageResult implements GenerateImageResult
⋮----
constructor(options: {
    images: Array&lt;GeneratedFile&gt;;
    warnings: Array&lt;ImageGenerationWarning&gt;;
    responses: Array&lt;ImageModelResponseMetadata&gt;;
})
get image()</file><file path="packages/ai/core/generate-image/index.ts"></file><file path="packages/ai/core/generate-object/generate-object-result.ts">import {
  CallWarning,
  FinishReason,
  LanguageModelRequestMetadata,
  LanguageModelResponseMetadata,
  LogProbs,
  ProviderMetadata,
} from &apos;../types&apos;;
import { LanguageModelUsage } from &apos;../types/usage&apos;;
/**
The result of a `generateObject` call.
 */
export interface GenerateObjectResult&lt;OBJECT&gt; {
  /**
  The generated object (typed according to the schema).
     */
  readonly object: OBJECT;
  /**
  The reason why the generation finished.
     */
  readonly finishReason: FinishReason;
  /**
  The token usage of the generated text.
     */
  readonly usage: LanguageModelUsage;
  /**
  Warnings from the model provider (e.g. unsupported settings).
     */
  readonly warnings: CallWarning[] | undefined;
  /**
Additional request information.
   */
  readonly request: LanguageModelRequestMetadata;
  /**
Additional response information.
   */
  readonly response: LanguageModelResponseMetadata &amp; {
    /**
Response body (available only for providers that use HTTP requests).
    */
    body?: unknown;
  };
  /**
 Logprobs for the completion.
`undefined` if the mode does not support logprobs or if was not enabled.
@deprecated Will become a provider extension in the future.
     */
  readonly logprobs: LogProbs | undefined;
  /**
Additional provider-specific metadata. They are passed through
from the provider to the AI SDK and enable provider-specific
results that can be fully encapsulated in the provider.
   */
  readonly providerMetadata: ProviderMetadata | undefined;
  /**
@deprecated Use `providerMetadata` instead.
   */
  readonly experimental_providerMetadata: ProviderMetadata | undefined;
  /**
  Converts the object to a JSON response.
  The response will have a status code of 200 and a content type of `application/json; charset=utf-8`.
     */
  toJsonResponse(init?: ResponseInit): Response;
}
⋮----
/**
  The generated object (typed according to the schema).
     */
⋮----
/**
  The reason why the generation finished.
     */
⋮----
/**
  The token usage of the generated text.
     */
⋮----
/**
  Warnings from the model provider (e.g. unsupported settings).
     */
⋮----
/**
Additional request information.
   */
⋮----
/**
Additional response information.
   */
⋮----
/**
Response body (available only for providers that use HTTP requests).
    */
⋮----
/**
 Logprobs for the completion.
`undefined` if the mode does not support logprobs or if was not enabled.
@deprecated Will become a provider extension in the future.
     */
⋮----
/**
Additional provider-specific metadata. They are passed through
from the provider to the AI SDK and enable provider-specific
results that can be fully encapsulated in the provider.
   */
⋮----
/**
@deprecated Use `providerMetadata` instead.
   */
⋮----
/**
  Converts the object to a JSON response.
  The response will have a status code of 200 and a content type of `application/json; charset=utf-8`.
     */
toJsonResponse(init?: ResponseInit): Response;</file><file path="packages/ai/core/generate-object/generate-object.test.ts">import { convertReadableStreamToArray } from &apos;@ai-sdk/provider-utils/test&apos;;
import { jsonSchema } from &apos;@ai-sdk/ui-utils&apos;;
import assert, { fail } from &apos;node:assert&apos;;
import { z } from &apos;zod&apos;;
import { verifyNoObjectGeneratedError as originalVerifyNoObjectGeneratedError } from &apos;../../errors/no-object-generated-error&apos;;
import { MockLanguageModelV1 } from &apos;../test/mock-language-model-v1&apos;;
import { MockTracer } from &apos;../test/mock-tracer&apos;;
import { generateObject } from &apos;./generate-object&apos;;
import { JSONParseError, TypeValidationError } from &apos;@ai-sdk/provider&apos;;
⋮----
function verifyNoObjectGeneratedError(
      error: unknown,
      { message }: { message: string },
)
⋮----
class MockLanguageModelWithImageSupport extends MockLanguageModelV1
⋮----
constructor()
⋮----
supportsUrl(url: URL)
⋮----
// Reference &apos;this&apos; to verify context</file><file path="packages/ai/core/generate-object/generate-object.ts">import {
  JSONParseError,
  JSONValue,
  TypeValidationError,
} from &apos;@ai-sdk/provider&apos;;
import { createIdGenerator, safeParseJSON } from &apos;@ai-sdk/provider-utils&apos;;
import { Schema } from &apos;@ai-sdk/ui-utils&apos;;
import { z } from &apos;zod&apos;;
import { NoObjectGeneratedError } from &apos;../../errors/no-object-generated-error&apos;;
import { CallSettings } from &apos;../prompt/call-settings&apos;;
import { convertToLanguageModelPrompt } from &apos;../prompt/convert-to-language-model-prompt&apos;;
import { prepareCallSettings } from &apos;../prompt/prepare-call-settings&apos;;
import { prepareRetries } from &apos;../prompt/prepare-retries&apos;;
import { Prompt } from &apos;../prompt/prompt&apos;;
import { standardizePrompt } from &apos;../prompt/standardize-prompt&apos;;
import { assembleOperationName } from &apos;../telemetry/assemble-operation-name&apos;;
import { getBaseTelemetryAttributes } from &apos;../telemetry/get-base-telemetry-attributes&apos;;
import { getTracer } from &apos;../telemetry/get-tracer&apos;;
import { recordSpan } from &apos;../telemetry/record-span&apos;;
import { selectTelemetryAttributes } from &apos;../telemetry/select-telemetry-attributes&apos;;
import { TelemetrySettings } from &apos;../telemetry/telemetry-settings&apos;;
import {
  CallWarning,
  FinishReason,
  LanguageModel,
  LogProbs,
  ProviderMetadata,
} from &apos;../types&apos;;
import { LanguageModelRequestMetadata } from &apos;../types/language-model-request-metadata&apos;;
import { LanguageModelResponseMetadata } from &apos;../types/language-model-response-metadata&apos;;
import { ProviderOptions } from &apos;../types/provider-metadata&apos;;
import { calculateLanguageModelUsage } from &apos;../types/usage&apos;;
import { prepareResponseHeaders } from &apos;../util/prepare-response-headers&apos;;
import { GenerateObjectResult } from &apos;./generate-object-result&apos;;
import { injectJsonInstruction } from &apos;./inject-json-instruction&apos;;
import { getOutputStrategy } from &apos;./output-strategy&apos;;
import { validateObjectGenerationInput } from &apos;./validate-object-generation-input&apos;;
import { stringifyForTelemetry } from &apos;../prompt/stringify-for-telemetry&apos;;
⋮----
/**
A function that attempts to repair the raw output of the mode
to enable JSON parsing.
Should return the repaired text or null if the text cannot be repaired.
     */
export type RepairTextFunction = (options: {
  text: string;
  error: JSONParseError | TypeValidationError;
}) =&gt; Promise&lt;string | null&gt;;
/**
Generate a structured, typed object for a given prompt and schema using a language model.
This function does not stream the output. If you want to stream the output, use `streamObject` instead.
@returns
A result object that contains the generated object, the finish reason, the token usage, and additional information.
 */
export async function generateObject&lt;OBJECT&gt;(
  options: Omit&lt;CallSettings, &apos;stopSequences&apos;&gt; &amp;
    Prompt &amp; {
      output?: &apos;object&apos; | undefined;
      /**
The language model to use.
     */
      model: LanguageModel;
      /**
The schema of the object that the model should generate.
     */
      schema: z.Schema&lt;OBJECT, z.ZodTypeDef, any&gt; | Schema&lt;OBJECT&gt;;
      /**
Optional name of the output that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema name.
     */
      schemaName?: string;
      /**
Optional description of the output that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema description.
     */
      schemaDescription?: string;
      /**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
      mode?: &apos;auto&apos; | &apos;json&apos; | &apos;tool&apos;;
      /**
A function that attempts to repair the raw output of the mode
to enable JSON parsing.
     */
      experimental_repairText?: RepairTextFunction;
      /**
Optional telemetry configuration (experimental).
       */
      experimental_telemetry?: TelemetrySettings;
      /**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
      providerOptions?: ProviderOptions;
      /**
@deprecated Use `providerOptions` instead.
*/
      experimental_providerMetadata?: ProviderMetadata;
      /**
       * Internal. For test use only. May change without notice.
       */
      _internal?: {
generateId?: ()
⋮----
/**
The language model to use.
     */
⋮----
/**
The schema of the object that the model should generate.
     */
⋮----
/**
Optional name of the output that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema name.
     */
⋮----
/**
Optional description of the output that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema description.
     */
⋮----
/**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
⋮----
/**
A function that attempts to repair the raw output of the mode
to enable JSON parsing.
     */
⋮----
/**
Optional telemetry configuration (experimental).
       */
⋮----
/**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
       * Internal. For test use only. May change without notice.
       */
⋮----
/**
Generate an array with structured, typed elements for a given prompt and element schema using a language model.
This function does not stream the output. If you want to stream the output, use `streamObject` instead.
@return
A result object that contains the generated object, the finish reason, the token usage, and additional information.
 */
export async function generateObject&lt;ELEMENT&gt;(
  options: Omit&lt;CallSettings, &apos;stopSequences&apos;&gt; &amp;
    Prompt &amp; {
      output: &apos;array&apos;;
      /**
The language model to use.
     */
      model: LanguageModel;
      /**
The element schema of the array that the model should generate.
 */
      schema: z.Schema&lt;ELEMENT, z.ZodTypeDef, any&gt; | Schema&lt;ELEMENT&gt;;
      /**
Optional name of the array that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema name.
     */
      schemaName?: string;
      /**
Optional description of the array that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema description.
 */
      schemaDescription?: string;
      /**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
      mode?: &apos;auto&apos; | &apos;json&apos; | &apos;tool&apos;;
      /**
A function that attempts to repair the raw output of the mode
to enable JSON parsing.
     */
      experimental_repairText?: RepairTextFunction;
      /**
Optional telemetry configuration (experimental).
     */
      experimental_telemetry?: TelemetrySettings;
      /**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
      providerOptions?: ProviderOptions;
      /**
@deprecated Use `providerOptions` instead.
*/
      experimental_providerMetadata?: ProviderMetadata;
      /**
       * Internal. For test use only. May change without notice.
       */
      _internal?: {
generateId?: ()
⋮----
/**
The language model to use.
     */
⋮----
/**
The element schema of the array that the model should generate.
 */
⋮----
/**
Optional name of the array that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema name.
     */
⋮----
/**
Optional description of the array that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema description.
 */
⋮----
/**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
⋮----
/**
A function that attempts to repair the raw output of the mode
to enable JSON parsing.
     */
⋮----
/**
Optional telemetry configuration (experimental).
     */
⋮----
/**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
       * Internal. For test use only. May change without notice.
       */
⋮----
/**
Generate a value from an enum (limited list of string values) using a language model.
This function does not stream the output.
@return
A result object that contains the generated value, the finish reason, the token usage, and additional information.
 */
export async function generateObject&lt;ENUM extends string&gt;(
  options: Omit&lt;CallSettings, &apos;stopSequences&apos;&gt; &amp;
    Prompt &amp; {
      output: &apos;enum&apos;;
      /**
The language model to use.
     */
      model: LanguageModel;
      /**
The enum values that the model should use.
     */
      enum: Array&lt;ENUM&gt;;
      /**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
      mode?: &apos;auto&apos; | &apos;json&apos; | &apos;tool&apos;;
      /**
A function that attempts to repair the raw output of the mode
to enable JSON parsing.
     */
      experimental_repairText?: RepairTextFunction;
      /**
Optional telemetry configuration (experimental).
     */
      experimental_telemetry?: TelemetrySettings;
      /**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
      providerOptions?: ProviderOptions;
      /**
@deprecated Use `providerOptions` instead.
*/
      experimental_providerMetadata?: ProviderMetadata;
      /**
       * Internal. For test use only. May change without notice.
       */
      _internal?: {
generateId?: ()
⋮----
/**
The language model to use.
     */
⋮----
/**
The enum values that the model should use.
     */
⋮----
/**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
⋮----
/**
A function that attempts to repair the raw output of the mode
to enable JSON parsing.
     */
⋮----
/**
Optional telemetry configuration (experimental).
     */
⋮----
/**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
       * Internal. For test use only. May change without notice.
       */
⋮----
/**
Generate JSON with any schema for a given prompt using a language model.
This function does not stream the output. If you want to stream the output, use `streamObject` instead.
@returns
A result object that contains the generated object, the finish reason, the token usage, and additional information.
 */
export async function generateObject(
  options: Omit&lt;CallSettings, &apos;stopSequences&apos;&gt; &amp;
    Prompt &amp; {
      output: &apos;no-schema&apos;;
      /**
The language model to use.
     */
      model: LanguageModel;
      /**
The mode to use for object generation. Must be &quot;json&quot; for no-schema output.
     */
      mode?: &apos;json&apos;;
      /**
A function that attempts to repair the raw output of the mode
to enable JSON parsing.
     */
      experimental_repairText?: RepairTextFunction;
      /**
Optional telemetry configuration (experimental).
       */
      experimental_telemetry?: TelemetrySettings;
      /**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
      providerOptions?: ProviderOptions;
      /**
@deprecated Use `providerOptions` instead.
*/
      experimental_providerMetadata?: ProviderMetadata;
      /**
       * Internal. For test use only. May change without notice.
       */
      _internal?: {
generateId?: ()
⋮----
/**
The language model to use.
     */
⋮----
/**
The mode to use for object generation. Must be &quot;json&quot; for no-schema output.
     */
⋮----
/**
A function that attempts to repair the raw output of the mode
to enable JSON parsing.
     */
⋮----
/**
Optional telemetry configuration (experimental).
       */
⋮----
/**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
       * Internal. For test use only. May change without notice.
       */
⋮----
export async function generateObject&lt;SCHEMA, RESULT&gt;({
  model,
  enum: enumValues, // rename bc enum is reserved by typescript
  schema: inputSchema,
  schemaName,
  schemaDescription,
  mode,
  output = &apos;object&apos;,
  system,
  prompt,
  messages,
  maxRetries: maxRetriesArg,
  abortSignal,
  headers,
  experimental_repairText: repairText,
  experimental_telemetry: telemetry,
  experimental_providerMetadata,
  providerOptions = experimental_providerMetadata,
  _internal: {
    generateId = originalGenerateId,
    currentDate = () =&gt; new Date(),
  } = {},
  ...settings
}: Omit&lt;CallSettings, &apos;stopSequences&apos;&gt; &amp;
  Prompt &amp; {
    /**
     * The expected structure of the output.
     *
     * - &apos;object&apos;: Generate a single object that conforms to the schema.
     * - &apos;array&apos;: Generate an array of objects that conform to the schema.
     * - &apos;no-schema&apos;: Generate any JSON object. No schema is specified.
     *
     * Default is &apos;object&apos; if not specified.
     */
    output?: &apos;object&apos; | &apos;array&apos; | &apos;enum&apos; | &apos;no-schema&apos;;
    model: LanguageModel;
    enum?: Array&lt;SCHEMA&gt;;
    schema?: z.Schema&lt;SCHEMA, z.ZodTypeDef, any&gt; | Schema&lt;SCHEMA&gt;;
    schemaName?: string;
    schemaDescription?: string;
    mode?: &apos;auto&apos; | &apos;json&apos; | &apos;tool&apos;;
    experimental_repairText?: RepairTextFunction;
    experimental_telemetry?: TelemetrySettings;
    experimental_providerMetadata?: ProviderMetadata;
    providerOptions?: ProviderOptions;
    /**
     * Internal. For test use only. May change without notice.
     */
    _internal?: {
generateId?: ()
⋮----
enum: enumValues, // rename bc enum is reserved by typescript
⋮----
/**
     * The expected structure of the output.
     *
     * - &apos;object&apos;: Generate a single object that conforms to the schema.
     * - &apos;array&apos;: Generate an array of objects that conform to the schema.
     * - &apos;no-schema&apos;: Generate any JSON object. No schema is specified.
     *
     * Default is &apos;object&apos; if not specified.
     */
⋮----
/**
     * Internal. For test use only. May change without notice.
     */
⋮----
// automatically set mode to &apos;json&apos; for no-schema output
⋮----
// specific settings that only make sense on the outer level:
⋮----
// use the default provider mode when the mode is set to &apos;auto&apos; or unspecified
⋮----
modelSupportsUrl: model.supportsUrl?.bind(model), // support &apos;this&apos; context
⋮----
// standardized gen-ai llm span attributes:
⋮----
// Add response information to the span:
⋮----
// standardized gen-ai llm span attributes:
⋮----
modelSupportsUrl: model.supportsUrl?.bind(model), // support &apos;this&apos; context,
⋮----
// standardized gen-ai llm span attributes:
⋮----
// Add response information to the span:
⋮----
// standardized gen-ai llm span attributes:
⋮----
function processResult(result: string): RESULT
⋮----
// Add response information to the span:
⋮----
class DefaultGenerateObjectResult&lt;T&gt; implements GenerateObjectResult&lt;T&gt;
⋮----
constructor(options: {
    object: GenerateObjectResult&lt;T&gt;[&apos;object&apos;];
    finishReason: GenerateObjectResult&lt;T&gt;[&apos;finishReason&apos;];
    usage: GenerateObjectResult&lt;T&gt;[&apos;usage&apos;];
    warnings: GenerateObjectResult&lt;T&gt;[&apos;warnings&apos;];
    logprobs: GenerateObjectResult&lt;T&gt;[&apos;logprobs&apos;];
    providerMetadata: GenerateObjectResult&lt;T&gt;[&apos;providerMetadata&apos;];
    response: GenerateObjectResult&lt;T&gt;[&apos;response&apos;];
    request: GenerateObjectResult&lt;T&gt;[&apos;request&apos;];
})
toJsonResponse(init?: ResponseInit): Response</file><file path="packages/ai/core/generate-object/index.ts"></file><file path="packages/ai/core/generate-object/inject-json-instruction.test.ts">import { JSONSchema7 } from &apos;@ai-sdk/provider&apos;;
import { injectJsonInstruction } from &apos;./inject-json-instruction&apos;;</file><file path="packages/ai/core/generate-object/inject-json-instruction.ts">import { JSONSchema7 } from &apos;@ai-sdk/provider&apos;;
⋮----
export function injectJsonInstruction({
  prompt,
  schema,
  schemaPrefix = schema != null ? DEFAULT_SCHEMA_PREFIX : undefined,
  schemaSuffix = schema != null
    ? DEFAULT_SCHEMA_SUFFIX
    : DEFAULT_GENERIC_SUFFIX,
}: {
  prompt?: string;
  schema?: JSONSchema7;
  schemaPrefix?: string;
  schemaSuffix?: string;
}): string
⋮----
prompt != null &amp;&amp; prompt.length &gt; 0 ? &apos;&apos; : undefined, // add a newline if prompt is not null</file><file path="packages/ai/core/generate-object/output-strategy.ts">import {
  isJSONArray,
  isJSONObject,
  JSONObject,
  JSONSchema7,
  JSONValue,
  TypeValidationError,
  UnsupportedFunctionalityError,
} from &apos;@ai-sdk/provider&apos;;
import { safeValidateTypes, ValidationResult } from &apos;@ai-sdk/provider-utils&apos;;
import { asSchema, DeepPartial, Schema } from &apos;@ai-sdk/ui-utils&apos;;
import { z } from &apos;zod&apos;;
import { NoObjectGeneratedError } from &apos;../../errors/no-object-generated-error&apos;;
import {
  AsyncIterableStream,
  createAsyncIterableStream,
} from &apos;../util/async-iterable-stream&apos;;
import { ObjectStreamPart } from &apos;./stream-object-result&apos;;
import {
  FinishReason,
  LanguageModelResponseMetadata,
  LanguageModelUsage,
} from &apos;../types&apos;;
export interface OutputStrategy&lt;PARTIAL, RESULT, ELEMENT_STREAM&gt; {
  readonly type: &apos;object&apos; | &apos;array&apos; | &apos;enum&apos; | &apos;no-schema&apos;;
  readonly jsonSchema: JSONSchema7 | undefined;
  validatePartialResult({
    value,
    textDelta,
    isFinalDelta,
  }: {
    value: JSONValue;
    textDelta: string;
    isFirstDelta: boolean;
    isFinalDelta: boolean;
    latestObject: PARTIAL | undefined;
  }): ValidationResult&lt;{
    partial: PARTIAL;
    textDelta: string;
  }&gt;;
  validateFinalResult(
    value: JSONValue | undefined,
    context: {
      text: string;
      response: LanguageModelResponseMetadata;
      usage: LanguageModelUsage;
    },
  ): ValidationResult&lt;RESULT&gt;;
  createElementStream(
    originalStream: ReadableStream&lt;ObjectStreamPart&lt;PARTIAL&gt;&gt;,
  ): ELEMENT_STREAM;
}
⋮----
validatePartialResult({
    value,
    textDelta,
    isFinalDelta,
  }: {
    value: JSONValue;
    textDelta: string;
    isFirstDelta: boolean;
    isFinalDelta: boolean;
    latestObject: PARTIAL | undefined;
}): ValidationResult&lt;
validateFinalResult(
    value: JSONValue | undefined,
    context: {
      text: string;
      response: LanguageModelResponseMetadata;
      usage: LanguageModelUsage;
    },
  ): ValidationResult&lt;RESULT&gt;;
createElementStream(
    originalStream: ReadableStream&lt;ObjectStreamPart&lt;PARTIAL&gt;&gt;,
  ): ELEMENT_STREAM;
⋮----
validateFinalResult(
    value: JSONValue | undefined,
    context: {
      text: string;
      response: LanguageModelResponseMetadata;
      usage: LanguageModelUsage;
      finishReason: FinishReason;
    },
): ValidationResult&lt;JSONValue&gt;
createElementStream()
⋮----
const objectOutputStrategy = &lt;OBJECT&gt;(
  schema: Schema&lt;OBJECT&gt;,
): OutputStrategy&lt;DeepPartial&lt;OBJECT&gt;, OBJECT, never&gt; =&gt; (
⋮----
// Note: currently no validation of partial results:
⋮----
validateFinalResult(value: JSONValue | undefined): ValidationResult&lt;OBJECT&gt;
⋮----
const arrayOutputStrategy = &lt;ELEMENT&gt;(
  schema: Schema&lt;ELEMENT&gt;,
): OutputStrategy&lt;ELEMENT[], ELEMENT[], AsyncIterableStream&lt;ELEMENT&gt;&gt; =&gt;
⋮----
// remove $schema from schema.jsonSchema:
⋮----
// wrap in object that contains array of elements, since most LLMs will not
// be able to generate an array directly:
// possible future optimization: use arrays directly when model supports grammar-guided generation
⋮----
// check that the value is an object that contains an array of elements:
⋮----
// special treatment for last processed element:
// ignore parse or validation failures, since they indicate that the
// last element is incomplete and should not be included in the result,
// unless it is the final delta
⋮----
// calculate delta:
⋮----
.slice(publishedElementCount) // only new elements
⋮----
validateFinalResult(
      value: JSONValue | undefined,
): ValidationResult&lt;Array&lt;ELEMENT&gt;&gt;
⋮----
// check that the value is an object that contains an array of elements:
⋮----
// check that each element in the array is of the correct type:
⋮----
createElementStream(
      originalStream: ReadableStream&lt;ObjectStreamPart&lt;ELEMENT[]&gt;&gt;,
)
⋮----
transform(chunk, controller)
⋮----
// publish new elements one by one:
⋮----
case &apos;error&apos;: // suppress error (use onError instead)
⋮----
const enumOutputStrategy = &lt;ENUM extends string&gt;(
  enumValues: Array&lt;ENUM&gt;,
): OutputStrategy&lt;ENUM, ENUM, never&gt; =&gt;
⋮----
// wrap in object that contains result, since most LLMs will not
// be able to generate an enum value directly:
// possible future optimization: use enums directly when model supports top-level enums
⋮----
validateFinalResult(value: JSONValue | undefined): ValidationResult&lt;ENUM&gt;
⋮----
// check that the value is an object that contains an array of elements:
⋮----
validatePartialResult()
⋮----
// no streaming in enum mode
⋮----
// no streaming in enum mode
⋮----
export function getOutputStrategy&lt;SCHEMA&gt;({
  output,
  schema,
  enumValues,
}: {
  output: &apos;object&apos; | &apos;array&apos; | &apos;enum&apos; | &apos;no-schema&apos;;
  schema?: z.Schema&lt;SCHEMA, z.ZodTypeDef, any&gt; | Schema&lt;SCHEMA&gt;;
  enumValues?: Array&lt;SCHEMA&gt;;
}): OutputStrategy&lt;any, any, any&gt;</file><file path="packages/ai/core/generate-object/stream-object-result.ts">import { ServerResponse } from &apos;http&apos;;
import {
  CallWarning,
  FinishReason,
  LanguageModelRequestMetadata,
  LanguageModelResponseMetadata,
  LogProbs,
  ProviderMetadata,
} from &apos;../types&apos;;
import { LanguageModelUsage } from &apos;../types/usage&apos;;
import { AsyncIterableStream } from &apos;../util/async-iterable-stream&apos;;
/**
The result of a `streamObject` call that contains the partial object stream and additional information.
 */
export interface StreamObjectResult&lt;PARTIAL, RESULT, ELEMENT_STREAM&gt; {
  /**
  Warnings from the model provider (e.g. unsupported settings)
     */
  readonly warnings: Promise&lt;CallWarning[] | undefined&gt;;
  /**
  The token usage of the generated response. Resolved when the response is finished.
     */
  readonly usage: Promise&lt;LanguageModelUsage&gt;;
  /**
Additional provider-specific metadata. They are passed through
from the provider to the AI SDK and enable provider-specific
results that can be fully encapsulated in the provider.
   */
  readonly providerMetadata: Promise&lt;ProviderMetadata | undefined&gt;;
  /**
@deprecated Use `providerMetadata` instead.
   */
  readonly experimental_providerMetadata: Promise&lt;ProviderMetadata | undefined&gt;;
  /**
Additional request information from the last step.
 */
  readonly request: Promise&lt;LanguageModelRequestMetadata&gt;;
  /**
Additional response information.
 */
  readonly response: Promise&lt;LanguageModelResponseMetadata&gt;;
  /**
  The generated object (typed according to the schema). Resolved when the response is finished.
     */
  readonly object: Promise&lt;RESULT&gt;;
  /**
  Stream of partial objects. It gets more complete as the stream progresses.
  Note that the partial object is not validated.
  If you want to be certain that the actual content matches your schema, you need to implement your own validation for partial results.
     */
  readonly partialObjectStream: AsyncIterableStream&lt;PARTIAL&gt;;
  /**
   * Stream over complete array elements. Only available if the output strategy is set to `array`.
   */
  readonly elementStream: ELEMENT_STREAM;
  /**
  Text stream of the JSON representation of the generated object. It contains text chunks.
  When the stream is finished, the object is valid JSON that can be parsed.
     */
  readonly textStream: AsyncIterableStream&lt;string&gt;;
  /**
  Stream of different types of events, including partial objects, errors, and finish events.
  Only errors that stop the stream, such as network errors, are thrown.
     */
  readonly fullStream: AsyncIterableStream&lt;ObjectStreamPart&lt;PARTIAL&gt;&gt;;
  /**
  Writes text delta output to a Node.js response-like object.
  It sets a `Content-Type` header to `text/plain; charset=utf-8` and
  writes each text delta as a separate chunk.
  @param response A Node.js response-like object (ServerResponse).
  @param init Optional headers, status code, and status text.
     */
  pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit): void;
  /**
  Creates a simple text stream response.
  The response has a `Content-Type` header set to `text/plain; charset=utf-8`.
  Each text delta is encoded as UTF-8 and sent as a separate chunk.
  Non-text-delta events are ignored.
  @param init Optional headers, status code, and status text.
     */
  toTextStreamResponse(init?: ResponseInit): Response;
}
⋮----
/**
  Warnings from the model provider (e.g. unsupported settings)
     */
⋮----
/**
  The token usage of the generated response. Resolved when the response is finished.
     */
⋮----
/**
Additional provider-specific metadata. They are passed through
from the provider to the AI SDK and enable provider-specific
results that can be fully encapsulated in the provider.
   */
⋮----
/**
@deprecated Use `providerMetadata` instead.
   */
⋮----
/**
Additional request information from the last step.
 */
⋮----
/**
Additional response information.
 */
⋮----
/**
  The generated object (typed according to the schema). Resolved when the response is finished.
     */
⋮----
/**
  Stream of partial objects. It gets more complete as the stream progresses.
  Note that the partial object is not validated.
  If you want to be certain that the actual content matches your schema, you need to implement your own validation for partial results.
     */
⋮----
/**
   * Stream over complete array elements. Only available if the output strategy is set to `array`.
   */
⋮----
/**
  Text stream of the JSON representation of the generated object. It contains text chunks.
  When the stream is finished, the object is valid JSON that can be parsed.
     */
⋮----
/**
  Stream of different types of events, including partial objects, errors, and finish events.
  Only errors that stop the stream, such as network errors, are thrown.
     */
⋮----
/**
  Writes text delta output to a Node.js response-like object.
  It sets a `Content-Type` header to `text/plain; charset=utf-8` and
  writes each text delta as a separate chunk.
  @param response A Node.js response-like object (ServerResponse).
  @param init Optional headers, status code, and status text.
     */
pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit): void;
/**
  Creates a simple text stream response.
  The response has a `Content-Type` header set to `text/plain; charset=utf-8`.
  Each text delta is encoded as UTF-8 and sent as a separate chunk.
  Non-text-delta events are ignored.
  @param init Optional headers, status code, and status text.
     */
toTextStreamResponse(init?: ResponseInit): Response;
⋮----
export type ObjectStreamPart&lt;PARTIAL&gt; =
  | {
      type: &apos;object&apos;;
      object: PARTIAL;
    }
  | {
      type: &apos;text-delta&apos;;
      textDelta: string;
    }
  | {
      type: &apos;error&apos;;
      error: unknown;
    }
  | {
      type: &apos;finish&apos;;
      finishReason: FinishReason;
      logprobs?: LogProbs;
      usage: LanguageModelUsage;
      response: LanguageModelResponseMetadata;
      providerMetadata?: ProviderMetadata;
    };</file><file path="packages/ai/core/generate-object/stream-object.test.ts">import {
  convertArrayToReadableStream,
  convertAsyncIterableToArray,
  convertReadableStreamToArray,
} from &apos;@ai-sdk/provider-utils/test&apos;;
import { jsonSchema } from &apos;@ai-sdk/ui-utils&apos;;
import assert, { fail } from &apos;node:assert&apos;;
import { z } from &apos;zod&apos;;
import {
  NoObjectGeneratedError,
  verifyNoObjectGeneratedError,
} from &apos;../../errors/no-object-generated-error&apos;;
import { MockLanguageModelV1 } from &apos;../test/mock-language-model-v1&apos;;
import { createMockServerResponse } from &apos;../test/mock-server-response&apos;;
import { MockTracer } from &apos;../test/mock-tracer&apos;;
import { AsyncIterableStream } from &apos;../util/async-iterable-stream&apos;;
import { streamObject } from &apos;./stream-object&apos;;
import { StreamObjectResult } from &apos;./stream-object-result&apos;;
⋮----
onError(event)
⋮----
// consume stream
⋮----
// consume stream (runs in parallel)
⋮----
// consume stream (runs in parallel)
⋮----
// consume stream (runs in parallel)
⋮----
// consume stream (runs in parallel)
⋮----
// consume stream (runs in parallel)
⋮----
// consume stream (runs in parallel)
⋮----
// consume stream (runs in parallel)
⋮----
// consume stream (runs in parallel)
⋮----
// consume stream (runs in parallel)
⋮----
// unhandled promise rejection should not be thrown (Vitest does this automatically)
⋮----
// consume stream
⋮----
// consume stream
⋮----
// consume expected error rejection
⋮----
// first element:
⋮----
// second element:
⋮----
// third element:
⋮----
// end of array
⋮----
// finish
⋮----
// consume stream
⋮----
// finish
⋮----
// consume stream
⋮----
// consume stream
⋮----
// consume stream
⋮----
// consume stream
⋮----
// consume stream
⋮----
// consume stream
⋮----
class MockLanguageModelWithImageSupport extends MockLanguageModelV1
⋮----
constructor()
⋮----
supportsUrl(url: URL)
⋮----
// Reference &apos;this&apos; to verify context</file><file path="packages/ai/core/generate-object/stream-object.ts">import {
  JSONValue,
  LanguageModelV1CallOptions,
  LanguageModelV1FinishReason,
  LanguageModelV1StreamPart,
} from &apos;@ai-sdk/provider&apos;;
import { createIdGenerator } from &apos;@ai-sdk/provider-utils&apos;;
import {
  DeepPartial,
  Schema,
  isDeepEqualData,
  parsePartialJson,
} from &apos;@ai-sdk/ui-utils&apos;;
import { ServerResponse } from &apos;http&apos;;
import { z } from &apos;zod&apos;;
import { NoObjectGeneratedError } from &apos;../../errors/no-object-generated-error&apos;;
import { DelayedPromise } from &apos;../../util/delayed-promise&apos;;
import { CallSettings } from &apos;../prompt/call-settings&apos;;
import { convertToLanguageModelPrompt } from &apos;../prompt/convert-to-language-model-prompt&apos;;
import { prepareCallSettings } from &apos;../prompt/prepare-call-settings&apos;;
import { prepareRetries } from &apos;../prompt/prepare-retries&apos;;
import { Prompt } from &apos;../prompt/prompt&apos;;
import { standardizePrompt } from &apos;../prompt/standardize-prompt&apos;;
import { assembleOperationName } from &apos;../telemetry/assemble-operation-name&apos;;
import { getBaseTelemetryAttributes } from &apos;../telemetry/get-base-telemetry-attributes&apos;;
import { getTracer } from &apos;../telemetry/get-tracer&apos;;
import { recordSpan } from &apos;../telemetry/record-span&apos;;
import { selectTelemetryAttributes } from &apos;../telemetry/select-telemetry-attributes&apos;;
import { TelemetrySettings } from &apos;../telemetry/telemetry-settings&apos;;
import {
  CallWarning,
  FinishReason,
  LanguageModel,
  LogProbs,
} from &apos;../types/language-model&apos;;
import { LanguageModelRequestMetadata } from &apos;../types/language-model-request-metadata&apos;;
import { LanguageModelResponseMetadata } from &apos;../types/language-model-response-metadata&apos;;
import { ProviderMetadata, ProviderOptions } from &apos;../types/provider-metadata&apos;;
import {
  LanguageModelUsage,
  calculateLanguageModelUsage,
} from &apos;../types/usage&apos;;
import {
  AsyncIterableStream,
  createAsyncIterableStream,
} from &apos;../util/async-iterable-stream&apos;;
import { createStitchableStream } from &apos;../util/create-stitchable-stream&apos;;
import { now as originalNow } from &apos;../util/now&apos;;
import { prepareOutgoingHttpHeaders } from &apos;../util/prepare-outgoing-http-headers&apos;;
import { prepareResponseHeaders } from &apos;../util/prepare-response-headers&apos;;
import { writeToServerResponse } from &apos;../util/write-to-server-response&apos;;
import { injectJsonInstruction } from &apos;./inject-json-instruction&apos;;
import { OutputStrategy, getOutputStrategy } from &apos;./output-strategy&apos;;
import { ObjectStreamPart, StreamObjectResult } from &apos;./stream-object-result&apos;;
import { validateObjectGenerationInput } from &apos;./validate-object-generation-input&apos;;
import { stringifyForTelemetry } from &apos;../prompt/stringify-for-telemetry&apos;;
⋮----
/**
Callback that is set using the `onError` option.
@param event - The event that is passed to the callback.
 */
export type StreamObjectOnErrorCallback = (event: {
  error: unknown;
}) =&gt; Promise&lt;void&gt; | void;
/**
Callback that is set using the `onFinish` option.
@param event - The event that is passed to the callback.
 */
export type StreamObjectOnFinishCallback&lt;RESULT&gt; = (event: {
  /**
The token usage of the generated response.
*/
  usage: LanguageModelUsage;
  /**
The generated object. Can be undefined if the final object does not match the schema.
*/
  object: RESULT | undefined;
  /**
Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.
*/
  error: unknown | undefined;
  /**
Response metadata.
 */
  response: LanguageModelResponseMetadata;
  /**
Warnings from the model provider (e.g. unsupported settings).
*/
  warnings?: CallWarning[];
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
*/
  providerMetadata: ProviderMetadata | undefined;
  /**
@deprecated Use `providerMetadata` instead.
*/
  experimental_providerMetadata?: ProviderMetadata;
}) =&gt; Promise&lt;void&gt; | void;
⋮----
/**
The token usage of the generated response.
*/
⋮----
/**
The generated object. Can be undefined if the final object does not match the schema.
*/
⋮----
/**
Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.
*/
⋮----
/**
Response metadata.
 */
⋮----
/**
Warnings from the model provider (e.g. unsupported settings).
*/
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
*/
⋮----
/**
@deprecated Use `providerMetadata` instead.
*/
⋮----
/**
Generate a structured, typed object for a given prompt and schema using a language model.
This function streams the output. If you do not want to stream the output, use `generateObject` instead.
@return
A result object for accessing the partial object stream and additional information.
 */
export function streamObject&lt;OBJECT&gt;(
  options: Omit&lt;CallSettings, &apos;stopSequences&apos;&gt; &amp;
    Prompt &amp; {
      output?: &apos;object&apos; | undefined;
      /**
The language model to use.
     */
      model: LanguageModel;
      /**
The schema of the object that the model should generate.
 */
      schema: z.Schema&lt;OBJECT, z.ZodTypeDef, any&gt; | Schema&lt;OBJECT&gt;;
      /**
Optional name of the output that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema name.
     */
      schemaName?: string;
      /**
Optional description of the output that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema description.
 */
      schemaDescription?: string;
      /**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
      mode?: &apos;auto&apos; | &apos;json&apos; | &apos;tool&apos;;
      /**
Optional telemetry configuration (experimental).
     */
      experimental_telemetry?: TelemetrySettings;
      /**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
      providerOptions?: ProviderOptions;
      /**
@deprecated Use `providerOptions` instead.
*/
      experimental_providerMetadata?: ProviderMetadata;
      /**
Callback that is invoked when an error occurs during streaming.
You can use it to log errors.
The stream processing will pause until the callback promise is resolved.
     */
      onError?: StreamObjectOnErrorCallback;
      /**
Callback that is called when the LLM response and the final object validation are finished.
     */
      onFinish?: StreamObjectOnFinishCallback&lt;OBJECT&gt;;
      /**
       * Internal. For test use only. May change without notice.
       */
      _internal?: {
generateId?: ()
⋮----
/**
The language model to use.
     */
⋮----
/**
The schema of the object that the model should generate.
 */
⋮----
/**
Optional name of the output that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema name.
     */
⋮----
/**
Optional description of the output that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema description.
 */
⋮----
/**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
⋮----
/**
Optional telemetry configuration (experimental).
     */
⋮----
/**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
Callback that is invoked when an error occurs during streaming.
You can use it to log errors.
The stream processing will pause until the callback promise is resolved.
     */
⋮----
/**
Callback that is called when the LLM response and the final object validation are finished.
     */
⋮----
/**
       * Internal. For test use only. May change without notice.
       */
⋮----
/**
Generate an array with structured, typed elements for a given prompt and element schema using a language model.
This function streams the output. If you do not want to stream the output, use `generateObject` instead.
@return
A result object for accessing the partial object stream and additional information.
 */
export function streamObject&lt;ELEMENT&gt;(
  options: Omit&lt;CallSettings, &apos;stopSequences&apos;&gt; &amp;
    Prompt &amp; {
      output: &apos;array&apos;;
      /**
The language model to use.
     */
      model: LanguageModel;
      /**
The element schema of the array that the model should generate.
 */
      schema: z.Schema&lt;ELEMENT, z.ZodTypeDef, any&gt; | Schema&lt;ELEMENT&gt;;
      /**
Optional name of the array that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema name.
     */
      schemaName?: string;
      /**
Optional description of the array that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema description.
 */
      schemaDescription?: string;
      /**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
      mode?: &apos;auto&apos; | &apos;json&apos; | &apos;tool&apos;;
      /**
Optional telemetry configuration (experimental).
     */
      experimental_telemetry?: TelemetrySettings;
      /**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
      providerOptions?: ProviderOptions;
      /**
@deprecated Use `providerOptions` instead.
*/
      experimental_providerMetadata?: ProviderMetadata;
      /**
Callback that is invoked when an error occurs during streaming.
You can use it to log errors.
The stream processing will pause until the callback promise is resolved.
     */
      onError?: StreamObjectOnErrorCallback;
      /**
Callback that is called when the LLM response and the final object validation are finished.
     */
      onFinish?: StreamObjectOnFinishCallback&lt;Array&lt;ELEMENT&gt;&gt;;
      /**
       * Internal. For test use only. May change without notice.
       */
      _internal?: {
generateId?: ()
⋮----
/**
The language model to use.
     */
⋮----
/**
The element schema of the array that the model should generate.
 */
⋮----
/**
Optional name of the array that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema name.
     */
⋮----
/**
Optional description of the array that should be generated.
Used by some providers for additional LLM guidance, e.g.
via tool or schema description.
 */
⋮----
/**
The mode to use for object generation.
The schema is converted into a JSON schema and used in one of the following ways
- &apos;auto&apos;: The provider will choose the best mode for the model.
- &apos;tool&apos;: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
- &apos;json&apos;: The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.
Please note that most providers do not support all modes.
Default and recommended: &apos;auto&apos; (best mode for the model).
     */
⋮----
/**
Optional telemetry configuration (experimental).
     */
⋮----
/**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
Callback that is invoked when an error occurs during streaming.
You can use it to log errors.
The stream processing will pause until the callback promise is resolved.
     */
⋮----
/**
Callback that is called when the LLM response and the final object validation are finished.
     */
⋮----
/**
       * Internal. For test use only. May change without notice.
       */
⋮----
/**
Generate JSON with any schema for a given prompt using a language model.
This function streams the output. If you do not want to stream the output, use `generateObject` instead.
@return
A result object for accessing the partial object stream and additional information.
 */
export function streamObject(
  options: Omit&lt;CallSettings, &apos;stopSequences&apos;&gt; &amp;
    Prompt &amp; {
      output: &apos;no-schema&apos;;
      /**
The language model to use.
     */
      model: LanguageModel;
      /**
The mode to use for object generation. Must be &quot;json&quot; for no-schema output.
     */
      mode?: &apos;json&apos;;
      /**
Optional telemetry configuration (experimental).
     */
      experimental_telemetry?: TelemetrySettings;
      /**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
      providerOptions?: ProviderOptions;
      /**
@deprecated Use `providerOptions` instead.
*/
      experimental_providerMetadata?: ProviderMetadata;
      /**
Callback that is invoked when an error occurs during streaming.
You can use it to log errors.
The stream processing will pause until the callback promise is resolved.
     */
      onError?: StreamObjectOnErrorCallback;
      /**
Callback that is called when the LLM response and the final object validation are finished.
     */
      onFinish?: StreamObjectOnFinishCallback&lt;JSONValue&gt;;
      /**
       * Internal. For test use only. May change without notice.
       */
      _internal?: {
generateId?: ()
⋮----
/**
The language model to use.
     */
⋮----
/**
The mode to use for object generation. Must be &quot;json&quot; for no-schema output.
     */
⋮----
/**
Optional telemetry configuration (experimental).
     */
⋮----
/**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
Callback that is invoked when an error occurs during streaming.
You can use it to log errors.
The stream processing will pause until the callback promise is resolved.
     */
⋮----
/**
Callback that is called when the LLM response and the final object validation are finished.
     */
⋮----
/**
       * Internal. For test use only. May change without notice.
       */
⋮----
export function streamObject&lt;SCHEMA, PARTIAL, RESULT, ELEMENT_STREAM&gt;({
  model,
  schema: inputSchema,
  schemaName,
  schemaDescription,
  mode,
  output = &apos;object&apos;,
  system,
  prompt,
  messages,
  maxRetries,
  abortSignal,
  headers,
  experimental_telemetry: telemetry,
  experimental_providerMetadata,
  providerOptions = experimental_providerMetadata,
  onError,
  onFinish,
  _internal: {
    generateId = originalGenerateId,
    currentDate = () =&gt; new Date(),
    now = originalNow,
  } = {},
  ...settings
}: Omit&lt;CallSettings, &apos;stopSequences&apos;&gt; &amp;
  Prompt &amp; {
    /**
     * The expected structure of the output.
     *
     * - &apos;object&apos;: Generate a single object that conforms to the schema.
     * - &apos;array&apos;: Generate an array of objects that conform to the schema.
     * - &apos;no-schema&apos;: Generate any JSON object. No schema is specified.
     *
     * Default is &apos;object&apos; if not specified.
     */
    output?: &apos;object&apos; | &apos;array&apos; | &apos;no-schema&apos;;
    model: LanguageModel;
    schema?: z.Schema&lt;SCHEMA, z.ZodTypeDef, any&gt; | Schema&lt;SCHEMA&gt;;
    schemaName?: string;
    schemaDescription?: string;
    mode?: &apos;auto&apos; | &apos;json&apos; | &apos;tool&apos;;
    experimental_telemetry?: TelemetrySettings;
    providerOptions?: ProviderOptions;
    experimental_providerMetadata?: ProviderMetadata;
    onError?: StreamObjectOnErrorCallback;
    onFinish?: StreamObjectOnFinishCallback&lt;RESULT&gt;;
    _internal?: {
generateId?: ()
⋮----
/**
     * The expected structure of the output.
     *
     * - &apos;object&apos;: Generate a single object that conforms to the schema.
     * - &apos;array&apos;: Generate an array of objects that conform to the schema.
     * - &apos;no-schema&apos;: Generate any JSON object. No schema is specified.
     *
     * Default is &apos;object&apos; if not specified.
     */
⋮----
// automatically set mode to &apos;json&apos; for no-schema output
⋮----
class DefaultStreamObjectResult&lt;PARTIAL, RESULT, ELEMENT_STREAM&gt;
implements StreamObjectResult&lt;PARTIAL, RESULT, ELEMENT_STREAM&gt;
⋮----
constructor({
    model,
    headers,
    telemetry,
    settings,
    maxRetries: maxRetriesArg,
    abortSignal,
    outputStrategy,
    system,
    prompt,
    messages,
    schemaName,
    schemaDescription,
    providerOptions,
    mode,
    onError,
    onFinish,
    generateId,
    currentDate,
    now,
  }: {
    model: LanguageModel;
    telemetry: TelemetrySettings | undefined;
    headers: Record&lt;string, string | undefined&gt; | undefined;
    settings: Omit&lt;CallSettings, &apos;abortSignal&apos; | &apos;headers&apos;&gt;;
    maxRetries: number | undefined;
    abortSignal: AbortSignal | undefined;
    outputStrategy: OutputStrategy&lt;PARTIAL, RESULT, ELEMENT_STREAM&gt;;
    system: Prompt[&apos;system&apos;];
    prompt: Prompt[&apos;prompt&apos;];
    messages: Prompt[&apos;messages&apos;];
    schemaName: string | undefined;
    schemaDescription: string | undefined;
    providerOptions: ProviderOptions | undefined;
    mode: &apos;auto&apos; | &apos;json&apos; | &apos;tool&apos; | undefined;
    onError: StreamObjectOnErrorCallback | undefined;
    onFinish: StreamObjectOnFinishCallback&lt;RESULT&gt; | undefined;
generateId: ()
⋮----
transform(chunk, controller)
⋮----
// specific settings that only make sense on the outer level:
⋮----
// use the default provider mode when the mode is set to &apos;auto&apos; or unspecified
⋮----
modelSupportsUrl: model.supportsUrl?.bind(model), // support &apos;this&apos; context
⋮----
modelSupportsUrl: model.supportsUrl?.bind(model), // support &apos;this&apos; context,
⋮----
// standardized gen-ai llm span attributes:
⋮----
// store information for onFinish callback:
⋮----
// pipe chunks through a transformation stream that extracts metadata:
⋮----
// Keep track of raw parse result before type validation, since e.g. Zod might
// change the object by mapping properties.
⋮----
async transform(chunk, controller): Promise&lt;void&gt;
⋮----
// Telemetry event for first chunk:
⋮----
// process partial text chunks
⋮----
// inside inner check to correctly parse the final element in array mode:
⋮----
// send final text delta:
⋮----
// store finish reason for telemetry:
⋮----
// store usage and metadata for promises and onFinish callback:
⋮----
// resolve promises that can be resolved now:
⋮----
// resolve the object promise with the latest object:
⋮----
// invoke onFinish callback and resolve toolResults promise when the stream is about to close:
async flush(controller)
⋮----
// standardized gen-ai llm span attributes:
⋮----
// finish doStreamSpan before other operations for correct timing:
⋮----
// Add response information to the root span:
⋮----
// call onFinish callback:
⋮----
// add an empty stream with an error to break the stream:
⋮----
start(controller)
⋮----
get object()
get usage()
get experimental_providerMetadata()
get providerMetadata()
get warnings()
get request()
get response()
get partialObjectStream(): AsyncIterableStream&lt;PARTIAL&gt;
⋮----
case &apos;error&apos;: // suppress error (use onError instead)
⋮----
get elementStream(): ELEMENT_STREAM
get textStream(): AsyncIterableStream&lt;string&gt;
⋮----
case &apos;error&apos;: // suppress error (use onError instead)
⋮----
get fullStream(): AsyncIterableStream&lt;ObjectStreamPart&lt;PARTIAL&gt;&gt;
pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit)
toTextStreamResponse(init?: ResponseInit): Response
⋮----
export type ObjectStreamInputPart =
  | {
      type: &apos;error&apos;;
      error: unknown;
    }
  | {
      type: &apos;response-metadata&apos;;
      id?: string;
      timestamp?: Date;
      modelId?: string;
    }
  | {
      type: &apos;finish&apos;;
      finishReason: FinishReason;
      logprobs?: LogProbs;
      usage: LanguageModelUsage;
      providerMetadata?: ProviderMetadata;
    };</file><file path="packages/ai/core/generate-object/validate-object-generation-input.ts">import { z } from &apos;zod&apos;;
import { InvalidArgumentError } from &apos;../../errors/invalid-argument-error&apos;;
import { Schema } from &apos;@ai-sdk/ui-utils&apos;;
export function validateObjectGenerationInput({
  output,
  mode,
  schema,
  schemaName,
  schemaDescription,
  enumValues,
}: {
  output?: &apos;object&apos; | &apos;array&apos; | &apos;enum&apos; | &apos;no-schema&apos;;
  schema?: z.Schema&lt;any, z.ZodTypeDef, any&gt; | Schema&lt;any&gt;;
  schemaName?: string;
  schemaDescription?: string;
  enumValues?: Array&lt;unknown&gt;;
  mode?: &apos;auto&apos; | &apos;json&apos; | &apos;tool&apos;;
})</file><file path="packages/ai/core/generate-speech/generate-speech-result.ts">import { JSONValue } from &apos;@ai-sdk/provider&apos;;
import { SpeechModelResponseMetadata } from &apos;../types/speech-model-response-metadata&apos;;
import { SpeechWarning } from &apos;../types&apos;;
import { GeneratedAudioFile } from &apos;./generated-audio-file&apos;;
/**
The result of a `generateSpeech` call.
It contains the audio data and additional information.
 */
export interface SpeechResult {
  /**
   * The audio data as a base64 encoded string or binary data.
   */
  readonly audio: GeneratedAudioFile;
  /**
  Warnings for the call, e.g. unsupported settings.
     */
  readonly warnings: Array&lt;SpeechWarning&gt;;
  /**
  Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.
   */
  readonly responses: Array&lt;SpeechModelResponseMetadata&gt;;
  /**
  Provider metadata from the provider.
   */
  readonly providerMetadata: Record&lt;string, Record&lt;string, JSONValue&gt;&gt;;
}
⋮----
/**
   * The audio data as a base64 encoded string or binary data.
   */
⋮----
/**
  Warnings for the call, e.g. unsupported settings.
     */
⋮----
/**
  Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.
   */
⋮----
/**
  Provider metadata from the provider.
   */</file><file path="packages/ai/core/generate-speech/generate-speech.test.ts">import {
  JSONValue,
  SpeechModelV1,
  SpeechModelV1CallWarning,
} from &apos;@ai-sdk/provider&apos;;
import { MockSpeechModelV1 } from &apos;../test/mock-speech-model-v1&apos;;
import { generateSpeech } from &apos;./generate-speech&apos;;
import {
  GeneratedAudioFile,
  DefaultGeneratedAudioFile,
} from &apos;./generated-audio-file&apos;;
const audio = new Uint8Array([1, 2, 3, 4]); // Sample audio data
⋮----
const createMockResponse = (options: {
  audio: GeneratedAudioFile;
  warnings?: SpeechModelV1CallWarning[];
  timestamp?: Date;
  modelId?: string;
  headers?: Record&lt;string, string&gt;;
  providerMetadata?: Record&lt;string, Record&lt;string, JSONValue&gt;&gt;;
}) =&gt; (</file><file path="packages/ai/core/generate-speech/generate-speech.ts">import { JSONValue, SpeechModelV1 } from &apos;@ai-sdk/provider&apos;;
import { NoSpeechGeneratedError } from &apos;../../errors/no-speech-generated-error&apos;;
import { prepareRetries } from &apos;../prompt/prepare-retries&apos;;
import { ProviderOptions } from &apos;../types/provider-metadata&apos;;
import { SpeechWarning } from &apos;../types/speech-model&apos;;
import { SpeechModelResponseMetadata } from &apos;../types/speech-model-response-metadata&apos;;
import { SpeechResult } from &apos;./generate-speech-result&apos;;
import {
  audioMimeTypeSignatures,
  detectMimeType,
} from &apos;../util/detect-mimetype&apos;;
import {
  DefaultGeneratedAudioFile,
  GeneratedAudioFile,
} from &apos;./generated-audio-file&apos;;
/**
Generates speech audio using a speech model.
@param model - The speech model to use.
@param text - The text to convert to speech.
@param voice - The voice to use for speech generation.
@param outputFormat - The output format to use for speech generation e.g. &quot;mp3&quot;, &quot;wav&quot;, etc.
@param instructions - Instructions for the speech generation e.g. &quot;Speak in a slow and steady tone&quot;.
@param speed - The speed of the speech generation.
@param providerOptions - Additional provider-specific options that are passed through to the provider
as body parameters.
@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.
@param abortSignal - An optional abort signal that can be used to cancel the call.
@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.
@returns A result object that contains the generated audio data.
 */
export async function generateSpeech({
  model,
  text,
  voice,
  outputFormat,
  instructions,
  speed,
  providerOptions = {},
  maxRetries: maxRetriesArg,
  abortSignal,
  headers,
}: {
  /**
The speech model to use.
     */
  model: SpeechModelV1;
  /**
The text to convert to speech.
   */
  text: string;
  /**
The voice to use for speech generation.
   */
  voice?: string;
  /**
   * The desired output format for the audio e.g. &quot;mp3&quot;, &quot;wav&quot;, etc.
   */
outputFormat?: &apos;mp3&apos; | &apos;wav&apos; | (string &amp;
⋮----
/**
The speech model to use.
     */
⋮----
/**
The text to convert to speech.
   */
⋮----
/**
The voice to use for speech generation.
   */
⋮----
/**
   * The desired output format for the audio e.g. &quot;mp3&quot;, &quot;wav&quot;, etc.
   */
⋮----
/**
    Instructions for the speech generation e.g. &quot;Speak in a slow and steady tone&quot;.
  */
⋮----
/**
  The speed of the speech generation.
   */
⋮----
/**
Additional provider-specific options that are passed through to the provider
as body parameters.
The outer record is keyed by the provider name, and the inner
record is keyed by the provider-specific metadata key.
```ts
{
  &quot;openai&quot;: {}
}
```
     */
⋮----
/**
Maximum number of retries per speech model call. Set to 0 to disable retries.
@default 2
   */
⋮----
/**
Abort signal.
 */
⋮----
/**
Additional headers to include in the request.
Only applicable for HTTP-based providers.
 */
⋮----
class DefaultSpeechResult implements SpeechResult
⋮----
constructor(options: {
    audio: GeneratedAudioFile;
    warnings: Array&lt;SpeechWarning&gt;;
    responses: Array&lt;SpeechModelResponseMetadata&gt;;
    providerMetadata: Record&lt;string, Record&lt;string, JSONValue&gt;&gt; | undefined;
})</file><file path="packages/ai/core/generate-speech/generated-audio-file.ts">import {
  GeneratedFile,
  DefaultGeneratedFile,
} from &apos;../generate-text/generated-file&apos;;
/**
 * A generated audio file.
 */
export interface GeneratedAudioFile extends GeneratedFile {
  /**
   * Audio format of the file (e.g., &apos;mp3&apos;, &apos;wav&apos;, etc.)
   */
  readonly format: string;
}
⋮----
/**
   * Audio format of the file (e.g., &apos;mp3&apos;, &apos;wav&apos;, etc.)
   */
⋮----
export class DefaultGeneratedAudioFile
extends DefaultGeneratedFile
⋮----
constructor({
    data,
    mimeType,
  }: {
    data: string | Uint8Array;
    mimeType: string;
})
⋮----
// If format is not provided, try to determine it from the mimeType
⋮----
// Handle special cases for audio formats
⋮----
export class DefaultGeneratedAudioFileWithType extends DefaultGeneratedAudioFile
⋮----
constructor(options: {
    data: string | Uint8Array;
    mimeType: string;
    format: string;
})</file><file path="packages/ai/core/generate-speech/index.ts"></file><file path="packages/ai/core/generate-text/generate-text-result.ts">import {
  CallWarning,
  FinishReason,
  LogProbs,
  ProviderMetadata,
} from &apos;../types&apos;;
import { Source } from &apos;../types/language-model&apos;;
import { LanguageModelRequestMetadata } from &apos;../types/language-model-request-metadata&apos;;
import { LanguageModelResponseMetadata } from &apos;../types/language-model-response-metadata&apos;;
import { LanguageModelUsage } from &apos;../types/usage&apos;;
import { GeneratedFile } from &apos;./generated-file&apos;;
import { ReasoningDetail } from &apos;./reasoning-detail&apos;;
import { ResponseMessage, StepResult } from &apos;./step-result&apos;;
import { ToolCallArray } from &apos;./tool-call&apos;;
import { ToolResultArray } from &apos;./tool-result&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
/**
The result of a `generateText` call.
It contains the generated text, the tool calls that were made during the generation, and the results of the tool calls.
 */
export interface GenerateTextResult&lt;TOOLS extends ToolSet, OUTPUT&gt; {
  /**
The generated text.
     */
  readonly text: string;
  /**
The reasoning text that the model has generated. Can be undefined if the model
has only generated text.
   */
  // TODO v5: rename to `reasoningText`
  readonly reasoning: string | undefined;
  /**
The files that were generated. Empty array if no files were generated.
     */
  readonly files: Array&lt;GeneratedFile&gt;;
  /**
The full reasoning that the model has generated.
   */
  // TODO v5: rename to `reasoning`
  readonly reasoningDetails: Array&lt;ReasoningDetail&gt;;
  /**
Sources that have been used as input to generate the response.
For multi-step generation, the sources are accumulated from all steps.
   */
  readonly sources: Source[];
  /**
The generated structured output. It uses the `experimental_output` specification.
   */
  readonly experimental_output: OUTPUT;
  /**
  The tool calls that were made during the generation.
   */
  readonly toolCalls: ToolCallArray&lt;TOOLS&gt;;
  /**
  The results of the tool calls.
   */
  readonly toolResults: ToolResultArray&lt;TOOLS&gt;;
  /**
  The reason why the generation finished.
   */
  readonly finishReason: FinishReason;
  /**
  The token usage of the generated text.
   */
  readonly usage: LanguageModelUsage;
  /**
  Warnings from the model provider (e.g. unsupported settings)
   */
  readonly warnings: CallWarning[] | undefined;
  /**
Details for all steps.
You can use this to get information about intermediate steps,
such as the tool calls or the response headers.
   */
  readonly steps: Array&lt;StepResult&lt;TOOLS&gt;&gt;;
  /**
Additional request information.
   */
  readonly request: LanguageModelRequestMetadata;
  /**
Additional response information.
   */
  readonly response: LanguageModelResponseMetadata &amp; {
    /**
The response messages that were generated during the call. It consists of an assistant message,
potentially containing tool calls.
When there are tool results, there is an additional tool message with the tool results that are available.
If there are tools that do not have execute functions, they are not included in the tool results and
need to be added separately.
       */
    messages: Array&lt;ResponseMessage&gt;;
    /**
Response body (available only for providers that use HTTP requests).
     */
    body?: unknown;
  };
  /**
Logprobs for the completion.
`undefined` if the mode does not support logprobs or if it was not enabled.
@deprecated Will become a provider extension in the future.
     */
  readonly logprobs: LogProbs | undefined;
  /**
Additional provider-specific metadata. They are passed through
from the provider to the AI SDK and enable provider-specific
results that can be fully encapsulated in the provider.
   */
  readonly providerMetadata: ProviderMetadata | undefined;
  /**
@deprecated Use `providerMetadata` instead.
   */
  readonly experimental_providerMetadata: ProviderMetadata | undefined;
}
⋮----
/**
The generated text.
     */
⋮----
/**
The reasoning text that the model has generated. Can be undefined if the model
has only generated text.
   */
// TODO v5: rename to `reasoningText`
⋮----
/**
The files that were generated. Empty array if no files were generated.
     */
⋮----
/**
The full reasoning that the model has generated.
   */
// TODO v5: rename to `reasoning`
⋮----
/**
Sources that have been used as input to generate the response.
For multi-step generation, the sources are accumulated from all steps.
   */
⋮----
/**
The generated structured output. It uses the `experimental_output` specification.
   */
⋮----
/**
  The tool calls that were made during the generation.
   */
⋮----
/**
  The results of the tool calls.
   */
⋮----
/**
  The reason why the generation finished.
   */
⋮----
/**
  The token usage of the generated text.
   */
⋮----
/**
  Warnings from the model provider (e.g. unsupported settings)
   */
⋮----
/**
Details for all steps.
You can use this to get information about intermediate steps,
such as the tool calls or the response headers.
   */
⋮----
/**
Additional request information.
   */
⋮----
/**
Additional response information.
   */
⋮----
/**
The response messages that were generated during the call. It consists of an assistant message,
potentially containing tool calls.
When there are tool results, there is an additional tool message with the tool results that are available.
If there are tools that do not have execute functions, they are not included in the tool results and
need to be added separately.
       */
⋮----
/**
Response body (available only for providers that use HTTP requests).
     */
⋮----
/**
Logprobs for the completion.
`undefined` if the mode does not support logprobs or if it was not enabled.
@deprecated Will become a provider extension in the future.
     */
⋮----
/**
Additional provider-specific metadata. They are passed through
from the provider to the AI SDK and enable provider-specific
results that can be fully encapsulated in the provider.
   */
⋮----
/**
@deprecated Use `providerMetadata` instead.
   */</file><file path="packages/ai/core/generate-text/generate-text.test.ts">import { LanguageModelV1CallOptions } from &apos;@ai-sdk/provider&apos;;
import { mockId } from &apos;@ai-sdk/provider-utils/test&apos;;
import { jsonSchema } from &apos;@ai-sdk/ui-utils&apos;;
import assert from &apos;node:assert&apos;;
import { z } from &apos;zod&apos;;
import { Output } from &apos;.&apos;;
import { ToolExecutionError } from &apos;../../errors&apos;;
import { MockLanguageModelV1 } from &apos;../test/mock-language-model-v1&apos;;
import { MockTracer } from &apos;../test/mock-tracer&apos;;
import { tool } from &apos;../tool/tool&apos;;
import { generateText } from &apos;./generate-text&apos;;
import { GenerateTextResult } from &apos;./generate-text-result&apos;;
import { StepResult } from &apos;./step-result&apos;;
⋮----
// 2nd tool to show typing:
⋮----
// test type inference
⋮----
// test type inference
⋮----
// trailing text is to be discarded, trailing whitespace is to be kept:
⋮----
finishReason: &apos;length&apos;, // trigger continue
⋮----
// case where there is no leading nor trailing whitespace:
⋮----
// test handling of custom response headers:
⋮----
// set up trailing whitespace for next step:
⋮----
// leading whitespace is to be discarded when there is whitespace from previous step
// (for models such as Anthropic that trim trailing whitespace in their inputs):
⋮----
// Abort the operation
⋮----
// 2nd tool to show typing:
⋮----
// test type inference
⋮----
class MockLanguageModelWithImageSupport extends MockLanguageModelV1
⋮----
constructor()
⋮----
supportsUrl(url: URL)
⋮----
// Reference &apos;this&apos; to verify context</file><file path="packages/ai/core/generate-text/generate-text.ts">import { createIdGenerator, IDGenerator } from &apos;@ai-sdk/provider-utils&apos;;
import { Tracer } from &apos;@opentelemetry/api&apos;;
import { InvalidArgumentError } from &apos;../../errors/invalid-argument-error&apos;;
import { NoOutputSpecifiedError } from &apos;../../errors/no-output-specified-error&apos;;
import { ToolExecutionError } from &apos;../../errors/tool-execution-error&apos;;
import { CoreAssistantMessage, CoreMessage } from &apos;../prompt&apos;;
import { CallSettings } from &apos;../prompt/call-settings&apos;;
import { convertToLanguageModelPrompt } from &apos;../prompt/convert-to-language-model-prompt&apos;;
import { prepareCallSettings } from &apos;../prompt/prepare-call-settings&apos;;
import { prepareRetries } from &apos;../prompt/prepare-retries&apos;;
import { prepareToolsAndToolChoice } from &apos;../prompt/prepare-tools-and-tool-choice&apos;;
import { Prompt } from &apos;../prompt/prompt&apos;;
import { standardizePrompt } from &apos;../prompt/standardize-prompt&apos;;
import { assembleOperationName } from &apos;../telemetry/assemble-operation-name&apos;;
import { getBaseTelemetryAttributes } from &apos;../telemetry/get-base-telemetry-attributes&apos;;
import { getTracer } from &apos;../telemetry/get-tracer&apos;;
import { recordSpan } from &apos;../telemetry/record-span&apos;;
import { selectTelemetryAttributes } from &apos;../telemetry/select-telemetry-attributes&apos;;
import { TelemetrySettings } from &apos;../telemetry/telemetry-settings&apos;;
import { LanguageModel, ToolChoice } from &apos;../types&apos;;
import { ProviderMetadata, ProviderOptions } from &apos;../types/provider-metadata&apos;;
import {
  addLanguageModelUsage,
  calculateLanguageModelUsage,
  LanguageModelUsage,
} from &apos;../types/usage&apos;;
import { removeTextAfterLastWhitespace } from &apos;../util/remove-text-after-last-whitespace&apos;;
import { GenerateTextResult } from &apos;./generate-text-result&apos;;
import { DefaultGeneratedFile, GeneratedFile } from &apos;./generated-file&apos;;
import { Output } from &apos;./output&apos;;
import { parseToolCall } from &apos;./parse-tool-call&apos;;
import { asReasoningText, ReasoningDetail } from &apos;./reasoning-detail&apos;;
import { ResponseMessage, StepResult } from &apos;./step-result&apos;;
import { toResponseMessages } from &apos;./to-response-messages&apos;;
import { ToolCallArray } from &apos;./tool-call&apos;;
import { ToolCallRepairFunction } from &apos;./tool-call-repair&apos;;
import { ToolResultArray } from &apos;./tool-result&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
import { stringifyForTelemetry } from &apos;../prompt/stringify-for-telemetry&apos;;
⋮----
/**
Callback that is set using the `onStepFinish` option.
@param stepResult - The result of the step.
 */
export type GenerateTextOnStepFinishCallback&lt;TOOLS extends ToolSet&gt; = (
  stepResult: StepResult&lt;TOOLS&gt;,
) =&gt; Promise&lt;void&gt; | void;
/**
Generate a text and call tools for a given prompt using a language model.
This function does not stream the output. If you want to stream the output, use `streamText` instead.
@param model - The language model to use.
@param tools - Tools that are accessible to and can be called by the model. The model needs to support calling tools.
@param toolChoice - The tool choice strategy. Default: &apos;auto&apos;.
@param system - A system message that will be part of the prompt.
@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.
@param messages - A list of messages. You can either use `prompt` or `messages` but not both.
@param maxTokens - Maximum number of tokens to generate.
@param temperature - Temperature setting.
The value is passed through to the provider. The range depends on the provider and model.
It is recommended to set either `temperature` or `topP`, but not both.
@param topP - Nucleus sampling.
The value is passed through to the provider. The range depends on the provider and model.
It is recommended to set either `temperature` or `topP`, but not both.
@param topK - Only sample from the top K options for each subsequent token.
Used to remove &quot;long tail&quot; low probability responses.
Recommended for advanced use cases only. You usually only need to use temperature.
@param presencePenalty - Presence penalty setting.
It affects the likelihood of the model to repeat information that is already in the prompt.
The value is passed through to the provider. The range depends on the provider and model.
@param frequencyPenalty - Frequency penalty setting.
It affects the likelihood of the model to repeatedly use the same words or phrases.
The value is passed through to the provider. The range depends on the provider and model.
@param stopSequences - Stop sequences.
If set, the model will stop generating text when one of the stop sequences is generated.
@param seed - The seed (integer) to use for random sampling.
If set and supported by the model, calls will generate deterministic results.
@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.
@param abortSignal - An optional abort signal that can be used to cancel the call.
@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.
@param maxSteps - Maximum number of sequential LLM calls (steps), e.g. when you use tool calls.
@param experimental_generateMessageId - Generate a unique ID for each message.
@param onStepFinish - Callback that is called when each step (LLM call) is finished, including intermediate steps.
@returns
A result object that contains the generated text, the results of the tool calls, and additional information.
 */
export async function generateText&lt;
  TOOLS extends ToolSet,
  OUTPUT = never,
  OUTPUT_PARTIAL = never,
&gt;({
  model,
  tools,
  toolChoice,
  system,
  prompt,
  messages,
  maxRetries: maxRetriesArg,
  abortSignal,
  headers,
  maxSteps = 1,
  experimental_generateMessageId: generateMessageId = originalGenerateMessageId,
  experimental_output: output,
  experimental_continueSteps: continueSteps = false,
  experimental_telemetry: telemetry,
  experimental_providerMetadata,
  providerOptions = experimental_providerMetadata,
  experimental_activeTools: activeTools,
  experimental_prepareStep: prepareStep,
  experimental_repairToolCall: repairToolCall,
  _internal: {
    generateId = originalGenerateId,
    currentDate = () =&gt; new Date(),
  } = {},
  onStepFinish,
  ...settings
}: CallSettings &amp;
  Prompt &amp; {
    /**
The language model to use.
     */
    model: LanguageModel;
    /**
The tools that the model can call. The model needs to support calling tools.
*/
    tools?: TOOLS;
    /**
The tool choice strategy. Default: &apos;auto&apos;.
     */
    toolChoice?: ToolChoice&lt;TOOLS&gt;;
    /**
Maximum number of sequential LLM calls (steps), e.g. when you use tool calls. Must be at least 1.
A maximum number is required to prevent infinite loops in the case of misconfigured tools.
By default, it&apos;s set to 1, which means that only a single LLM call is made.
     */
    maxSteps?: number;
    /**
Generate a unique ID for each message.
     */
    experimental_generateMessageId?: IDGenerator;
    /**
When enabled, the model will perform additional steps if the finish reason is &quot;length&quot; (experimental).
By default, it&apos;s set to false.
     */
    experimental_continueSteps?: boolean;
    /**
Optional telemetry configuration (experimental).
     */
    experimental_telemetry?: TelemetrySettings;
    /**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
    providerOptions?: ProviderOptions;
    /**
@deprecated Use `providerOptions` instead.
     */
    experimental_providerMetadata?: ProviderMetadata;
    /**
Limits the tools that are available for the model to call without
changing the tool call and result types in the result.
     */
    experimental_activeTools?: Array&lt;keyof TOOLS&gt;;
    /**
Optional specification for parsing structured outputs from the LLM response.
     */
    experimental_output?: Output&lt;OUTPUT, OUTPUT_PARTIAL&gt;;
    /**
Optional function that you can use to provide different settings for a step.
@param options - The options for the step.
@param options.steps - The steps that have been executed so far.
@param options.stepNumber - The number of the step that is being executed.
@param options.maxSteps - The maximum number of steps.
@param options.model - The model that is being used.
@returns An object that contains the settings for the step.
If you return undefined (or for undefined settings), the settings from the outer level will be used.
    */
    experimental_prepareStep?: (options: {
      steps: Array&lt;StepResult&lt;TOOLS&gt;&gt;;
      stepNumber: number;
      maxSteps: number;
      model: LanguageModel;
    }) =&gt; PromiseLike&lt;
      | {
          model?: LanguageModel;
          toolChoice?: ToolChoice&lt;TOOLS&gt;;
          experimental_activeTools?: Array&lt;keyof TOOLS&gt;;
        }
      | undefined
    &gt;;
    /**
A function that attempts to repair a tool call that failed to parse.
     */
    experimental_repairToolCall?: ToolCallRepairFunction&lt;TOOLS&gt;;
    /**
    Callback that is called when each step (LLM call) is finished, including intermediate steps.
    */
    onStepFinish?: GenerateTextOnStepFinishCallback&lt;TOOLS&gt;;
    /**
     * Internal. For test use only. May change without notice.
     */
    _internal?: {
      generateId?: IDGenerator;
currentDate?: ()
⋮----
/**
The language model to use.
     */
⋮----
/**
The tools that the model can call. The model needs to support calling tools.
*/
⋮----
/**
The tool choice strategy. Default: &apos;auto&apos;.
     */
⋮----
/**
Maximum number of sequential LLM calls (steps), e.g. when you use tool calls. Must be at least 1.
A maximum number is required to prevent infinite loops in the case of misconfigured tools.
By default, it&apos;s set to 1, which means that only a single LLM call is made.
     */
⋮----
/**
Generate a unique ID for each message.
     */
⋮----
/**
When enabled, the model will perform additional steps if the finish reason is &quot;length&quot; (experimental).
By default, it&apos;s set to false.
     */
⋮----
/**
Optional telemetry configuration (experimental).
     */
⋮----
/**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
     */
⋮----
/**
Limits the tools that are available for the model to call without
changing the tool call and result types in the result.
     */
⋮----
/**
Optional specification for parsing structured outputs from the LLM response.
     */
⋮----
/**
Optional function that you can use to provide different settings for a step.
@param options - The options for the step.
@param options.steps - The steps that have been executed so far.
@param options.stepNumber - The number of the step that is being executed.
@param options.maxSteps - The maximum number of steps.
@param options.model - The model that is being used.
@returns An object that contains the settings for the step.
If you return undefined (or for undefined settings), the settings from the outer level will be used.
    */
⋮----
/**
A function that attempts to repair a tool call that failed to parse.
     */
⋮----
/**
    Callback that is called when each step (LLM call) is finished, including intermediate steps.
    */
⋮----
/**
     * Internal. For test use only. May change without notice.
     */
⋮----
// model:
⋮----
// specific settings that only make sense on the outer level:
⋮----
// after the 1st step, we need to switch to messages format:
⋮----
modelSupportsUrl: stepModel.supportsUrl?.bind(stepModel), // support &apos;this&apos; context
⋮----
// model:
⋮----
// prompt:
⋮----
// convert the language model level tools:
⋮----
// standardized gen-ai llm span attributes:
⋮----
// Fill in default values:
⋮----
// Add response information to the span:
⋮----
// standardized gen-ai llm span attributes:
⋮----
// parse tool calls:
⋮----
// execute tools:
⋮----
// token usage:
⋮----
// check if another step is needed:
⋮----
// only use continue when there are no tool calls:
⋮----
// there are tool calls:
⋮----
// all current tool calls have results:
⋮----
// text:
⋮----
stepType === &apos;continue&apos; &amp;&amp; // only for continue steps
text.trimEnd() !== text // only trim when there is preceding whitespace
⋮----
// sources:
⋮----
// append to messages for potential next step:
⋮----
// continue step: update the last assistant message
// continue is only possible when there are no tool calls,
// so we can assume that there is a single last assistant message:
⋮----
// Add step information (after response messages are updated):
⋮----
// TODO v5: rename reasoning to reasoningText (and use reasoning for composite array)
⋮----
// deep clone msgs to avoid mutating past messages in multi-step:
⋮----
// Add response information to the span:
⋮----
async function executeTools&lt;TOOLS extends ToolSet&gt;({
  toolCalls,
  tools,
  tracer,
  telemetry,
  messages,
  abortSignal,
}: {
  toolCalls: ToolCallArray&lt;TOOLS&gt;;
  tools: TOOLS;
  tracer: Tracer;
  telemetry: TelemetrySettings | undefined;
  messages: CoreMessage[];
  abortSignal: AbortSignal | undefined;
}): Promise&lt;ToolResultArray&lt;TOOLS&gt;&gt;
⋮----
// JSON stringify might fail if the result is not serializable,
// in which case we just ignore it. In the future we might want to
// add an optional serialize method to the tool interface and warn
// if the result is not serializable.
⋮----
class DefaultGenerateTextResult&lt;TOOLS extends ToolSet, OUTPUT&gt;
implements GenerateTextResult&lt;TOOLS, OUTPUT&gt;
⋮----
constructor(options: {
    text: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;text&apos;];
    files: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;files&apos;];
    reasoning: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;reasoning&apos;];
    reasoningDetails: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;reasoningDetails&apos;];
    toolCalls: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;toolCalls&apos;];
    toolResults: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;toolResults&apos;];
    finishReason: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;finishReason&apos;];
    usage: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;usage&apos;];
    warnings: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;warnings&apos;];
    logprobs: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;logprobs&apos;];
    steps: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;steps&apos;];
    providerMetadata: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;providerMetadata&apos;];
    response: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;response&apos;];
    request: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;request&apos;];
    outputResolver: () =&gt; GenerateTextResult&lt;
      TOOLS,
      OUTPUT
    &gt;[&apos;experimental_output&apos;];
    sources: GenerateTextResult&lt;TOOLS, OUTPUT&gt;[&apos;sources&apos;];
})
get experimental_output()
⋮----
function asReasoningDetails(
  reasoning:
    | string
    | Array&lt;
        | { type: &apos;text&apos;; text: string; signature?: string }
        | { type: &apos;redacted&apos;; data: string }
      &gt;
    | undefined,
): Array&lt;
  | { type: &apos;text&apos;; text: string; signature?: string }
  | { type: &apos;redacted&apos;; data: string }
&gt; {
if (reasoning == null)
function asFiles(
  files:
    | Array&lt;{
        data: string | Uint8Array;
        mimeType: string;
      }&gt;
    | undefined,
): Array&lt;GeneratedFile&gt;</file><file path="packages/ai/core/generate-text/generated-file.ts">import {
  convertBase64ToUint8Array,
  convertUint8ArrayToBase64,
} from &apos;@ai-sdk/provider-utils&apos;;
/**
 * A generated file.
 */
export interface GeneratedFile {
  /**
File as a base64 encoded string.
     */
  readonly base64: string;
  /**
File as a Uint8Array.
     */
  readonly uint8Array: Uint8Array;
  /**
MIME type of the file
   */
  readonly mimeType: string;
}
⋮----
/**
File as a base64 encoded string.
     */
⋮----
/**
File as a Uint8Array.
     */
⋮----
/**
MIME type of the file
   */
⋮----
export class DefaultGeneratedFile implements GeneratedFile
⋮----
constructor({
    data,
    mimeType,
  }: {
    data: string | Uint8Array;
    mimeType: string;
})
// lazy conversion with caching to avoid unnecessary conversion overhead:
get base64()
// lazy conversion with caching to avoid unnecessary conversion overhead:
get uint8Array()
⋮----
export class DefaultGeneratedFileWithType extends DefaultGeneratedFile
⋮----
constructor(options:</file><file path="packages/ai/core/generate-text/index.ts">GeneratedFile as Experimental_GeneratedImage, // Image for backwards compatibility, TODO remove in v5</file><file path="packages/ai/core/generate-text/output.test.ts">import { fail } from &apos;assert&apos;;
import { z } from &apos;zod&apos;;
import { verifyNoObjectGeneratedError } from &apos;../../errors/no-object-generated-error&apos;;
import { object } from &apos;./output&apos;;
import { FinishReason } from &apos;../types&apos;;</file><file path="packages/ai/core/generate-text/output.ts">import { safeParseJSON, safeValidateTypes } from &apos;@ai-sdk/provider-utils&apos;;
import {
  asSchema,
  DeepPartial,
  parsePartialJson,
  Schema,
} from &apos;@ai-sdk/ui-utils&apos;;
import { z } from &apos;zod&apos;;
import { NoObjectGeneratedError } from &apos;../../errors&apos;;
import { injectJsonInstruction } from &apos;../generate-object/inject-json-instruction&apos;;
import {
  FinishReason,
  LanguageModel,
  LanguageModelV1CallOptions,
} from &apos;../types/language-model&apos;;
import { LanguageModelResponseMetadata } from &apos;../types/language-model-response-metadata&apos;;
import { LanguageModelUsage } from &apos;../types/usage&apos;;
export interface Output&lt;OUTPUT, PARTIAL&gt; {
  readonly type: &apos;object&apos; | &apos;text&apos;;
  injectIntoSystemPrompt(options: {
    system: string | undefined;
    model: LanguageModel;
  }): string | undefined;
  responseFormat: (options: {
    model: LanguageModel;
  }) =&gt; LanguageModelV1CallOptions[&apos;responseFormat&apos;];
  parsePartial(options: { text: string }): { partial: PARTIAL } | undefined;
  parseOutput(
    options: { text: string },
    context: {
      response: LanguageModelResponseMetadata;
      usage: LanguageModelUsage;
      finishReason: FinishReason;
    },
  ): OUTPUT;
}
⋮----
injectIntoSystemPrompt(options: {
    system: string | undefined;
    model: LanguageModel;
  }): string | undefined;
⋮----
parsePartial(options:
parseOutput(
    options: { text: string },
    context: {
      response: LanguageModelResponseMetadata;
      usage: LanguageModelUsage;
      finishReason: FinishReason;
    },
  ): OUTPUT;
⋮----
export const text = (): Output&lt;string, string&gt; =&gt; (
⋮----
injectIntoSystemPrompt(
parsePartial(
⋮----
export const object = &lt;OUTPUT&gt;({
  schema: inputSchema,
}: {
  schema: z.Schema&lt;OUTPUT, z.ZodTypeDef, any&gt; | Schema&lt;OUTPUT&gt;;
}): Output&lt;OUTPUT, DeepPartial&lt;OUTPUT&gt;&gt; =&gt;
⋮----
// when the model supports structured outputs,
// we can use the system prompt as is:
⋮----
// Note: currently no validation of partial results:
⋮----
parseOutput(
      { text }: { text: string },
      context: {
        response: LanguageModelResponseMetadata;
        usage: LanguageModelUsage;
        finishReason: FinishReason;
      },
)</file><file path="packages/ai/core/generate-text/parse-tool-call.test.ts">import { z } from &apos;zod&apos;;
import { InvalidToolArgumentsError } from &apos;../../errors/invalid-tool-arguments-error&apos;;
import { NoSuchToolError } from &apos;../../errors/no-such-tool-error&apos;;
import { tool } from &apos;../tool&apos;;
import { parseToolCall } from &apos;./parse-tool-call&apos;;
import { ToolCallRepairError } from &apos;../../errors/tool-call-repair-error&apos;;
⋮----
args: &apos;{&quot;param1&quot;: &quot;test&quot;}&apos;, // Missing required param2
⋮----
args: &apos;invalid json&apos;, // This will trigger repair
⋮----
// Verify repair function was called
⋮----
// Verify the repaired result was used</file><file path="packages/ai/core/generate-text/parse-tool-call.ts">import { LanguageModelV1FunctionToolCall } from &apos;@ai-sdk/provider&apos;;
import { safeParseJSON, safeValidateTypes } from &apos;@ai-sdk/provider-utils&apos;;
import { Schema, asSchema } from &apos;@ai-sdk/ui-utils&apos;;
import { InvalidToolArgumentsError } from &apos;../../errors/invalid-tool-arguments-error&apos;;
import { NoSuchToolError } from &apos;../../errors/no-such-tool-error&apos;;
import { ToolCallRepairError } from &apos;../../errors/tool-call-repair-error&apos;;
import { CoreMessage } from &apos;../prompt&apos;;
import { inferParameters } from &apos;../tool/tool&apos;;
import { ToolCallUnion } from &apos;./tool-call&apos;;
import { ToolCallRepairFunction } from &apos;./tool-call-repair&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
export async function parseToolCall&lt;TOOLS extends ToolSet&gt;({
  toolCall,
  tools,
  repairToolCall,
  system,
  messages,
}: {
  toolCall: LanguageModelV1FunctionToolCall;
  tools: TOOLS | undefined;
  repairToolCall: ToolCallRepairFunction&lt;TOOLS&gt; | undefined;
  system: string | undefined;
  messages: CoreMessage[];
}): Promise&lt;ToolCallUnion&lt;TOOLS&gt;&gt;
⋮----
// no repaired tool call returned
⋮----
async function doParseToolCall&lt;TOOLS extends ToolSet&gt;({
  toolCall,
  tools,
}: {
  toolCall: LanguageModelV1FunctionToolCall;
  tools: TOOLS;
}): Promise&lt;ToolCallUnion&lt;TOOLS&gt;&gt;
⋮----
// when the tool call has no arguments, we try passing an empty object to the schema
// (many LLMs generate empty strings for tool calls with no arguments)</file><file path="packages/ai/core/generate-text/reasoning-detail.ts">export type ReasoningDetail =
  | { type: &apos;text&apos;; text: string; signature?: string }
  | { type: &apos;redacted&apos;; data: string };
export function asReasoningText(
  reasoning: Array&lt;ReasoningDetail&gt;,
): string | undefined</file><file path="packages/ai/core/generate-text/run-tools-transformation.test.ts">import { LanguageModelV1StreamPart } from &apos;@ai-sdk/provider&apos;;
import { delay } from &apos;@ai-sdk/provider-utils&apos;;
import {
  convertArrayToReadableStream,
  convertReadableStreamToArray,
} from &apos;@ai-sdk/provider-utils/test&apos;;
import { z } from &apos;zod&apos;;
import { NoSuchToolError } from &apos;../../errors&apos;;
import { MockTracer } from &apos;../test/mock-tracer&apos;;
import { runToolsTransformation } from &apos;./run-tools-transformation&apos;;
⋮----
await delay(0); // Simulate delayed execution</file><file path="packages/ai/core/generate-text/run-tools-transformation.ts">import { LanguageModelV1StreamPart } from &apos;@ai-sdk/provider&apos;;
import { generateId } from &apos;@ai-sdk/ui-utils&apos;;
import { Tracer } from &apos;@opentelemetry/api&apos;;
import { ToolExecutionError } from &apos;../../errors&apos;;
import { CoreMessage } from &apos;../prompt/message&apos;;
import { assembleOperationName } from &apos;../telemetry/assemble-operation-name&apos;;
import { recordSpan } from &apos;../telemetry/record-span&apos;;
import { selectTelemetryAttributes } from &apos;../telemetry/select-telemetry-attributes&apos;;
import { TelemetrySettings } from &apos;../telemetry/telemetry-settings&apos;;
import {
  FinishReason,
  LanguageModelUsage,
  LogProbs,
  ProviderMetadata,
} from &apos;../types&apos;;
import { Source } from &apos;../types/language-model&apos;;
import { calculateLanguageModelUsage } from &apos;../types/usage&apos;;
import { DefaultGeneratedFileWithType, GeneratedFile } from &apos;./generated-file&apos;;
import { parseToolCall } from &apos;./parse-tool-call&apos;;
import { ToolCallUnion } from &apos;./tool-call&apos;;
import { ToolCallRepairFunction } from &apos;./tool-call-repair&apos;;
import { ToolResultUnion } from &apos;./tool-result&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
export type SingleRequestTextStreamPart&lt;TOOLS extends ToolSet&gt; =
  | {
      type: &apos;text-delta&apos;;
      textDelta: string;
    }
  | {
      type: &apos;reasoning&apos;;
      textDelta: string;
    }
  | {
      type: &apos;reasoning-signature&apos;;
      signature: string;
    }
  | {
      type: &apos;redacted-reasoning&apos;;
      data: string;
    }
  | ({
      type: &apos;file&apos;;
    } &amp; GeneratedFile)
  | {
      type: &apos;source&apos;;
      source: Source;
    }
  | ({
      type: &apos;tool-call&apos;;
    } &amp; ToolCallUnion&lt;TOOLS&gt;)
  | {
      type: &apos;tool-call-streaming-start&apos;;
      toolCallId: string;
      toolName: string;
    }
  | {
      type: &apos;tool-call-delta&apos;;
      toolCallId: string;
      toolName: string;
      argsTextDelta: string;
    }
  | ({
      type: &apos;tool-result&apos;;
    } &amp; ToolResultUnion&lt;TOOLS&gt;)
  | {
      type: &apos;response-metadata&apos;;
      id?: string;
      timestamp?: Date;
      modelId?: string;
    }
  | {
      type: &apos;finish&apos;;
      finishReason: FinishReason;
      logprobs?: LogProbs;
      usage: LanguageModelUsage;
      experimental_providerMetadata?: ProviderMetadata;
    }
  | {
      type: &apos;error&apos;;
      error: unknown;
    };
export function runToolsTransformation&lt;TOOLS extends ToolSet&gt;({
  tools,
  generatorStream,
  toolCallStreaming,
  tracer,
  telemetry,
  system,
  messages,
  abortSignal,
  repairToolCall,
}: {
  tools: TOOLS | undefined;
  generatorStream: ReadableStream&lt;LanguageModelV1StreamPart&gt;;
  toolCallStreaming: boolean;
  tracer: Tracer;
  telemetry: TelemetrySettings | undefined;
  system: string | undefined;
  messages: CoreMessage[];
  abortSignal: AbortSignal | undefined;
  repairToolCall: ToolCallRepairFunction&lt;TOOLS&gt; | undefined;
}): ReadableStream&lt;SingleRequestTextStreamPart&lt;TOOLS&gt;&gt;
⋮----
// tool results stream
⋮----
start(controller)
⋮----
// keep track of active tool calls for tool call streaming:
⋮----
// keep track of outstanding tool results for stream closing:
⋮----
function attemptClose()
⋮----
// close the tool results controller if no more outstanding tool calls
⋮----
// we delay sending the finish chunk until all tool results (incl. delayed ones)
// are received to ensure that the frontend receives tool results before a message
// finish event arrives.
⋮----
// forward stream
⋮----
async transform(
      chunk: LanguageModelV1StreamPart,
      controller: TransformStreamDefaultController&lt;
        SingleRequestTextStreamPart&lt;TOOLS&gt;
      &gt;,
)
⋮----
// forward:
⋮----
// forward with less information:
⋮----
// process tool call:
⋮----
const toolExecutionId = generateId(); // use our own id to guarantee uniqueness
⋮----
// Note: we don&apos;t await the tool execution here (by leaving out &apos;await&apos; on recordSpan),
// because we want to process the next chunk as soon as possible.
// This is important for the case where the tool execution takes a long time.
⋮----
// record telemetry
⋮----
// JSON stringify might fail if the result is not serializable,
// in which case we just ignore it. In the future we might want to
// add an optional serialize method to the tool interface and warn
// if the result is not serializable.
⋮----
flush()
⋮----
// combine the generator stream and the tool results stream
⋮----
async start(controller)
⋮----
// need to wait for both pipes so there are no dangling promises that
// can cause uncaught promise rejections when the stream is aborted
⋮----
write(chunk)
close()
⋮----
// the generator stream controller is automatically closed when it&apos;s consumed</file><file path="packages/ai/core/generate-text/smooth-stream.test.ts">import { describe, expect, it } from &apos;vitest&apos;;
import { convertArrayToReadableStream } from &apos;../../test&apos;;
import { smoothStream } from &apos;./smooth-stream&apos;;
⋮----
async function consumeStream(stream: ReadableStream&lt;any&gt;)
function delay(delayInMs: number | null)
⋮----
// note: leading whitespace is included here
// because it is part of the new chunk:</file><file path="packages/ai/core/generate-text/smooth-stream.ts">import { delay as originalDelay } from &apos;@ai-sdk/provider-utils&apos;;
import { TextStreamPart } from &apos;./stream-text-result&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
import { InvalidArgumentError } from &apos;@ai-sdk/provider&apos;;
⋮----
/**
 * Detects the first chunk in a buffer.
 *
 * @param buffer - The buffer to detect the first chunk in.
 *
 * @returns The first detected chunk, or `undefined` if no chunk was detected.
 */
export type ChunkDetector = (buffer: string) =&gt; string | undefined | null;
/**
 * Smooths text streaming output.
 *
 * @param delayInMs - The delay in milliseconds between each chunk. Defaults to 10ms. Can be set to `null` to skip the delay.
 * @param chunking - Controls how the text is chunked for streaming. Use &quot;word&quot; to stream word by word (default), &quot;line&quot; to stream line by line, or provide a custom RegExp pattern for custom chunking.
 *
 * @returns A transform stream that smooths text streaming output.
 */
export function smoothStream&lt;TOOLS extends ToolSet&gt;({
  delayInMs = 10,
  chunking = &apos;word&apos;,
  _internal: { delay = originalDelay } = {},
}: {
  delayInMs?: number | null;
  chunking?: &apos;word&apos; | &apos;line&apos; | RegExp | ChunkDetector;
  /**
   * Internal. For test use only. May change without notice.
   */
  _internal?: {
delay?: (delayInMs: number | null)
⋮----
/**
   * Internal. For test use only. May change without notice.
   */
⋮----
detectChunk = buffer =&gt; {
      const match = chunking(buffer);
⋮----
detectChunk = buffer =&gt; {
      const match = chunkingRegex.exec(buffer);
⋮----
async transform(chunk, controller)</file><file path="packages/ai/core/generate-text/step-result.ts">import { CoreAssistantMessage, CoreToolMessage } from &apos;../prompt/message&apos;;
import {
  CallWarning,
  FinishReason,
  LanguageModelRequestMetadata,
  LanguageModelResponseMetadata,
  LogProbs,
  ProviderMetadata,
} from &apos;../types&apos;;
import { Source } from &apos;../types/language-model&apos;;
import { LanguageModelUsage } from &apos;../types/usage&apos;;
import { GeneratedFile } from &apos;./generated-file&apos;;
import { ReasoningDetail } from &apos;./reasoning-detail&apos;;
import { ToolCallArray } from &apos;./tool-call&apos;;
import { ToolResultArray } from &apos;./tool-result&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
/**
A message that was generated during the generation process.
It can be either an assistant message or a tool message.
 */
export type ResponseMessage = (CoreAssistantMessage | CoreToolMessage) &amp; {
  /**
Message ID generated by the AI SDK.
   */
  id: string;
};
⋮----
/**
Message ID generated by the AI SDK.
   */
⋮----
/**
 * The result of a single step in the generation process.
 */
export type StepResult&lt;TOOLS extends ToolSet&gt; = {
  /**
The generated text.
*/
  readonly text: string;
  /**
The reasoning that was generated during the generation.
*/
  // TODO v5: rename to `reasoningText`
  readonly reasoning: string | undefined;
  // TODO v5: rename to `reasoning`
  readonly reasoningDetails: Array&lt;ReasoningDetail&gt;;
  /**
The files that were generated during the generation.
*/
  readonly files: GeneratedFile[];
  /**
The sources that were used to generate the text.
*/
  readonly sources: Source[];
  /**
The tool calls that were made during the generation.
*/
  readonly toolCalls: ToolCallArray&lt;TOOLS&gt;;
  /**
The results of the tool calls.
*/
  readonly toolResults: ToolResultArray&lt;TOOLS&gt;;
  /**
The reason why the generation finished.
*/
  readonly finishReason: FinishReason;
  /**
The token usage of the generated text.
*/
  readonly usage: LanguageModelUsage;
  /**
Warnings from the model provider (e.g. unsupported settings).
*/
  readonly warnings: CallWarning[] | undefined;
  /**
Logprobs for the completion.
`undefined` if the mode does not support logprobs or if was not enabled.
*/
  readonly logprobs: LogProbs | undefined;
  /**
Additional request information.
   */
  readonly request: LanguageModelRequestMetadata;
  /**
Additional response information.
*/
  readonly response: LanguageModelResponseMetadata &amp; {
    /**
The response messages that were generated during the call.
Response messages can be either assistant messages or tool messages.
They contain a generated id.
*/
    readonly messages: Array&lt;ResponseMessage&gt;;
    /**
Response body (available only for providers that use HTTP requests).
     */
    body?: unknown;
  };
  /**
Additional provider-specific metadata. They are passed through
from the provider to the AI SDK and enable provider-specific
results that can be fully encapsulated in the provider.
   */
  readonly providerMetadata: ProviderMetadata | undefined;
  /**
@deprecated Use `providerMetadata` instead.
   */
  readonly experimental_providerMetadata: ProviderMetadata | undefined;
  /**
The type of step that this result is for. The first step is always
an &quot;initial&quot; step, and subsequent steps are either &quot;continue&quot; steps
or &quot;tool-result&quot; steps.
   */
  readonly stepType: &apos;initial&apos; | &apos;continue&apos; | &apos;tool-result&apos;;
  /**
True when there will be a continuation step with a continuation text.
   */
  readonly isContinued: boolean;
};
⋮----
/**
The generated text.
*/
⋮----
/**
The reasoning that was generated during the generation.
*/
// TODO v5: rename to `reasoningText`
⋮----
// TODO v5: rename to `reasoning`
⋮----
/**
The files that were generated during the generation.
*/
⋮----
/**
The sources that were used to generate the text.
*/
⋮----
/**
The tool calls that were made during the generation.
*/
⋮----
/**
The results of the tool calls.
*/
⋮----
/**
The reason why the generation finished.
*/
⋮----
/**
The token usage of the generated text.
*/
⋮----
/**
Warnings from the model provider (e.g. unsupported settings).
*/
⋮----
/**
Logprobs for the completion.
`undefined` if the mode does not support logprobs or if was not enabled.
*/
⋮----
/**
Additional request information.
   */
⋮----
/**
Additional response information.
*/
⋮----
/**
The response messages that were generated during the call.
Response messages can be either assistant messages or tool messages.
They contain a generated id.
*/
⋮----
/**
Response body (available only for providers that use HTTP requests).
     */
⋮----
/**
Additional provider-specific metadata. They are passed through
from the provider to the AI SDK and enable provider-specific
results that can be fully encapsulated in the provider.
   */
⋮----
/**
@deprecated Use `providerMetadata` instead.
   */
⋮----
/**
The type of step that this result is for. The first step is always
an &quot;initial&quot; step, and subsequent steps are either &quot;continue&quot; steps
or &quot;tool-result&quot; steps.
   */
⋮----
/**
True when there will be a continuation step with a continuation text.
   */</file><file path="packages/ai/core/generate-text/stream-text-result.ts">import { ServerResponse } from &apos;node:http&apos;;
import { StreamData } from &apos;../../streams/stream-data&apos;;
import { DataStreamWriter } from &apos;../data-stream/data-stream-writer&apos;;
import {
  CallWarning,
  FinishReason,
  LanguageModelRequestMetadata,
  LogProbs,
  ProviderMetadata,
} from &apos;../types&apos;;
import { Source } from &apos;../types/language-model&apos;;
import { LanguageModelResponseMetadata } from &apos;../types/language-model-response-metadata&apos;;
import { LanguageModelUsage } from &apos;../types/usage&apos;;
import { AsyncIterableStream } from &apos;../util/async-iterable-stream&apos;;
import { GeneratedFile } from &apos;./generated-file&apos;;
import { ReasoningDetail } from &apos;./reasoning-detail&apos;;
import { ResponseMessage, StepResult } from &apos;./step-result&apos;;
import { ToolCallUnion } from &apos;./tool-call&apos;;
import { ToolResultUnion } from &apos;./tool-result&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
export type DataStreamOptions = {
  /**
   * Send usage parts to the client.
   * Default to true.
   */
  // TODO change default to false in v5: secure by default
  sendUsage?: boolean;
  /**
   * Send reasoning parts to the client.
   * Default to false.
   */
  sendReasoning?: boolean;
  /**
   * Send source parts to the client.
   * Default to false.
   */
  sendSources?: boolean;
  /**
   * Send the finish event to the client.
   * Set to false if you are using additional streamText calls
   * that send additional data.
   * Default to true.
   */
  experimental_sendFinish?: boolean;
  /**
   * Send the message start event to the client.
   * Set to false if you are using additional streamText calls
   * and the message start event has already been sent.
   * Default to true.
   *
   * Note: this setting is currently not used, but you should
   * already set it to false if you are using additional
   * streamText calls that send additional data to prevent
   * the message start event from being sent multiple times.
   */
  experimental_sendStart?: boolean;
};
⋮----
/**
   * Send usage parts to the client.
   * Default to true.
   */
// TODO change default to false in v5: secure by default
⋮----
/**
   * Send reasoning parts to the client.
   * Default to false.
   */
⋮----
/**
   * Send source parts to the client.
   * Default to false.
   */
⋮----
/**
   * Send the finish event to the client.
   * Set to false if you are using additional streamText calls
   * that send additional data.
   * Default to true.
   */
⋮----
/**
   * Send the message start event to the client.
   * Set to false if you are using additional streamText calls
   * and the message start event has already been sent.
   * Default to true.
   *
   * Note: this setting is currently not used, but you should
   * already set it to false if you are using additional
   * streamText calls that send additional data to prevent
   * the message start event from being sent multiple times.
   */
⋮----
export type ConsumeStreamOptions = {
  onError?: (error: unknown) =&gt; void;
};
/**
A result object for accessing different stream types and additional information.
 */
export interface StreamTextResult&lt;TOOLS extends ToolSet, PARTIAL_OUTPUT&gt; {
  /**
Warnings from the model provider (e.g. unsupported settings) for the first step.
     */
  readonly warnings: Promise&lt;CallWarning[] | undefined&gt;;
  /**
The total token usage of the generated response.
When there are multiple steps, the usage is the sum of all step usages.
Resolved when the response is finished.
     */
  readonly usage: Promise&lt;LanguageModelUsage&gt;;
  /**
Sources that have been used as input to generate the response.
For multi-step generation, the sources are accumulated from all steps.
Resolved when the response is finished.
   */
  readonly sources: Promise&lt;Source[]&gt;;
  /**
Files that have been generated by the model in the last step.
Resolved when the response is finished.
   */
  readonly files: Promise&lt;GeneratedFile[]&gt;;
  /**
The reason why the generation finished. Taken from the last step.
Resolved when the response is finished.
     */
  readonly finishReason: Promise&lt;FinishReason&gt;;
  /**
Additional provider-specific metadata from the last step.
Metadata is passed through from the provider to the AI SDK and
enables provider-specific results that can be fully encapsulated in the provider.
   */
  readonly providerMetadata: Promise&lt;ProviderMetadata | undefined&gt;;
  /**
@deprecated Use `providerMetadata` instead.
   */
  readonly experimental_providerMetadata: Promise&lt;ProviderMetadata | undefined&gt;;
  /**
The full text that has been generated by the last step.
Resolved when the response is finished.
     */
  readonly text: Promise&lt;string&gt;;
  /**
The reasoning that has been generated by the last step.
Resolved when the response is finished.
     */
  // TODO v5: rename to `reasoningText`
  readonly reasoning: Promise&lt;string | undefined&gt;;
  /**
The full reasoning that the model has generated.
Resolved when the response is finished.
   */
  // TODO v5: rename to `reasoning`
  readonly reasoningDetails: Promise&lt;Array&lt;ReasoningDetail&gt;&gt;;
  /**
The tool calls that have been executed in the last step.
Resolved when the response is finished.
     */
  readonly toolCalls: Promise&lt;ToolCallUnion&lt;TOOLS&gt;[]&gt;;
  /**
The tool results that have been generated in the last step.
Resolved when the all tool executions are finished.
     */
  readonly toolResults: Promise&lt;ToolResultUnion&lt;TOOLS&gt;[]&gt;;
  /**
Details for all steps.
You can use this to get information about intermediate steps,
such as the tool calls or the response headers.
   */
  readonly steps: Promise&lt;Array&lt;StepResult&lt;TOOLS&gt;&gt;&gt;;
  /**
Additional request information from the last step.
 */
  readonly request: Promise&lt;LanguageModelRequestMetadata&gt;;
  /**
Additional response information from the last step.
 */
  readonly response: Promise&lt;
    LanguageModelResponseMetadata &amp; {
      /**
The response messages that were generated during the call. It consists of an assistant message,
potentially containing tool calls.
When there are tool results, there is an additional tool message with the tool results that are available.
If there are tools that do not have execute functions, they are not included in the tool results and
need to be added separately.
       */
      messages: Array&lt;ResponseMessage&gt;;
    }
  &gt;;
  /**
  A text stream that returns only the generated text deltas. You can use it
  as either an AsyncIterable or a ReadableStream. When an error occurs, the
  stream will throw the error.
     */
  readonly textStream: AsyncIterableStream&lt;string&gt;;
  /**
  A stream with all events, including text deltas, tool calls, tool results, and
  errors.
  You can use it as either an AsyncIterable or a ReadableStream.
  Only errors that stop the stream, such as network errors, are thrown.
     */
  readonly fullStream: AsyncIterableStream&lt;TextStreamPart&lt;TOOLS&gt;&gt;;
  /**
A stream of partial outputs. It uses the `experimental_output` specification.
   */
  readonly experimental_partialOutputStream: AsyncIterableStream&lt;PARTIAL_OUTPUT&gt;;
  /**
Consumes the stream without processing the parts.
This is useful to force the stream to finish.
It effectively removes the backpressure and allows the stream to finish,
triggering the `onFinish` callback and the promise resolution.
If an error occurs, it is passed to the optional `onError` callback.
  */
  consumeStream(options?: ConsumeStreamOptions): Promise&lt;void&gt;;
  /**
  Converts the result to a data stream.
  @param data an optional StreamData object that will be merged into the stream.
  @param getErrorMessage an optional function that converts an error to an error message.
  @param sendUsage whether to send the usage information to the client. Defaults to true.
  @param sendReasoning whether to send the reasoning information to the client. Defaults to false.
  @return A data stream.
     */
  toDataStream(
    options?: {
      data?: StreamData;
      getErrorMessage?: (error: unknown) =&gt; string;
    } &amp; DataStreamOptions,
  ): ReadableStream&lt;Uint8Array&gt;;
  /**
   * Merges the result as a data stream into another data stream.
   *
   * @param dataStream A data stream writer.
   * @param options.sendUsage Whether to send the usage information to the client. Defaults to true.
   * @param options.sendReasoning Whether to send the reasoning information to the client. Defaults to false.
   */
  mergeIntoDataStream(
    dataStream: DataStreamWriter,
    options?: DataStreamOptions,
  ): void;
  /**
  Writes data stream output to a Node.js response-like object.
  @param response A Node.js response-like object (ServerResponse).
  @param options.status The status code.
  @param options.statusText The status text.
  @param options.headers The headers.
  @param options.data The stream data.
  @param options.getErrorMessage An optional function that converts an error to an error message.
  @param options.sendUsage Whether to send the usage information to the client. Defaults to true.
  @param options.sendReasoning Whether to send the reasoning information to the client. Defaults to false.
     */
  pipeDataStreamToResponse(
    response: ServerResponse,
    options?: ResponseInit &amp; {
      data?: StreamData;
      getErrorMessage?: (error: unknown) =&gt; string;
    } &amp; DataStreamOptions,
  ): void;
  /**
  Writes text delta output to a Node.js response-like object.
  It sets a `Content-Type` header to `text/plain; charset=utf-8` and
  writes each text delta as a separate chunk.
  @param response A Node.js response-like object (ServerResponse).
  @param init Optional headers, status code, and status text.
     */
  pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit): void;
  /**
  Converts the result to a streamed response object with a stream data part stream.
  It can be used with the `useChat` and `useCompletion` hooks.
  @param options.status The status code.
  @param options.statusText The status text.
  @param options.headers The headers.
  @param options.data The stream data.
  @param options.getErrorMessage An optional function that converts an error to an error message.
  @param options.sendUsage Whether to send the usage information to the client. Defaults to true.
  @param options.sendReasoning Whether to send the reasoning information to the client. Defaults to false.
  @return A response object.
     */
  toDataStreamResponse(
    options?: ResponseInit &amp; {
      data?: StreamData;
      getErrorMessage?: (error: unknown) =&gt; string;
    } &amp; DataStreamOptions,
  ): Response;
  /**
  Creates a simple text stream response.
  Each text delta is encoded as UTF-8 and sent as a separate chunk.
  Non-text-delta events are ignored.
  @param init Optional headers, status code, and status text.
     */
  toTextStreamResponse(init?: ResponseInit): Response;
}
⋮----
/**
Warnings from the model provider (e.g. unsupported settings) for the first step.
     */
⋮----
/**
The total token usage of the generated response.
When there are multiple steps, the usage is the sum of all step usages.
Resolved when the response is finished.
     */
⋮----
/**
Sources that have been used as input to generate the response.
For multi-step generation, the sources are accumulated from all steps.
Resolved when the response is finished.
   */
⋮----
/**
Files that have been generated by the model in the last step.
Resolved when the response is finished.
   */
⋮----
/**
The reason why the generation finished. Taken from the last step.
Resolved when the response is finished.
     */
⋮----
/**
Additional provider-specific metadata from the last step.
Metadata is passed through from the provider to the AI SDK and
enables provider-specific results that can be fully encapsulated in the provider.
   */
⋮----
/**
@deprecated Use `providerMetadata` instead.
   */
⋮----
/**
The full text that has been generated by the last step.
Resolved when the response is finished.
     */
⋮----
/**
The reasoning that has been generated by the last step.
Resolved when the response is finished.
     */
// TODO v5: rename to `reasoningText`
⋮----
/**
The full reasoning that the model has generated.
Resolved when the response is finished.
   */
// TODO v5: rename to `reasoning`
⋮----
/**
The tool calls that have been executed in the last step.
Resolved when the response is finished.
     */
⋮----
/**
The tool results that have been generated in the last step.
Resolved when the all tool executions are finished.
     */
⋮----
/**
Details for all steps.
You can use this to get information about intermediate steps,
such as the tool calls or the response headers.
   */
⋮----
/**
Additional request information from the last step.
 */
⋮----
/**
Additional response information from the last step.
 */
⋮----
/**
The response messages that were generated during the call. It consists of an assistant message,
potentially containing tool calls.
When there are tool results, there is an additional tool message with the tool results that are available.
If there are tools that do not have execute functions, they are not included in the tool results and
need to be added separately.
       */
⋮----
/**
  A text stream that returns only the generated text deltas. You can use it
  as either an AsyncIterable or a ReadableStream. When an error occurs, the
  stream will throw the error.
     */
⋮----
/**
  A stream with all events, including text deltas, tool calls, tool results, and
  errors.
  You can use it as either an AsyncIterable or a ReadableStream.
  Only errors that stop the stream, such as network errors, are thrown.
     */
⋮----
/**
A stream of partial outputs. It uses the `experimental_output` specification.
   */
⋮----
/**
Consumes the stream without processing the parts.
This is useful to force the stream to finish.
It effectively removes the backpressure and allows the stream to finish,
triggering the `onFinish` callback and the promise resolution.
If an error occurs, it is passed to the optional `onError` callback.
  */
consumeStream(options?: ConsumeStreamOptions): Promise&lt;void&gt;;
/**
  Converts the result to a data stream.
  @param data an optional StreamData object that will be merged into the stream.
  @param getErrorMessage an optional function that converts an error to an error message.
  @param sendUsage whether to send the usage information to the client. Defaults to true.
  @param sendReasoning whether to send the reasoning information to the client. Defaults to false.
  @return A data stream.
     */
toDataStream(
    options?: {
      data?: StreamData;
getErrorMessage?: (error: unknown)
/**
   * Merges the result as a data stream into another data stream.
   *
   * @param dataStream A data stream writer.
   * @param options.sendUsage Whether to send the usage information to the client. Defaults to true.
   * @param options.sendReasoning Whether to send the reasoning information to the client. Defaults to false.
   */
mergeIntoDataStream(
    dataStream: DataStreamWriter,
    options?: DataStreamOptions,
  ): void;
/**
  Writes data stream output to a Node.js response-like object.
  @param response A Node.js response-like object (ServerResponse).
  @param options.status The status code.
  @param options.statusText The status text.
  @param options.headers The headers.
  @param options.data The stream data.
  @param options.getErrorMessage An optional function that converts an error to an error message.
  @param options.sendUsage Whether to send the usage information to the client. Defaults to true.
  @param options.sendReasoning Whether to send the reasoning information to the client. Defaults to false.
     */
pipeDataStreamToResponse(
    response: ServerResponse,
    options?: ResponseInit &amp; {
      data?: StreamData;
getErrorMessage?: (error: unknown)
/**
  Writes text delta output to a Node.js response-like object.
  It sets a `Content-Type` header to `text/plain; charset=utf-8` and
  writes each text delta as a separate chunk.
  @param response A Node.js response-like object (ServerResponse).
  @param init Optional headers, status code, and status text.
     */
pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit): void;
/**
  Converts the result to a streamed response object with a stream data part stream.
  It can be used with the `useChat` and `useCompletion` hooks.
  @param options.status The status code.
  @param options.statusText The status text.
  @param options.headers The headers.
  @param options.data The stream data.
  @param options.getErrorMessage An optional function that converts an error to an error message.
  @param options.sendUsage Whether to send the usage information to the client. Defaults to true.
  @param options.sendReasoning Whether to send the reasoning information to the client. Defaults to false.
  @return A response object.
     */
toDataStreamResponse(
    options?: ResponseInit &amp; {
      data?: StreamData;
getErrorMessage?: (error: unknown)
/**
  Creates a simple text stream response.
  Each text delta is encoded as UTF-8 and sent as a separate chunk.
  Non-text-delta events are ignored.
  @param init Optional headers, status code, and status text.
     */
toTextStreamResponse(init?: ResponseInit): Response;
⋮----
export type TextStreamPart&lt;TOOLS extends ToolSet&gt; =
  | {
      type: &apos;text-delta&apos;;
      textDelta: string;
    }
  | {
      type: &apos;reasoning&apos;;
      textDelta: string;
    }
  | {
      type: &apos;reasoning-signature&apos;;
      signature: string;
    }
  | {
      type: &apos;redacted-reasoning&apos;;
      data: string;
    }
  | {
      type: &apos;source&apos;;
      source: Source;
    }
  | ({
      type: &apos;file&apos;;
    } &amp; GeneratedFile)
  | ({
      type: &apos;tool-call&apos;;
    } &amp; ToolCallUnion&lt;TOOLS&gt;)
  | {
      type: &apos;tool-call-streaming-start&apos;;
      toolCallId: string;
      toolName: string;
    }
  | {
      type: &apos;tool-call-delta&apos;;
      toolCallId: string;
      toolName: string;
      argsTextDelta: string;
    }
  | ({
      type: &apos;tool-result&apos;;
    } &amp; ToolResultUnion&lt;TOOLS&gt;)
  | {
      type: &apos;step-start&apos;;
      messageId: string;
      request: LanguageModelRequestMetadata;
      warnings: CallWarning[];
    }
  | {
      type: &apos;step-finish&apos;;
      messageId: string;
      // TODO 5.0 breaking change: remove logprobs
      logprobs?: LogProbs;
      // TODO 5.0 breaking change: remove request (on start instead)
      request: LanguageModelRequestMetadata;
      // TODO 5.0 breaking change: remove warnings (on start instead)
      warnings: CallWarning[] | undefined;
      response: LanguageModelResponseMetadata;
      usage: LanguageModelUsage;
      finishReason: FinishReason;
      providerMetadata: ProviderMetadata | undefined;
      /**
       * @deprecated Use `providerMetadata` instead.
       */
      // TODO 5.0 breaking change: remove
      experimental_providerMetadata?: ProviderMetadata;
      isContinued: boolean;
    }
  | {
      type: &apos;finish&apos;;
      finishReason: FinishReason;
      usage: LanguageModelUsage;
      providerMetadata: ProviderMetadata | undefined;
      /**
       * @deprecated Use `providerMetadata` instead.
       */
      // TODO 5.0 breaking change: remove
      experimental_providerMetadata?: ProviderMetadata;
      /**
       * @deprecated will be moved into provider metadata
       */
      // TODO 5.0 breaking change: remove logprobs
      logprobs?: LogProbs;
      /**
       * @deprecated use response on step-finish instead
       */
      // TODO 5.0 breaking change: remove response (on step instead)
      response: LanguageModelResponseMetadata;
    }
  | {
      type: &apos;error&apos;;
      error: unknown;
    };
⋮----
// TODO 5.0 breaking change: remove logprobs
⋮----
// TODO 5.0 breaking change: remove request (on start instead)
⋮----
// TODO 5.0 breaking change: remove warnings (on start instead)
⋮----
/**
       * @deprecated Use `providerMetadata` instead.
       */
// TODO 5.0 breaking change: remove
⋮----
/**
       * @deprecated Use `providerMetadata` instead.
       */
// TODO 5.0 breaking change: remove
⋮----
/**
       * @deprecated will be moved into provider metadata
       */
// TODO 5.0 breaking change: remove logprobs
⋮----
/**
       * @deprecated use response on step-finish instead
       */
// TODO 5.0 breaking change: remove response (on step instead)</file><file path="packages/ai/core/generate-text/stream-text.test.ts">import {
  LanguageModelV1,
  LanguageModelV1CallOptions,
  LanguageModelV1CallWarning,
  LanguageModelV1StreamPart,
} from &apos;@ai-sdk/provider&apos;;
import { delay } from &apos;@ai-sdk/provider-utils&apos;;
import {
  convertArrayToReadableStream,
  convertAsyncIterableToArray,
  convertReadableStreamToArray,
  convertResponseStreamToArray,
  mockId,
} from &apos;@ai-sdk/provider-utils/test&apos;;
import { jsonSchema } from &apos;@ai-sdk/ui-utils&apos;;
import assert from &apos;node:assert&apos;;
import { z } from &apos;zod&apos;;
import { ToolExecutionError } from &apos;../../errors/tool-execution-error&apos;;
import { StreamData } from &apos;../../streams/stream-data&apos;;
import { createDataStream } from &apos;../data-stream/create-data-stream&apos;;
import { MockLanguageModelV1 } from &apos;../test/mock-language-model-v1&apos;;
import { createMockServerResponse } from &apos;../test/mock-server-response&apos;;
import { MockTracer } from &apos;../test/mock-tracer&apos;;
import { mockValues } from &apos;../test/mock-values&apos;;
import { tool } from &apos;../tool/tool&apos;;
import { object, text } from &apos;./output&apos;;
import { StepResult } from &apos;./step-result&apos;;
import { streamText } from &apos;./stream-text&apos;;
import { StreamTextResult, TextStreamPart } from &apos;./stream-text-result&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
const defaultSettings = ()
function createTestModel({
  stream = convertArrayToReadableStream([
    {
      type: &apos;response-metadata&apos;,
      id: &apos;id-0&apos;,
      modelId: &apos;mock-model-id&apos;,
      timestamp: new Date(0),
    },
    { type: &apos;text-delta&apos;, textDelta: &apos;Hello&apos; },
    { type: &apos;text-delta&apos;, textDelta: &apos;, &apos; },
    { type: &apos;text-delta&apos;, textDelta: `world!` },
    {
      type: &apos;finish&apos;,
      finishReason: &apos;stop&apos;,
      logprobs: undefined,
      usage: { completionTokens: 10, promptTokens: 3 },
    },
  ]),
  rawCall = { rawPrompt: &apos;prompt&apos;, rawSettings: {} },
  rawResponse = undefined,
  request = undefined,
  warnings,
}: {
  stream?: ReadableStream&lt;LanguageModelV1StreamPart&gt;;
  rawResponse?: { headers: Record&lt;string, string&gt; };
  rawCall?: { rawPrompt: string; rawSettings: Record&lt;string, unknown&gt; };
  request?: { body: string };
  warnings?: LanguageModelV1CallWarning[];
} =
⋮----
await delay(50); // delay to show bug where step finish is sent before tool result
⋮----
execute(writer)
⋮----
start(controller)
⋮----
onChunk(event)
⋮----
onError(event)
⋮----
onFinish() {}, // just defined; do nothing
⋮----
// trailing text is to be discarded, trailing whitespace is to be kept:
⋮----
// case where there is no leading nor trailing whitespace:
⋮----
// set up trailing whitespace for next step:
⋮----
// leading whitespace is to be discarded when there is whitespace from previous step
// (for models such as Anthropic that trim trailing whitespace in their inputs):
{ type: &apos;text-delta&apos;, textDelta: &apos; &apos; }, // split into 2 chunks for test coverage
⋮----
class MockLanguageModelWithImageSupport extends MockLanguageModelV1
⋮----
constructor()
⋮----
supportsUrl(url: URL)
⋮----
// Reference &apos;this&apos; to verify context
⋮----
const upperCaseTransform =
&lt;TOOLS extends ToolSet&gt;()
⋮----
transform(chunk, controller)
⋮----
// assuming test arg structure:
⋮----
const toUppercaseAndAddCommaTransform =
&lt;TOOLS extends ToolSet&gt;()
const omitCommaTransform =
&lt;TOOLS extends ToolSet&gt;()
⋮----
const stopWordTransform =
&lt;TOOLS extends ToolSet&gt;()
⋮----
// note: this is a simplified transformation for testing;
// in a real-world version more there would need to be
// stream buffering and scanning to correctly emit prior text
// and to detect all STOP occurrences.
⋮----
// key difference: need to combine after `:`</file><file path="packages/ai/core/generate-text/stream-text.ts">import { AISDKError, LanguageModelV1Source } from &apos;@ai-sdk/provider&apos;;
import { createIdGenerator, IDGenerator } from &apos;@ai-sdk/provider-utils&apos;;
import { DataStreamString, formatDataStreamPart } from &apos;@ai-sdk/ui-utils&apos;;
import { Span } from &apos;@opentelemetry/api&apos;;
import { ServerResponse } from &apos;node:http&apos;;
import { InvalidArgumentError } from &apos;../../errors/invalid-argument-error&apos;;
import { InvalidStreamPartError } from &apos;../../errors/invalid-stream-part-error&apos;;
import { NoOutputSpecifiedError } from &apos;../../errors/no-output-specified-error&apos;;
import { StreamData } from &apos;../../streams/stream-data&apos;;
import { asArray } from &apos;../../util/as-array&apos;;
import { consumeStream } from &apos;../../util/consume-stream&apos;;
import { DelayedPromise } from &apos;../../util/delayed-promise&apos;;
import { DataStreamWriter } from &apos;../data-stream/data-stream-writer&apos;;
import { CallSettings } from &apos;../prompt/call-settings&apos;;
import { convertToLanguageModelPrompt } from &apos;../prompt/convert-to-language-model-prompt&apos;;
import { CoreAssistantMessage } from &apos;../prompt/message&apos;;
import { prepareCallSettings } from &apos;../prompt/prepare-call-settings&apos;;
import { prepareRetries } from &apos;../prompt/prepare-retries&apos;;
import { prepareToolsAndToolChoice } from &apos;../prompt/prepare-tools-and-tool-choice&apos;;
import { Prompt } from &apos;../prompt/prompt&apos;;
import { standardizePrompt } from &apos;../prompt/standardize-prompt&apos;;
import { assembleOperationName } from &apos;../telemetry/assemble-operation-name&apos;;
import { getBaseTelemetryAttributes } from &apos;../telemetry/get-base-telemetry-attributes&apos;;
import { getTracer } from &apos;../telemetry/get-tracer&apos;;
import { recordSpan } from &apos;../telemetry/record-span&apos;;
import { selectTelemetryAttributes } from &apos;../telemetry/select-telemetry-attributes&apos;;
import { TelemetrySettings } from &apos;../telemetry/telemetry-settings&apos;;
import {
  FinishReason,
  LanguageModel,
  LogProbs,
  ToolChoice,
} from &apos;../types/language-model&apos;;
import { LanguageModelResponseMetadata } from &apos;../types/language-model-response-metadata&apos;;
import { ProviderMetadata, ProviderOptions } from &apos;../types/provider-metadata&apos;;
import { addLanguageModelUsage, LanguageModelUsage } from &apos;../types/usage&apos;;
import {
  AsyncIterableStream,
  createAsyncIterableStream,
} from &apos;../util/async-iterable-stream&apos;;
import { createStitchableStream } from &apos;../util/create-stitchable-stream&apos;;
import { mergeStreams } from &apos;../util/merge-streams&apos;;
import { now as originalNow } from &apos;../util/now&apos;;
import { prepareOutgoingHttpHeaders } from &apos;../util/prepare-outgoing-http-headers&apos;;
import { prepareResponseHeaders } from &apos;../util/prepare-response-headers&apos;;
import { splitOnLastWhitespace } from &apos;../util/split-on-last-whitespace&apos;;
import { writeToServerResponse } from &apos;../util/write-to-server-response&apos;;
import { GeneratedFile } from &apos;./generated-file&apos;;
import { Output } from &apos;./output&apos;;
import { asReasoningText, ReasoningDetail } from &apos;./reasoning-detail&apos;;
import {
  runToolsTransformation,
  SingleRequestTextStreamPart,
} from &apos;./run-tools-transformation&apos;;
import { ResponseMessage, StepResult } from &apos;./step-result&apos;;
import {
  ConsumeStreamOptions,
  DataStreamOptions,
  StreamTextResult,
  TextStreamPart,
} from &apos;./stream-text-result&apos;;
import { toResponseMessages } from &apos;./to-response-messages&apos;;
import { ToolCallUnion } from &apos;./tool-call&apos;;
import { ToolCallRepairFunction } from &apos;./tool-call-repair&apos;;
import { ToolResultUnion } from &apos;./tool-result&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
import { stringifyForTelemetry } from &apos;../prompt/stringify-for-telemetry&apos;;
⋮----
/**
A transformation that is applied to the stream.
@param stopStream - A function that stops the source stream.
@param tools - The tools that are accessible to and can be called by the model. The model needs to support calling tools.
 */
export type StreamTextTransform&lt;TOOLS extends ToolSet&gt; = (options: {
  tools: TOOLS; // for type inference
  stopStream: () =&gt; void;
}) =&gt; TransformStream&lt;TextStreamPart&lt;TOOLS&gt;, TextStreamPart&lt;TOOLS&gt;&gt;;
⋮----
tools: TOOLS; // for type inference
⋮----
/**
Callback that is set using the `onError` option.
@param event - The event that is passed to the callback.
 */
export type StreamTextOnErrorCallback = (event: {
  error: unknown;
}) =&gt; Promise&lt;void&gt; | void;
/**
Callback that is set using the `onStepFinish` option.
@param stepResult - The result of the step.
 */
export type StreamTextOnStepFinishCallback&lt;TOOLS extends ToolSet&gt; = (
  stepResult: StepResult&lt;TOOLS&gt;,
) =&gt; Promise&lt;void&gt; | void;
/**
Callback that is set using the `onChunk` option.
@param event - The event that is passed to the callback.
 */
export type StreamTextOnChunkCallback&lt;TOOLS extends ToolSet&gt; = (event: {
  chunk: Extract&lt;
    TextStreamPart&lt;TOOLS&gt;,
    {
      type:
        | &apos;text-delta&apos;
        | &apos;reasoning&apos;
        | &apos;source&apos;
        | &apos;tool-call&apos;
        | &apos;tool-call-streaming-start&apos;
        | &apos;tool-call-delta&apos;
        | &apos;tool-result&apos;;
    }
  &gt;;
}) =&gt; Promise&lt;void&gt; | void;
/**
Callback that is set using the `onFinish` option.
@param event - The event that is passed to the callback.
 */
export type StreamTextOnFinishCallback&lt;TOOLS extends ToolSet&gt; = (
  event: Omit&lt;StepResult&lt;TOOLS&gt;, &apos;stepType&apos; | &apos;isContinued&apos;&gt; &amp; {
    /**
Details for all steps.
   */
    readonly steps: StepResult&lt;TOOLS&gt;[];
  },
) =&gt; Promise&lt;void&gt; | void;
⋮----
/**
Details for all steps.
   */
⋮----
/**
Generate a text and call tools for a given prompt using a language model.
This function streams the output. If you do not want to stream the output, use `generateText` instead.
@param model - The language model to use.
@param tools - Tools that are accessible to and can be called by the model. The model needs to support calling tools.
@param system - A system message that will be part of the prompt.
@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.
@param messages - A list of messages. You can either use `prompt` or `messages` but not both.
@param maxTokens - Maximum number of tokens to generate.
@param temperature - Temperature setting.
The value is passed through to the provider. The range depends on the provider and model.
It is recommended to set either `temperature` or `topP`, but not both.
@param topP - Nucleus sampling.
The value is passed through to the provider. The range depends on the provider and model.
It is recommended to set either `temperature` or `topP`, but not both.
@param topK - Only sample from the top K options for each subsequent token.
Used to remove &quot;long tail&quot; low probability responses.
Recommended for advanced use cases only. You usually only need to use temperature.
@param presencePenalty - Presence penalty setting.
It affects the likelihood of the model to repeat information that is already in the prompt.
The value is passed through to the provider. The range depends on the provider and model.
@param frequencyPenalty - Frequency penalty setting.
It affects the likelihood of the model to repeatedly use the same words or phrases.
The value is passed through to the provider. The range depends on the provider and model.
@param stopSequences - Stop sequences.
If set, the model will stop generating text when one of the stop sequences is generated.
@param seed - The seed (integer) to use for random sampling.
If set and supported by the model, calls will generate deterministic results.
@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.
@param abortSignal - An optional abort signal that can be used to cancel the call.
@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.
@param maxSteps - Maximum number of sequential LLM calls (steps), e.g. when you use tool calls.
@param experimental_generateMessageId - Generate a unique ID for each message.
@param onChunk - Callback that is called for each chunk of the stream. The stream processing will pause until the callback promise is resolved.
@param onError - Callback that is called when an error occurs during streaming. You can use it to log errors.
@param onStepFinish - Callback that is called when each step (LLM call) is finished, including intermediate steps.
@param onFinish - Callback that is called when the LLM response and all request tool executions
(for tools that have an `execute` function) are finished.
@return
A result object for accessing different stream types and additional information.
 */
export function streamText&lt;
  TOOLS extends ToolSet,
  OUTPUT = never,
  PARTIAL_OUTPUT = never,
&gt;({
  model,
  tools,
  toolChoice,
  system,
  prompt,
  messages,
  maxRetries,
  abortSignal,
  headers,
  maxSteps = 1,
  experimental_generateMessageId: generateMessageId = originalGenerateMessageId,
  experimental_output: output,
  experimental_continueSteps: continueSteps = false,
  experimental_telemetry: telemetry,
  experimental_providerMetadata,
  providerOptions = experimental_providerMetadata,
  experimental_toolCallStreaming = false,
  toolCallStreaming = experimental_toolCallStreaming,
  experimental_activeTools: activeTools,
  experimental_repairToolCall: repairToolCall,
  experimental_transform: transform,
  onChunk,
  onError,
  onFinish,
  onStepFinish,
  _internal: {
    now = originalNow,
    generateId = originalGenerateId,
    currentDate = () =&gt; new Date(),
  } = {},
  ...settings
}: CallSettings &amp;
  Prompt &amp; {
    /**
The language model to use.
     */
    model: LanguageModel;
    /**
The tools that the model can call. The model needs to support calling tools.
    */
    tools?: TOOLS;
    /**
The tool choice strategy. Default: &apos;auto&apos;.
     */
    toolChoice?: ToolChoice&lt;TOOLS&gt;;
    /**
Maximum number of sequential LLM calls (steps), e.g. when you use tool calls. Must be at least 1.
A maximum number is required to prevent infinite loops in the case of misconfigured tools.
By default, it&apos;s set to 1, which means that only a single LLM call is made.
 */
    maxSteps?: number;
    /**
Generate a unique ID for each message.
     */
    experimental_generateMessageId?: IDGenerator;
    /**
When enabled, the model will perform additional steps if the finish reason is &quot;length&quot; (experimental).
By default, it&apos;s set to false.
     */
    experimental_continueSteps?: boolean;
    /**
Optional telemetry configuration (experimental).
     */
    experimental_telemetry?: TelemetrySettings;
    /**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
    providerOptions?: ProviderOptions;
    /**
@deprecated Use `providerOptions` instead.
 */
    experimental_providerMetadata?: ProviderMetadata;
    /**
Limits the tools that are available for the model to call without
changing the tool call and result types in the result.
     */
    experimental_activeTools?: Array&lt;keyof TOOLS&gt;;
    /**
Optional specification for parsing structured outputs from the LLM response.
     */
    experimental_output?: Output&lt;OUTPUT, PARTIAL_OUTPUT&gt;;
    /**
A function that attempts to repair a tool call that failed to parse.
     */
    experimental_repairToolCall?: ToolCallRepairFunction&lt;TOOLS&gt;;
    /**
Enable streaming of tool call deltas as they are generated. Disabled by default.
     */
    toolCallStreaming?: boolean;
    /**
@deprecated Use `toolCallStreaming` instead.
     */
    experimental_toolCallStreaming?: boolean;
    /**
Optional stream transformations.
They are applied in the order they are provided.
The stream transformations must maintain the stream structure for streamText to work correctly.
     */
    experimental_transform?:
      | StreamTextTransform&lt;TOOLS&gt;
      | Array&lt;StreamTextTransform&lt;TOOLS&gt;&gt;;
    /**
Callback that is called for each chunk of the stream.
The stream processing will pause until the callback promise is resolved.
     */
    onChunk?: StreamTextOnChunkCallback&lt;TOOLS&gt;;
    /**
Callback that is invoked when an error occurs during streaming.
You can use it to log errors.
The stream processing will pause until the callback promise is resolved.
     */
    onError?: StreamTextOnErrorCallback;
    /**
Callback that is called when the LLM response and all request tool executions
(for tools that have an `execute` function) are finished.
The usage is the combined usage of all steps.
     */
    onFinish?: StreamTextOnFinishCallback&lt;TOOLS&gt;;
    /**
Callback that is called when each step (LLM call) is finished, including intermediate steps.
    */
    onStepFinish?: StreamTextOnStepFinishCallback&lt;TOOLS&gt;;
    /**
Internal. For test use only. May change without notice.
     */
    _internal?: {
now?: ()
⋮----
/**
The language model to use.
     */
⋮----
/**
The tools that the model can call. The model needs to support calling tools.
    */
⋮----
/**
The tool choice strategy. Default: &apos;auto&apos;.
     */
⋮----
/**
Maximum number of sequential LLM calls (steps), e.g. when you use tool calls. Must be at least 1.
A maximum number is required to prevent infinite loops in the case of misconfigured tools.
By default, it&apos;s set to 1, which means that only a single LLM call is made.
 */
⋮----
/**
Generate a unique ID for each message.
     */
⋮----
/**
When enabled, the model will perform additional steps if the finish reason is &quot;length&quot; (experimental).
By default, it&apos;s set to false.
     */
⋮----
/**
Optional telemetry configuration (experimental).
     */
⋮----
/**
Additional provider-specific options. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
 */
⋮----
/**
Limits the tools that are available for the model to call without
changing the tool call and result types in the result.
     */
⋮----
/**
Optional specification for parsing structured outputs from the LLM response.
     */
⋮----
/**
A function that attempts to repair a tool call that failed to parse.
     */
⋮----
/**
Enable streaming of tool call deltas as they are generated. Disabled by default.
     */
⋮----
/**
@deprecated Use `toolCallStreaming` instead.
     */
⋮----
/**
Optional stream transformations.
They are applied in the order they are provided.
The stream transformations must maintain the stream structure for streamText to work correctly.
     */
⋮----
/**
Callback that is called for each chunk of the stream.
The stream processing will pause until the callback promise is resolved.
     */
⋮----
/**
Callback that is invoked when an error occurs during streaming.
You can use it to log errors.
The stream processing will pause until the callback promise is resolved.
     */
⋮----
/**
Callback that is called when the LLM response and all request tool executions
(for tools that have an `execute` function) are finished.
The usage is the combined usage of all steps.
     */
⋮----
/**
Callback that is called when each step (LLM call) is finished, including intermediate steps.
    */
⋮----
/**
Internal. For test use only. May change without notice.
     */
⋮----
type EnrichedStreamPart&lt;TOOLS extends ToolSet, PARTIAL_OUTPUT&gt; = {
  part: TextStreamPart&lt;TOOLS&gt;;
  partialOutput: PARTIAL_OUTPUT | undefined;
};
function createOutputTransformStream&lt;
  TOOLS extends ToolSet,
  OUTPUT,
  PARTIAL_OUTPUT,
&gt;(
  output: Output&lt;OUTPUT, PARTIAL_OUTPUT&gt; | undefined,
): TransformStream&lt;
  TextStreamPart&lt;TOOLS&gt;,
  EnrichedStreamPart&lt;TOOLS, PARTIAL_OUTPUT&gt;
&gt; {
if (!output)
⋮----
transform(chunk, controller)
⋮----
function publishTextChunk({
    controller,
    partialOutput = undefined,
  }: {
    controller: TransformStreamDefaultController&lt;
      EnrichedStreamPart&lt;TOOLS, PARTIAL_OUTPUT&gt;
    &gt;;
    partialOutput?: PARTIAL_OUTPUT;
})
⋮----
// ensure that we publish the last text chunk before the step finish:
⋮----
// only publish if partial json can be parsed:
⋮----
// only send new json if it has changed:
⋮----
flush(controller)
⋮----
// publish remaining text (there should be none if the content was correctly formatted):
⋮----
class DefaultStreamTextResult&lt;TOOLS extends ToolSet, OUTPUT, PARTIAL_OUTPUT&gt;
implements StreamTextResult&lt;TOOLS, PARTIAL_OUTPUT&gt;
⋮----
constructor({
    model,
    telemetry,
    headers,
    settings,
    maxRetries: maxRetriesArg,
    abortSignal,
    system,
    prompt,
    messages,
    tools,
    toolChoice,
    toolCallStreaming,
    transforms,
    activeTools,
    repairToolCall,
    maxSteps,
    output,
    continueSteps,
    providerOptions,
    now,
    currentDate,
    generateId,
    generateMessageId,
    onChunk,
    onError,
    onFinish,
    onStepFinish,
  }: {
    model: LanguageModel;
    telemetry: TelemetrySettings | undefined;
    headers: Record&lt;string, string | undefined&gt; | undefined;
    settings: Omit&lt;CallSettings, &apos;abortSignal&apos; | &apos;headers&apos;&gt;;
    maxRetries: number | undefined;
    abortSignal: AbortSignal | undefined;
    system: Prompt[&apos;system&apos;];
    prompt: Prompt[&apos;prompt&apos;];
    messages: Prompt[&apos;messages&apos;];
    tools: TOOLS | undefined;
    toolChoice: ToolChoice&lt;TOOLS&gt; | undefined;
    toolCallStreaming: boolean;
    transforms: Array&lt;StreamTextTransform&lt;TOOLS&gt;&gt;;
    activeTools: Array&lt;keyof TOOLS&gt; | undefined;
    repairToolCall: ToolCallRepairFunction&lt;TOOLS&gt; | undefined;
    maxSteps: number;
    output: Output&lt;OUTPUT, PARTIAL_OUTPUT&gt; | undefined;
    continueSteps: boolean;
    providerOptions: ProviderOptions | undefined;
now: ()
⋮----
// callbacks:
⋮----
// event processor for telemetry, invoking callbacks, etc.
// The event processor reads the transformed stream to enable correct
// recording of the final transformed outputs.
⋮----
async transform(chunk, controller)
⋮----
controller.enqueue(chunk); // forward the chunk to the next stream
⋮----
activeReasoningText = undefined; // signature concludes reasoning part
⋮----
// determine the next step type
⋮----
// only use continue when there are no tool calls:
⋮----
// there are tool calls:
⋮----
// all current tool calls have results:
⋮----
// Add step information (after response messages are updated):
⋮----
async flush(controller)
⋮----
return; // no steps recorded (e.g. in error scenario)
⋮----
// from last step (when there are errors there may be no last step)
⋮----
// derived:
⋮----
// from finish:
⋮----
// aggregate results:
⋮----
// call onFinish callback:
⋮----
// Add response information to the root span:
⋮----
// initialize the stitchable stream and the transformed stream:
⋮----
// transform the stream before output parsing
// to enable replacement of stream segments:
⋮----
stopStream()
⋮----
// specific settings that only make sense on the outer level:
⋮----
async function streamStep({
          currentStep,
          responseMessages,
          usage,
          stepType,
          previousStepText,
          hasLeadingWhitespace,
          messageId,
        }: {
          currentStep: number;
          responseMessages: Array&lt;ResponseMessage&gt;;
          usage: LanguageModelUsage;
          stepType: &apos;initial&apos; | &apos;continue&apos; | &apos;tool-result&apos;;
          previousStepText: string;
          hasLeadingWhitespace: boolean;
          messageId: string;
})
⋮----
// after the 1st step, we need to switch to messages format:
⋮----
modelSupportsUrl: model.supportsUrl?.bind(model), // support &apos;this&apos; context
⋮----
// convert the language model level tools:
⋮----
// standardized gen-ai llm span attributes:
⋮----
startTimestampMs: now(), // get before the call
⋮----
// chunk buffer when using continue:
⋮----
let hasWhitespaceSuffix = false; // for next step. when true, step ended with whitespace
async function publishTextChunk({
            controller,
            chunk,
          }: {
            controller: TransformStreamDefaultController&lt;TextStreamPart&lt;TOOLS&gt;&gt;;
            chunk: TextStreamPart&lt;TOOLS&gt; &amp; { type: &apos;text-delta&apos; };
})
⋮----
async transform(chunk, controller): Promise&lt;void&gt;
⋮----
// Telemetry for first chunk:
⋮----
// Step start:
⋮----
// Filter out empty text deltas
⋮----
// when a new step starts, leading whitespace is to be discarded
// when there is already preceding whitespace in the chunk buffer
⋮----
// publish the text until the last whitespace:
⋮----
activeReasoningText = undefined; // signature concludes reasoning part
⋮----
// store tool calls for onFinish callback and toolCalls promise:
⋮----
// store tool results for onFinish callback and toolResults promise:
⋮----
// Note: tool executions might not be finished yet when the finish event is emitted.
// store usage and finish reason for promises and onFinish callback:
⋮----
// Telemetry for finish event timing
// (since tool executions can take longer and distort calculations)
⋮----
// forward:
⋮----
// invoke onFinish callback and resolve toolResults promise when the stream is about to close:
⋮----
// determine the next step type
⋮----
// only use continue when there are no tool calls:
⋮----
// there are tool calls:
⋮----
// all current tool calls have results:
⋮----
// when using continuation, publish buffer on final step or if there
// was no whitespace in the step:
⋮----
(nextStepType !== &apos;continue&apos; || // when the next step is a regular step, publish the buffer
(stepType === &apos;continue&apos; &amp;&amp; !chunkTextPublished)) // when the next step is a continue step, publish the buffer if no text was published in the step
⋮----
// record telemetry information first to ensure best effort timing
⋮----
// standardized gen-ai llm span attributes:
⋮----
// ignore error setting telemetry attributes
⋮----
// finish doStreamSpan before other operations for correct timing:
⋮----
self.closeStream(); // close the stitchable stream
⋮----
// append to messages for the next step:
⋮----
// continue step: update the last assistant message
// continue is only possible when there are no tool calls,
// so we can assume that there is a single last assistant message:
⋮----
// keep the same id when continuing a step:
⋮----
// add the initial stream to the stitchable stream
⋮----
// add an error stream part and close the streams:
⋮----
start(controller)
⋮----
get warnings()
get usage()
get finishReason()
get experimental_providerMetadata()
get providerMetadata()
get text()
get reasoning()
get reasoningDetails()
get sources()
get files()
get toolCalls()
get toolResults()
get request()
get response()
get steps()
/**
Split out a new stream from the original stream.
The original stream is replaced to allow for further splitting,
since we do not know how many times the stream will be split.
Note: this leads to buffering the stream content on the server.
However, the LLM results are expected to be small enough to not cause issues.
   */
private teeStream()
get textStream(): AsyncIterableStream&lt;string&gt;
⋮----
transform(
⋮----
get fullStream(): AsyncIterableStream&lt;TextStreamPart&lt;TOOLS&gt;&gt;
async consumeStream(options?: ConsumeStreamOptions): Promise&lt;void&gt;
get experimental_partialOutputStream(): AsyncIterableStream&lt;PARTIAL_OUTPUT&gt;
private toDataStreamInternal({
    getErrorMessage = () =&gt; &apos;An error occurred.&apos;, // mask error messages for safety by default
    sendUsage = true,
    sendReasoning = false,
    sendSources = false,
    experimental_sendFinish = true,
  }: {
getErrorMessage: ((error: unknown)
⋮----
getErrorMessage = () =&gt; &apos;An error occurred.&apos;, // mask error messages for safety by default
⋮----
pipeDataStreamToResponse(
    response: ServerResponse,
    {
      status,
      statusText,
      headers,
      data,
      getErrorMessage,
      sendUsage,
      sendReasoning,
      sendSources,
      experimental_sendFinish,
    }: ResponseInit &amp;
      DataStreamOptions &amp; {
        data?: StreamData;
getErrorMessage?: (error: unknown)
pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit)
// TODO breaking change 5.0: remove pipeThrough(new TextEncoderStream())
toDataStream(
    options?: DataStreamOptions &amp; {
      data?: StreamData;
getErrorMessage?: (error: unknown)
mergeIntoDataStream(writer: DataStreamWriter, options?: DataStreamOptions)
toDataStreamResponse({
    headers,
    status,
    statusText,
    data,
    getErrorMessage,
    sendUsage,
    sendReasoning,
    sendSources,
    experimental_sendFinish,
  }: ResponseInit &amp;
    DataStreamOptions &amp; {
      data?: StreamData;
getErrorMessage?: (error: unknown)
toTextStreamResponse(init?: ResponseInit): Response</file><file path="packages/ai/core/generate-text/to-response-messages.test.ts">import { z } from &apos;zod&apos;;
import { mockValues } from &apos;../test/mock-values&apos;;
import { tool } from &apos;../tool&apos;;
import { DefaultGeneratedFile } from &apos;./generated-file&apos;;
import { toResponseMessages } from &apos;./to-response-messages&apos;;
⋮----
experimental_toToolResultContent(result)</file><file path="packages/ai/core/generate-text/to-response-messages.ts">import { ToolResultPart } from &apos;../prompt&apos;;
import { GeneratedFile } from &apos;./generated-file&apos;;
import { ReasoningDetail } from &apos;./reasoning-detail&apos;;
import { ResponseMessage } from &apos;./step-result&apos;;
import { ToolCallArray } from &apos;./tool-call&apos;;
import { ToolResultArray } from &apos;./tool-result&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
/**
Converts the result of a `generateText` or `streamText` call to a list of response messages.
 */
export function toResponseMessages&lt;TOOLS extends ToolSet&gt;({
  text = &apos;&apos;,
  files,
  reasoning,
  tools,
  toolCalls,
  toolResults,
  messageId,
  generateMessageId,
}: {
  text: string | undefined;
  files: Array&lt;GeneratedFile&gt;;
  reasoning: Array&lt;ReasoningDetail&gt;;
  tools: TOOLS;
  toolCalls: ToolCallArray&lt;TOOLS&gt;;
  toolResults: ToolResultArray&lt;TOOLS&gt;;
  messageId: string;
generateMessageId: ()
⋮----
// TODO language model v2: switch to order response content (instead of type-based ordering)</file><file path="packages/ai/core/generate-text/tool-call-repair.ts">import { JSONSchema7, LanguageModelV1FunctionToolCall } from &apos;@ai-sdk/provider&apos;;
import { InvalidToolArgumentsError } from &apos;../../errors/invalid-tool-arguments-error&apos;;
import { NoSuchToolError } from &apos;../../errors/no-such-tool-error&apos;;
import { CoreMessage } from &apos;../prompt&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
/**
 * A function that attempts to repair a tool call that failed to parse.
 *
 * It receives the error and the context as arguments and returns the repair
 * tool call JSON as text.
 *
 * @param options.system - The system prompt.
 * @param options.messages - The messages in the current generation step.
 * @param options.toolCall - The tool call that failed to parse.
 * @param options.tools - The tools that are available.
 * @param options.parameterSchema - A function that returns the JSON Schema for a tool.
 * @param options.error - The error that occurred while parsing the tool call.
 */
export type ToolCallRepairFunction&lt;TOOLS extends ToolSet&gt; = (options: {
  system: string | undefined;
  messages: CoreMessage[];
  toolCall: LanguageModelV1FunctionToolCall;
  tools: TOOLS;
  parameterSchema: (options: { toolName: string }) =&gt; JSONSchema7;
  error: NoSuchToolError | InvalidToolArgumentsError;
}) =&gt; Promise&lt;LanguageModelV1FunctionToolCall | null&gt;;</file><file path="packages/ai/core/generate-text/tool-call.ts">import { inferParameters } from &apos;../tool/tool&apos;;
import { ValueOf } from &apos;../util/value-of&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
⋮----
// transforms the tools into a tool call union
export type ToolCallUnion&lt;TOOLS extends ToolSet&gt; = ValueOf&lt;{
  [NAME in keyof TOOLS]: {
    type: &apos;tool-call&apos;;
    toolCallId: string;
    toolName: NAME &amp; string;
    args: inferParameters&lt;TOOLS[NAME][&apos;parameters&apos;]&gt;;
  };
}&gt;;
/**
 * @deprecated Use `ToolCallUnion` instead.
 */
// TODO remove in v5
export type CoreToolCallUnion&lt;TOOLS extends ToolSet&gt; = ToolCallUnion&lt;ToolSet&gt;;
export type ToolCallArray&lt;TOOLS extends ToolSet&gt; = Array&lt;ToolCallUnion&lt;TOOLS&gt;&gt;;</file><file path="packages/ai/core/generate-text/tool-result.ts">import { inferParameters } from &apos;../tool/tool&apos;;
import { ValueOf } from &apos;../util/value-of&apos;;
import { ToolSet } from &apos;./tool-set&apos;;
⋮----
// limits the tools to those with an execute value
type ToToolsWithExecute&lt;TOOLS extends ToolSet&gt; = {
  [K in keyof TOOLS as TOOLS[K] extends { execute: any } ? K : never]: TOOLS[K];
};
// limits the tools to those that have execute !== undefined
export type ToToolsWithDefinedExecute&lt;TOOLS extends ToolSet&gt; = {
  [K in keyof TOOLS as TOOLS[K][&apos;execute&apos;] extends undefined
    ? never
    : K]: TOOLS[K];
};
// transforms the tools into a tool result union
type ToToolResultObject&lt;TOOLS extends ToolSet&gt; = ValueOf&lt;{
  [NAME in keyof TOOLS]: {
    type: &apos;tool-result&apos;;
    toolCallId: string;
    toolName: NAME &amp; string;
    args: inferParameters&lt;TOOLS[NAME][&apos;parameters&apos;]&gt;;
    result: Awaited&lt;ReturnType&lt;Exclude&lt;TOOLS[NAME][&apos;execute&apos;], undefined&gt;&gt;&gt;;
  };
}&gt;;
export type ToolResultUnion&lt;TOOLS extends ToolSet&gt; = ToToolResultObject&lt;
  ToToolsWithDefinedExecute&lt;ToToolsWithExecute&lt;TOOLS&gt;&gt;
&gt;;
/**
 * @deprecated Use `ToolResultUnion` instead.
 */
// TODO remove in v5
export type CoreToolResultUnion&lt;TOOLS extends ToolSet&gt; = ToolResultUnion&lt;TOOLS&gt;;
export type ToolResultArray&lt;TOOLS extends ToolSet&gt; = Array&lt;
  ToolResultUnion&lt;TOOLS&gt;
&gt;;</file><file path="packages/ai/core/generate-text/tool-set.ts">import { Tool } from &apos;../tool&apos;;
export type ToolSet = Record&lt;string, Tool&gt;;</file><file path="packages/ai/core/middleware/default-settings-middleware.test.ts">import { LanguageModelV1CallOptions } from &apos;@ai-sdk/provider&apos;;
import { defaultSettingsMiddleware } from &apos;./default-settings-middleware&apos;;</file><file path="packages/ai/core/middleware/default-settings-middleware.ts">import {
  LanguageModelV1CallOptions,
  LanguageModelV1ProviderMetadata,
} from &apos;@ai-sdk/provider&apos;;
import type { LanguageModelV1Middleware } from &apos;./language-model-v1-middleware&apos;;
import { mergeObjects } from &apos;../util/merge-objects&apos;;
/**
 * Applies default settings for a language model.
 */
export function defaultSettingsMiddleware({
  settings,
}: {
  settings: Partial&lt;
    LanguageModelV1CallOptions &amp; {
      providerMetadata?: LanguageModelV1ProviderMetadata;
    }
  &gt;;
}): LanguageModelV1Middleware
⋮----
// special case for temperature 0
// TODO remove when temperature defaults to undefined</file><file path="packages/ai/core/middleware/extract-reasoning-middleware.test.ts">import {
  convertArrayToReadableStream,
  convertAsyncIterableToArray,
  mockId,
} from &apos;@ai-sdk/provider-utils/test&apos;;
import { generateText, streamText } from &apos;../generate-text&apos;;
import { wrapLanguageModel } from &apos;../middleware/wrap-language-model&apos;;
import { MockLanguageModelV1 } from &apos;../test/mock-language-model-v1&apos;;
import { extractReasoningMiddleware } from &apos;./extract-reasoning-middleware&apos;;
⋮----
async doGenerate()
⋮----
async doStream()</file><file path="packages/ai/core/middleware/extract-reasoning-middleware.ts">import type { LanguageModelV1StreamPart } from &apos;@ai-sdk/provider&apos;;
import { getPotentialStartIndex } from &apos;../util/get-potential-start-index&apos;;
import type { LanguageModelV1Middleware } from &apos;./language-model-v1-middleware&apos;;
/**
 * Extract an XML-tagged reasoning section from the generated text and exposes it
 * as a `reasoning` property on the result.
 *
 * @param tagName - The name of the XML tag to extract reasoning from.
 * @param separator - The separator to use between reasoning and text sections.
 * @param startWithReasoning - Whether to start with reasoning tokens.
 */
export function extractReasoningMiddleware({
  tagName,
  separator = &apos;\n&apos;,
  startWithReasoning = false,
}: {
  tagName: string;
  separator?: string;
  startWithReasoning?: boolean;
}): LanguageModelV1Middleware
⋮----
function publish(text: string)
⋮----
// no opening or closing tag found, publish the buffer
⋮----
// publish text before the tag</file><file path="packages/ai/core/middleware/index.ts"></file><file path="packages/ai/core/middleware/language-model-v1-middleware.ts">import { LanguageModelV1, LanguageModelV1CallOptions } from &apos;@ai-sdk/provider&apos;;
/**
 * Experimental middleware for LanguageModelV1.
 * This type defines the structure for middleware that can be used to modify
 * the behavior of LanguageModelV1 operations.
 */
export type LanguageModelV1Middleware = {
  /**
   * Middleware specification version. Use `v1` for the current version.
   */
  middlewareVersion?: &apos;v1&apos; | undefined; // backwards compatibility
  /**
   * Transforms the parameters before they are passed to the language model.
   * @param options - Object containing the type of operation and the parameters.
   * @param options.type - The type of operation (&apos;generate&apos; or &apos;stream&apos;).
   * @param options.params - The original parameters for the language model call.
   * @returns A promise that resolves to the transformed parameters.
   */
  transformParams?: (options: {
    type: &apos;generate&apos; | &apos;stream&apos;;
    params: LanguageModelV1CallOptions;
  }) =&gt; PromiseLike&lt;LanguageModelV1CallOptions&gt;;
  /**
   * Wraps the generate operation of the language model.
   * @param options - Object containing the generate function, parameters, and model.
   * @param options.doGenerate - The original generate function.
   * @param options.doStream - The original stream function.
   * @param options.params - The parameters for the generate call. If the
   * `transformParams` middleware is used, this will be the transformed parameters.
   * @param options.model - The language model instance.
   * @returns A promise that resolves to the result of the generate operation.
   */
  wrapGenerate?: (options: {
    doGenerate: () =&gt; ReturnType&lt;LanguageModelV1[&apos;doGenerate&apos;]&gt;;
    doStream: () =&gt; ReturnType&lt;LanguageModelV1[&apos;doStream&apos;]&gt;;
    params: LanguageModelV1CallOptions;
    model: LanguageModelV1;
  }) =&gt; Promise&lt;Awaited&lt;ReturnType&lt;LanguageModelV1[&apos;doGenerate&apos;]&gt;&gt;&gt;;
  /**
   * Wraps the stream operation of the language model.
   *
   * @param options - Object containing the stream function, parameters, and model.
   * @param options.doGenerate - The original generate function.
   * @param options.doStream - The original stream function.
   * @param options.params - The parameters for the stream call. If the
   * `transformParams` middleware is used, this will be the transformed parameters.
   * @param options.model - The language model instance.
   * @returns A promise that resolves to the result of the stream operation.
   */
  wrapStream?: (options: {
    doGenerate: () =&gt; ReturnType&lt;LanguageModelV1[&apos;doGenerate&apos;]&gt;;
    doStream: () =&gt; ReturnType&lt;LanguageModelV1[&apos;doStream&apos;]&gt;;
    params: LanguageModelV1CallOptions;
    model: LanguageModelV1;
  }) =&gt; PromiseLike&lt;Awaited&lt;ReturnType&lt;LanguageModelV1[&apos;doStream&apos;]&gt;&gt;&gt;;
};
⋮----
/**
   * Middleware specification version. Use `v1` for the current version.
   */
middlewareVersion?: &apos;v1&apos; | undefined; // backwards compatibility
/**
   * Transforms the parameters before they are passed to the language model.
   * @param options - Object containing the type of operation and the parameters.
   * @param options.type - The type of operation (&apos;generate&apos; or &apos;stream&apos;).
   * @param options.params - The original parameters for the language model call.
   * @returns A promise that resolves to the transformed parameters.
   */
⋮----
/**
   * Wraps the generate operation of the language model.
   * @param options - Object containing the generate function, parameters, and model.
   * @param options.doGenerate - The original generate function.
   * @param options.doStream - The original stream function.
   * @param options.params - The parameters for the generate call. If the
   * `transformParams` middleware is used, this will be the transformed parameters.
   * @param options.model - The language model instance.
   * @returns A promise that resolves to the result of the generate operation.
   */
⋮----
/**
   * Wraps the stream operation of the language model.
   *
   * @param options - Object containing the stream function, parameters, and model.
   * @param options.doGenerate - The original generate function.
   * @param options.doStream - The original stream function.
   * @param options.params - The parameters for the stream call. If the
   * `transformParams` middleware is used, this will be the transformed parameters.
   * @param options.model - The language model instance.
   * @returns A promise that resolves to the result of the stream operation.
   */
⋮----
/**
 * @deprecated Use `LanguageModelV1Middleware` instead.
 */
// TODO remove in v5
export type Experimental_LanguageModelV1Middleware = LanguageModelV1Middleware;</file><file path="packages/ai/core/middleware/simulate-streaming-middleware.test.ts">import {
  convertAsyncIterableToArray,
  mockId,
} from &apos;@ai-sdk/provider-utils/test&apos;;
import { streamText } from &apos;../generate-text&apos;;
import { wrapLanguageModel } from &apos;../middleware/wrap-language-model&apos;;
import { MockLanguageModelV1 } from &apos;../test/mock-language-model-v1&apos;;
import { simulateStreamingMiddleware } from &apos;./simulate-streaming-middleware&apos;;
⋮----
async doGenerate()</file><file path="packages/ai/core/middleware/simulate-streaming-middleware.ts">import type { LanguageModelV1StreamPart } from &apos;@ai-sdk/provider&apos;;
import type { LanguageModelV1Middleware } from &apos;./language-model-v1-middleware&apos;;
/**
 * Simulates streaming chunks with the response from a generate call.
 */
export function simulateStreamingMiddleware(): LanguageModelV1Middleware
⋮----
start(controller)</file><file path="packages/ai/core/middleware/wrap-language-model.test.ts">import { LanguageModelV1, LanguageModelV1CallOptions } from &apos;@ai-sdk/provider&apos;;
import { wrapLanguageModel } from &apos;../middleware/wrap-language-model&apos;;
import { MockLanguageModelV1 } from &apos;../test/mock-language-model-v1&apos;;
⋮----
class MockLanguageModelWithImageSupport implements LanguageModelV1
⋮----
supportsUrl(url: URL)
⋮----
// Reference &apos;this&apos; to verify context
⋮----
// The middlewares should wrap in order, applying wrapGenerate2 last
⋮----
// The middlewares should wrap in order, applying wrapStream2 last</file><file path="packages/ai/core/middleware/wrap-language-model.ts">import { LanguageModelV1, LanguageModelV1CallOptions } from &apos;@ai-sdk/provider&apos;;
import { LanguageModelV1Middleware } from &apos;./language-model-v1-middleware&apos;;
import { asArray } from &apos;../../util/as-array&apos;;
/**
 * Wraps a LanguageModelV1 instance with middleware functionality.
 * This function allows you to apply middleware to transform parameters,
 * wrap generate operations, and wrap stream operations of a language model.
 *
 * @param options - Configuration options for wrapping the language model.
 * @param options.model - The original LanguageModelV1 instance to be wrapped.
 * @param options.middleware - The middleware to be applied to the language model. When multiple middlewares are provided, the first middleware will transform the input first, and the last middleware will be wrapped directly around the model.
 * @param options.modelId - Optional custom model ID to override the original model&apos;s ID.
 * @param options.providerId - Optional custom provider ID to override the original model&apos;s provider.
 * @returns A new LanguageModelV1 instance with middleware applied.
 */
export const wrapLanguageModel = ({
  model,
  middleware: middlewareArg,
  modelId,
  providerId,
}: {
  model: LanguageModelV1;
  middleware: LanguageModelV1Middleware | LanguageModelV1Middleware[];
  modelId?: string;
  providerId?: string;
}): LanguageModelV1 =&gt;
const doWrap = ({
  model,
  middleware: { transformParams, wrapGenerate, wrapStream },
  modelId,
  providerId,
}: {
  model: LanguageModelV1;
  middleware: LanguageModelV1Middleware;
  modelId?: string;
  providerId?: string;
}): LanguageModelV1 =&gt;
⋮----
async function doTransform({
    params,
    type,
  }: {
    params: LanguageModelV1CallOptions;
    type: &apos;generate&apos; | &apos;stream&apos;;
})
⋮----
async doGenerate(
      params: LanguageModelV1CallOptions,
): Promise&lt;Awaited&lt;ReturnType&lt;LanguageModelV1[&apos;doGenerate&apos;]&gt;&gt;&gt;
⋮----
const doGenerate = async ()
const doStream = async ()
⋮----
async doStream(
      params: LanguageModelV1CallOptions,
): Promise&lt;Awaited&lt;ReturnType&lt;LanguageModelV1[&apos;doStream&apos;]&gt;&gt;&gt;
⋮----
/**
 * @deprecated Use `wrapLanguageModel` instead.
 */
// TODO remove in v5</file><file path="packages/ai/core/prompt/append-client-message.test.ts">import { appendClientMessage } from &apos;./append-client-message&apos;;
import { Message } from &apos;@ai-sdk/ui-utils&apos;;</file><file path="packages/ai/core/prompt/append-client-message.ts">import { Message } from &apos;@ai-sdk/ui-utils&apos;;
/**
 * Appends a client message to the messages array.
 * If the last message in the array has the same id as the new message, it will be replaced.
 * Otherwise, the new message will be appended.
 */
export function appendClientMessage({
  messages,
  message,
}: {
  messages: Message[];
  message: Message;
})</file><file path="packages/ai/core/prompt/append-response-messages.test.ts">import { describe, expect, it } from &apos;vitest&apos;;
import { appendResponseMessages } from &apos;./append-response-messages&apos;;</file><file path="packages/ai/core/prompt/append-response-messages.ts">import {
  extractMaxToolInvocationStep,
  FileUIPart,
  Message,
  ReasoningUIPart,
  StepStartUIPart,
  TextUIPart,
  ToolInvocation,
  ToolInvocationUIPart,
} from &apos;@ai-sdk/ui-utils&apos;;
import { ResponseMessage } from &apos;../generate-text/step-result&apos;;
import { convertDataContentToBase64String } from &apos;./data-content&apos;;
import { AISDKError } from &apos;@ai-sdk/provider&apos;;
/**
 * Appends the ResponseMessage[] from the response to a Message[] (for useChat).
 * The messages are converted to Messages before being appended.
 * Timestamps are generated for the new messages.
 *
 * @returns A new Message[] with the response messages appended.
 */
export function appendResponseMessages({
  messages,
  responseMessages,
  _internal: { currentDate = () =&gt; new Date() } = {},
}: {
  messages: Message[];
  responseMessages: ResponseMessage[];
  /**
Internal. For test use only. May change without notice.
     */
  _internal?: {
currentDate?: ()
⋮----
/**
Internal. For test use only. May change without notice.
     */
⋮----
// check if the last message is an assistant message:
⋮----
function getToolInvocations(step: number)
⋮----
&gt; = [{ type: &apos;step-start&apos; as const }]; // always start with a step-start part
⋮----
reasoningPart = undefined; // reset the reasoning part
⋮----
// last message was a user message, add the assistant message:
⋮----
createdAt: currentDate(), // generate a createdAt date for the message, will be overridden by the client
⋮----
// for tool call results, add the result to previous message:
lastMessage.toolInvocations ??= []; // ensure the toolInvocations array exists
⋮----
// find the tool call in the previous message:
⋮----
// add the result to the tool call:</file><file path="packages/ai/core/prompt/attachments-to-parts.ts">import { Attachment } from &apos;@ai-sdk/ui-utils&apos;;
import { FilePart, ImagePart, TextPart } from &apos;./content-part&apos;;
import {
  convertDataContentToUint8Array,
  convertUint8ArrayToText,
} from &apos;./data-content&apos;;
type ContentPart = TextPart | ImagePart | FilePart;
/**
 * Converts a list of attachments to a list of content parts
 * for consumption by `ai/core` functions.
 * Currently only supports images and text attachments.
 */
export function attachmentsToParts(attachments: Attachment[]): ContentPart[]</file><file path="packages/ai/core/prompt/call-settings.ts">export type CallSettings = {
  /**
Maximum number of tokens to generate.
   */
  maxTokens?: number;
  /**
Temperature setting. This is a number between 0 (almost no randomness) and
1 (very random).
It is recommended to set either `temperature` or `topP`, but not both.
@default 0
   */
  temperature?: number;
  /**
Nucleus sampling. This is a number between 0 and 1.
E.g. 0.1 would mean that only tokens with the top 10% probability mass
are considered.
It is recommended to set either `temperature` or `topP`, but not both.
   */
  topP?: number;
  /**
Only sample from the top K options for each subsequent token.
Used to remove &quot;long tail&quot; low probability responses.
Recommended for advanced use cases only. You usually only need to use temperature.
   */
  topK?: number;
  /**
Presence penalty setting. It affects the likelihood of the model to
repeat information that is already in the prompt.
The presence penalty is a number between -1 (increase repetition)
and 1 (maximum penalty, decrease repetition). 0 means no penalty.
   */
  presencePenalty?: number;
  /**
Frequency penalty setting. It affects the likelihood of the model
to repeatedly use the same words or phrases.
The frequency penalty is a number between -1 (increase repetition)
and 1 (maximum penalty, decrease repetition). 0 means no penalty.
   */
  frequencyPenalty?: number;
  /**
Stop sequences.
If set, the model will stop generating text when one of the stop sequences is generated.
Providers may have limits on the number of stop sequences.
   */
  stopSequences?: string[];
  /**
The seed (integer) to use for random sampling. If set and supported
by the model, calls will generate deterministic results.
   */
  seed?: number;
  /**
Maximum number of retries. Set to 0 to disable retries.
@default 2
   */
  maxRetries?: number;
  /**
Abort signal.
   */
  abortSignal?: AbortSignal;
  /**
Additional HTTP headers to be sent with the request.
Only applicable for HTTP-based providers.
   */
  headers?: Record&lt;string, string | undefined&gt;;
};
⋮----
/**
Maximum number of tokens to generate.
   */
⋮----
/**
Temperature setting. This is a number between 0 (almost no randomness) and
1 (very random).
It is recommended to set either `temperature` or `topP`, but not both.
@default 0
   */
⋮----
/**
Nucleus sampling. This is a number between 0 and 1.
E.g. 0.1 would mean that only tokens with the top 10% probability mass
are considered.
It is recommended to set either `temperature` or `topP`, but not both.
   */
⋮----
/**
Only sample from the top K options for each subsequent token.
Used to remove &quot;long tail&quot; low probability responses.
Recommended for advanced use cases only. You usually only need to use temperature.
   */
⋮----
/**
Presence penalty setting. It affects the likelihood of the model to
repeat information that is already in the prompt.
The presence penalty is a number between -1 (increase repetition)
and 1 (maximum penalty, decrease repetition). 0 means no penalty.
   */
⋮----
/**
Frequency penalty setting. It affects the likelihood of the model
to repeatedly use the same words or phrases.
The frequency penalty is a number between -1 (increase repetition)
and 1 (maximum penalty, decrease repetition). 0 means no penalty.
   */
⋮----
/**
Stop sequences.
If set, the model will stop generating text when one of the stop sequences is generated.
Providers may have limits on the number of stop sequences.
   */
⋮----
/**
The seed (integer) to use for random sampling. If set and supported
by the model, calls will generate deterministic results.
   */
⋮----
/**
Maximum number of retries. Set to 0 to disable retries.
@default 2
   */
⋮----
/**
Abort signal.
   */
⋮----
/**
Additional HTTP headers to be sent with the request.
Only applicable for HTTP-based providers.
   */</file><file path="packages/ai/core/prompt/content-part.ts">import { z } from &apos;zod&apos;;
import {
  ProviderMetadata,
  providerMetadataSchema,
  ProviderOptions,
} from &apos;../types/provider-metadata&apos;;
import { DataContent, dataContentSchema } from &apos;./data-content&apos;;
import {
  ToolResultContent,
  toolResultContentSchema,
} from &apos;./tool-result-content&apos;;
/**
Text content part of a prompt. It contains a string of text.
 */
export interface TextPart {
  type: &apos;text&apos;;
  /**
The text content.
   */
  text: string;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
 */
  experimental_providerMetadata?: ProviderMetadata;
}
⋮----
/**
The text content.
   */
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
 */
⋮----
/**
@internal
 */
⋮----
/**
Image content part of a prompt. It contains an image.
 */
export interface ImagePart {
  type: &apos;image&apos;;
  /**
Image data. Can either be:
- data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer
- URL: a URL that points to the image
   */
  image: DataContent | URL;
  /**
Optional mime type of the image.
   */
  mimeType?: string;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
 */
  experimental_providerMetadata?: ProviderMetadata;
}
⋮----
/**
Image data. Can either be:
- data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer
- URL: a URL that points to the image
   */
⋮----
/**
Optional mime type of the image.
   */
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
 */
⋮----
/**
@internal
 */
⋮----
/**
File content part of a prompt. It contains a file.
 */
export interface FilePart {
  type: &apos;file&apos;;
  /**
File data. Can either be:
- data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer
- URL: a URL that points to the image
   */
  data: DataContent | URL;
  /**
Optional filename of the file.
   */
  filename?: string;
  /**
Mime type of the file.
   */
  mimeType: string;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
 */
  experimental_providerMetadata?: ProviderMetadata;
}
⋮----
/**
File data. Can either be:
- data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer
- URL: a URL that points to the image
   */
⋮----
/**
Optional filename of the file.
   */
⋮----
/**
Mime type of the file.
   */
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
 */
⋮----
/**
@internal
 */
⋮----
/**
 * Reasoning content part of a prompt. It contains a reasoning.
 */
export interface ReasoningPart {
  type: &apos;reasoning&apos;;
  /**
The reasoning text.
   */
  text: string;
  /**
An optional signature for verifying that the reasoning originated from the model.
   */
  signature?: string;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
 */
  experimental_providerMetadata?: ProviderMetadata;
}
⋮----
/**
The reasoning text.
   */
⋮----
/**
An optional signature for verifying that the reasoning originated from the model.
   */
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
 */
⋮----
/**
@internal
 */
⋮----
/**
Redacted reasoning content part of a prompt.
 */
export interface RedactedReasoningPart {
  type: &apos;redacted-reasoning&apos;;
  /**
Redacted reasoning data.
   */
  data: string;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
 */
  experimental_providerMetadata?: ProviderMetadata;
}
⋮----
/**
Redacted reasoning data.
   */
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
 */
⋮----
/**
@internal
 */
⋮----
/**
Tool call content part of a prompt. It contains a tool call (usually generated by the AI model).
 */
export interface ToolCallPart {
  type: &apos;tool-call&apos;;
  /**
ID of the tool call. This ID is used to match the tool call with the tool result.
 */
  toolCallId: string;
  /**
Name of the tool that is being called.
 */
  toolName: string;
  /**
Arguments of the tool call. This is a JSON-serializable object that matches the tool&apos;s input schema.
   */
  args: unknown;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
 */
  experimental_providerMetadata?: ProviderMetadata;
}
⋮----
/**
ID of the tool call. This ID is used to match the tool call with the tool result.
 */
⋮----
/**
Name of the tool that is being called.
 */
⋮----
/**
Arguments of the tool call. This is a JSON-serializable object that matches the tool&apos;s input schema.
   */
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
 */
⋮----
/**
@internal
 */
⋮----
}) as z.ZodType&lt;ToolCallPart&gt;; // necessary bc args is optional on Zod type
/**
Tool result content part of a prompt. It contains the result of the tool call with the matching ID.
 */
export interface ToolResultPart {
  type: &apos;tool-result&apos;;
  /**
ID of the tool call that this result is associated with.
 */
  toolCallId: string;
  /**
Name of the tool that generated this result.
  */
  toolName: string;
  /**
Result of the tool call. This is a JSON-serializable object.
   */
  result: unknown;
  /**
Multi-part content of the tool result. Only for tools that support multipart results.
   */
  experimental_content?: ToolResultContent;
  /**
Optional flag if the result is an error or an error message.
   */
  isError?: boolean;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
 */
  experimental_providerMetadata?: ProviderMetadata;
}
⋮----
/**
ID of the tool call that this result is associated with.
 */
⋮----
/**
Name of the tool that generated this result.
  */
⋮----
/**
Result of the tool call. This is a JSON-serializable object.
   */
⋮----
/**
Multi-part content of the tool result. Only for tools that support multipart results.
   */
⋮----
/**
Optional flag if the result is an error or an error message.
   */
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
 */
⋮----
/**
@internal
 */
⋮----
}) as z.ZodType&lt;ToolResultPart&gt;; // necessary bc result is optional on Zod type</file><file path="packages/ai/core/prompt/convert-to-core-messages.test.ts">import { Attachment, Message } from &apos;@ai-sdk/ui-utils&apos;;
import { convertToCoreMessages } from &apos;./convert-to-core-messages&apos;;
import { tool } from &apos;../tool/tool&apos;;
import { z } from &apos;zod&apos;;
import { CoreMessage } from &apos;./message&apos;;
⋮----
content: &apos;&apos;, // empty content
⋮----
content: &apos;&apos;, // empty content
⋮----
content: &apos;&apos;, // empty content
⋮----
content: &apos;&apos;, // empty content
⋮----
content: &apos;&apos;, // empty content
toolInvocations: [], // empty invocations
⋮----
{ tools }, // separate tools to ensure that types are inferred correctly
⋮----
content: &apos;&apos;, // empty content
toolInvocations: [], // empty invocations
⋮----
{ tools }, // separate tools to ensure that types are inferred correctly
⋮----
content: &apos;&apos;, // empty content
⋮----
{ tools }, // separate tools to ensure that types are inferred correctly
⋮----
content: &apos;&apos;, // empty content
toolInvocations: [], // empty invocations
⋮----
{ tools }, // separate tools to ensure that types are inferred correctly
⋮----
content: &apos;&apos;, // empty content
toolInvocations: [], // empty invocations
⋮----
{ tools }, // separate tools to ensure that types are inferred correctly
⋮----
{ tools }, // separate tools to ensure that types are inferred correctly
⋮----
{ tools }, // separate tools to ensure that types are inferred correctly</file><file path="packages/ai/core/prompt/convert-to-core-messages.ts">import {
  FileUIPart,
  Message,
  ReasoningUIPart,
  TextUIPart,
  ToolInvocationUIPart,
} from &apos;@ai-sdk/ui-utils&apos;;
import { ToolSet } from &apos;../generate-text/tool-set&apos;;
import {
  AssistantContent,
  CoreMessage,
  ToolCallPart,
  ToolResultPart,
} from &apos;../prompt&apos;;
import { attachmentsToParts } from &apos;./attachments-to-parts&apos;;
import { MessageConversionError } from &apos;./message-conversion-error&apos;;
/**
Converts an array of messages from useChat into an array of CoreMessages that can be used
with the AI core functions (e.g. `streamText`).
 */
export function convertToCoreMessages&lt;TOOLS extends ToolSet = never&gt;(
  messages: Array&lt;Omit&lt;Message, &apos;id&apos;&gt;&gt;,
  options?: { tools?: TOOLS },
)
⋮----
function processBlock()
⋮----
// check if there are tool invocations with results in the block
⋮----
// tool message with tool results
⋮----
// updates for next block
⋮----
processBlock(); // text must come before tool invocations
⋮----
// assistant message with tool calls
⋮----
// tool message with tool results
⋮----
// ignore</file><file path="packages/ai/core/prompt/convert-to-language-model-prompt.test.ts">import { convertUint8ArrayToBase64 } from &apos;@ai-sdk/provider-utils&apos;;
import {
  convertToLanguageModelMessage,
  convertToLanguageModelPrompt,
} from &apos;./convert-to-language-model-prompt&apos;;
⋮----
const base64Data = &apos;SGVsbG8sIFdvcmxkIQ==&apos;; // &quot;Hello, World!&quot; in base64
⋮----
const uint8Data = new Uint8Array([72, 101, 108, 108, 111]); // &quot;Hello&quot; in ASCII
⋮----
data: &apos;SGVsbG8=&apos;, // base64 encoded &quot;Hello&quot;
⋮----
data: &apos;SGVsbG8sIFdvcmxkIQ==&apos;, // &quot;Hello, World!&quot; in base64
⋮----
// incorrect mimetype:
⋮----
data: &apos;dGVzdA==&apos;, // &quot;test&quot; in base64</file><file path="packages/ai/core/prompt/convert-to-language-model-prompt.ts">import {
  LanguageModelV1FilePart,
  LanguageModelV1ImagePart,
  LanguageModelV1Message,
  LanguageModelV1Prompt,
  LanguageModelV1TextPart,
} from &apos;@ai-sdk/provider&apos;;
import { download } from &apos;../../util/download&apos;;
import { CoreMessage } from &apos;../prompt/message&apos;;
import {
  detectMimeType,
  imageMimeTypeSignatures,
} from &apos;../util/detect-mimetype&apos;;
import { FilePart, ImagePart, TextPart } from &apos;./content-part&apos;;
import {
  convertDataContentToBase64String,
  convertDataContentToUint8Array,
  DataContent,
} from &apos;./data-content&apos;;
import { InvalidMessageRoleError } from &apos;./invalid-message-role-error&apos;;
import { splitDataUrl } from &apos;./split-data-url&apos;;
import { StandardizedPrompt } from &apos;./standardize-prompt&apos;;
export async function convertToLanguageModelPrompt({
  prompt,
  modelSupportsImageUrls = true,
  modelSupportsUrl = () =&gt; false,
  downloadImplementation = download,
}: {
  prompt: StandardizedPrompt;
  modelSupportsImageUrls: boolean | undefined;
modelSupportsUrl: undefined | ((url: URL)
/**
 * Convert a CoreMessage to a LanguageModelV1Message.
 *
 * @param message The CoreMessage to convert.
 * @param downloadedAssets A map of URLs to their downloaded data. Only
 *   available if the model does not support URLs, null otherwise.
 */
export function convertToLanguageModelMessage(
  message: CoreMessage,
  downloadedAssets: Record&lt;
    string,
    { mimeType: string | undefined; data: Uint8Array }
  &gt;,
): LanguageModelV1Message
⋮----
// remove empty text parts:
⋮----
// remove empty text parts:
⋮----
/**
 * Downloads images and files from URLs in the messages.
 */
async function downloadAssets(
  messages: CoreMessage[],
  downloadImplementation: typeof download,
  modelSupportsImageUrls: boolean | undefined,
  modelSupportsUrl: (url: URL) =&gt; boolean,
): Promise&lt;Record&lt;string,
⋮----
/**
     * Filter out image parts if the model supports image URLs, before letting it
     * decide if it supports a particular URL.
     */
⋮----
// support string urls:
⋮----
/**
     * Filter out URLs that the model supports natively, so we don&apos;t download them.
     */
⋮----
// download in parallel:
⋮----
/**
 * Convert part of a message to a LanguageModelV1Part.
 * @param part The part to convert.
 * @param downloadedAssets A map of URLs to their downloaded data. Only
 *  available if the model does not support URLs, null otherwise.
 *
 * @returns The converted part.
 */
function convertPartToLanguageModelPart(
  part: TextPart | ImagePart | FilePart,
  downloadedAssets: Record&lt;
    string,
    { mimeType: string | undefined; data: Uint8Array }
  &gt;,
):
  | LanguageModelV1TextPart
  | LanguageModelV1ImagePart
  | LanguageModelV1FilePart {
if (part.type === &apos;text&apos;)
⋮----
// Attempt to create a URL from the data. If it fails, we can assume the data
// is not a URL and likely some other sort of data.
⋮----
// If we successfully created a URL, we can use that to normalize the data
// either by passing it through or converting normalizing the base64 content
// to a Uint8Array.
⋮----
// If the content is a data URL, we want to convert that to a Uint8Array
⋮----
/**
       * If the content is a URL, we should first see if it was downloaded. And if not,
       * we can let the model decide if it wants to support the URL. This also allows
       * for non-HTTP URLs to be passed through (e.g. gs://).
       */
⋮----
// Since we know now the content is not a URL, we can attempt to normalize
// the data assuming it is some sort of data.
⋮----
// Now that we have the normalized data either as a URL or a Uint8Array,
// we can create the LanguageModelV1Part.
⋮----
// When possible, try to detect the mimetype automatically
// to deal with incorrect mimetype inputs.
// When detection fails, use provided mimetype.
⋮----
// We should have a mimeType at this point, if not, throw an error.</file><file path="packages/ai/core/prompt/data-content.test.ts">import { dataContentSchema } from &apos;./data-content&apos;;</file><file path="packages/ai/core/prompt/data-content.ts">import {
  convertBase64ToUint8Array,
  convertUint8ArrayToBase64,
} from &apos;@ai-sdk/provider-utils&apos;;
import { InvalidDataContentError } from &apos;./invalid-data-content-error&apos;;
import { z } from &apos;zod&apos;;
/**
Data content. Can either be a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer.
 */
export type DataContent = string | Uint8Array | ArrayBuffer | Buffer;
/**
@internal
 */
⋮----
// Buffer might not be available in some environments such as CloudFlare:
⋮----
/**
Converts data content to a base64-encoded string.
@param content - Data content to convert.
@returns Base64-encoded string.
*/
export function convertDataContentToBase64String(content: DataContent): string
/**
Converts data content to a Uint8Array.
@param content - Data content to convert.
@returns Uint8Array.
 */
export function convertDataContentToUint8Array(
  content: DataContent,
): Uint8Array
/**
 * Converts a Uint8Array to a string of text.
 *
 * @param uint8Array - The Uint8Array to convert.
 * @returns The converted string.
 */
export function convertUint8ArrayToText(uint8Array: Uint8Array): string</file><file path="packages/ai/core/prompt/index.ts"></file><file path="packages/ai/core/prompt/invalid-data-content-error.ts">import { AISDKError } from &apos;@ai-sdk/provider&apos;;
⋮----
export class InvalidDataContentError extends AISDKError
⋮----
private readonly [symbol] = true; // used in isInstance
⋮----
constructor({
    content,
    cause,
    message = `Invalid data content. Expected a base64 string, Uint8Array, ArrayBuffer, or Buffer, but got ${typeof content}.`,
  }: {
    content: unknown;
    cause?: unknown;
    message?: string;
})
static isInstance(error: unknown): error is InvalidDataContentError</file><file path="packages/ai/core/prompt/invalid-message-role-error.ts">import { AISDKError } from &apos;@ai-sdk/provider&apos;;
⋮----
export class InvalidMessageRoleError extends AISDKError
⋮----
private readonly [symbol] = true; // used in isInstance
⋮----
constructor({
    role,
    message = `Invalid message role: &apos;${role}&apos;. Must be one of: &quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;, &quot;tool&quot;.`,
  }: {
    role: string;
    message?: string;
})
static isInstance(error: unknown): error is InvalidMessageRoleError</file><file path="packages/ai/core/prompt/message-conversion-error.ts">import { AISDKError } from &apos;@ai-sdk/provider&apos;;
import { Message } from &apos;@ai-sdk/ui-utils&apos;;
⋮----
export class MessageConversionError extends AISDKError
⋮----
private readonly [symbol] = true; // used in isInstance
⋮----
constructor({
    originalMessage,
    message,
  }: {
    originalMessage: Omit&lt;Message, &apos;id&apos;&gt;;
    message: string;
})
static isInstance(error: unknown): error is MessageConversionError</file><file path="packages/ai/core/prompt/message.ts">import { z } from &apos;zod&apos;;
import { ProviderMetadata } from &apos;../types&apos;;
import {
  providerMetadataSchema,
  ProviderOptions,
} from &apos;../types/provider-metadata&apos;;
import {
  FilePart,
  filePartSchema,
  ImagePart,
  imagePartSchema,
  ReasoningPart,
  reasoningPartSchema,
  RedactedReasoningPart,
  redactedReasoningPartSchema,
  TextPart,
  textPartSchema,
  ToolCallPart,
  toolCallPartSchema,
  ToolResultPart,
  toolResultPartSchema,
} from &apos;./content-part&apos;;
/**
 A system message. It can contain system information.
 Note: using the &quot;system&quot; part of the prompt is strongly preferred
 to increase the resilience against prompt injection attacks,
 and because not all providers support several system messages.
 */
export type CoreSystemMessage = {
  role: &apos;system&apos;;
  content: string;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
 */
  experimental_providerMetadata?: ProviderMetadata;
};
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
 */
⋮----
/**
A user message. It can contain text or a combination of text and images.
 */
export type CoreUserMessage = {
  role: &apos;user&apos;;
  content: UserContent;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
*/
  experimental_providerMetadata?: ProviderMetadata;
};
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
Content of a user message. It can be a string or an array of text and image parts.
 */
export type UserContent = string | Array&lt;TextPart | ImagePart | FilePart&gt;;
/**
An assistant message. It can contain text, tool calls, or a combination of text and tool calls.
 */
export type CoreAssistantMessage = {
  role: &apos;assistant&apos;;
  content: AssistantContent;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
*/
  experimental_providerMetadata?: ProviderMetadata;
};
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
Content of an assistant message.
It can be a string or an array of text, image, reasoning, redacted reasoning, and tool call parts.
 */
export type AssistantContent =
  | string
  | Array&lt;
      TextPart | FilePart | ReasoningPart | RedactedReasoningPart | ToolCallPart
    &gt;;
/**
A tool message. It contains the result of one or more tool calls.
 */
export type CoreToolMessage = {
  role: &apos;tool&apos;;
  content: ToolContent;
  /**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
  providerOptions?: ProviderOptions;
  /**
@deprecated Use `providerOptions` instead.
*/
  experimental_providerMetadata?: ProviderMetadata;
};
⋮----
/**
Additional provider-specific metadata. They are passed through
to the provider from the AI SDK and enable provider-specific
functionality that can be fully encapsulated in the provider.
 */
⋮----
/**
@deprecated Use `providerOptions` instead.
*/
⋮----
/**
Content of a tool message. It is an array of tool result parts.
 */
export type ToolContent = Array&lt;ToolResultPart&gt;;
/**
A message that can be used in the `messages` field of a prompt.
It can be a user message, an assistant message, or a tool message.
 */
export type CoreMessage =
  | CoreSystemMessage
  | CoreUserMessage
  | CoreAssistantMessage
  | CoreToolMessage;</file><file path="packages/ai/core/prompt/prepare-call-settings.test.ts">import { expect, it } from &apos;vitest&apos;;
import { prepareCallSettings } from &apos;./prepare-call-settings&apos;;</file><file path="packages/ai/core/prompt/prepare-call-settings.ts">import { InvalidArgumentError } from &apos;../../errors/invalid-argument-error&apos;;
import { CallSettings } from &apos;./call-settings&apos;;
/**
 * Validates call settings and sets default values.
 */
export function prepareCallSettings({
  maxTokens,
  temperature,
  topP,
  topK,
  presencePenalty,
  frequencyPenalty,
  stopSequences,
  seed,
}: Omit&lt;CallSettings, &apos;abortSignal&apos; | &apos;headers&apos; | &apos;maxRetries&apos;&gt;): Omit&lt;
  CallSettings,
  &apos;abortSignal&apos; | &apos;headers&apos; | &apos;maxRetries&apos;
&gt; {
if (maxTokens != null)
⋮----
// TODO v5 remove default 0 for temperature</file><file path="packages/ai/core/prompt/prepare-retries.test.ts">import { expect, it } from &apos;vitest&apos;;
import { prepareRetries } from &apos;./prepare-retries&apos;;</file><file path="packages/ai/core/prompt/prepare-retries.ts">import { InvalidArgumentError } from &apos;../../errors/invalid-argument-error&apos;;
import {
  RetryFunction,
  retryWithExponentialBackoff,
} from &apos;../../util/retry-with-exponential-backoff&apos;;
/**
 * Validate and prepare retries.
 */
export function prepareRetries({
  maxRetries,
}: {
  maxRetries: number | undefined;
}):</file><file path="packages/ai/core/prompt/prepare-tools-and-tool-choice.test.ts">import { z } from &apos;zod&apos;;
import { ToolSet } from &apos;../generate-text/tool-set&apos;;
import { Tool, tool } from &apos;../tool/tool&apos;;
import { prepareToolsAndToolChoice } from &apos;./prepare-tools-and-tool-choice&apos;;</file><file path="packages/ai/core/prompt/prepare-tools-and-tool-choice.ts">import {
  LanguageModelV1FunctionTool,
  LanguageModelV1ProviderDefinedTool,
  LanguageModelV1ToolChoice,
} from &apos;@ai-sdk/provider&apos;;
import { asSchema } from &apos;@ai-sdk/ui-utils&apos;;
import { ToolSet } from &apos;../generate-text&apos;;
import { ToolChoice } from &apos;../types/language-model&apos;;
import { isNonEmptyObject } from &apos;../util/is-non-empty-object&apos;;
export function prepareToolsAndToolChoice&lt;TOOLS extends ToolSet&gt;({
  tools,
  toolChoice,
  activeTools,
}: {
  tools: TOOLS | undefined;
  toolChoice: ToolChoice&lt;TOOLS&gt; | undefined;
  activeTools: Array&lt;keyof TOOLS&gt; | undefined;
}):
⋮----
// when activeTools is provided, we only include the tools that are in the list:</file><file path="packages/ai/core/prompt/prompt.ts">import { Message } from &apos;@ai-sdk/ui-utils&apos;;
import { CoreMessage } from &apos;./message&apos;;
/**
Prompt part of the AI function options.
It contains a system message, a simple text prompt, or a list of messages.
 */
export type Prompt = {
  /**
System message to include in the prompt. Can be used with `prompt` or `messages`.
   */
  system?: string;
  /**
A simple text prompt. You can either use `prompt` or `messages` but not both.
 */
  prompt?: string;
  /**
A list of messages. You can either use `prompt` or `messages` but not both.
   */
  messages?: Array&lt;CoreMessage&gt; | Array&lt;Omit&lt;Message, &apos;id&apos;&gt;&gt;;
};
⋮----
/**
System message to include in the prompt. Can be used with `prompt` or `messages`.
   */
⋮----
/**
A simple text prompt. You can either use `prompt` or `messages` but not both.
 */
⋮----
/**
A list of messages. You can either use `prompt` or `messages` but not both.
   */</file><file path="packages/ai/core/prompt/split-data-url.ts">export function splitDataUrl(dataUrl: string):</file><file path="packages/ai/core/prompt/standardize-prompt.test.ts">import { Message } from &apos;@ai-sdk/ui-utils&apos;;
import { standardizePrompt } from &apos;./standardize-prompt&apos;;
import { CoreMessage } from &apos;./message&apos;;</file><file path="packages/ai/core/prompt/standardize-prompt.ts">import { InvalidPromptError } from &apos;@ai-sdk/provider&apos;;
import { safeValidateTypes } from &apos;@ai-sdk/provider-utils&apos;;
import { Message } from &apos;@ai-sdk/ui-utils&apos;;
import { z } from &apos;zod&apos;;
import { ToolSet } from &apos;../generate-text/tool-set&apos;;
import { convertToCoreMessages } from &apos;./convert-to-core-messages&apos;;
import { CoreMessage, coreMessageSchema } from &apos;./message&apos;;
import { Prompt } from &apos;./prompt&apos;;
export type StandardizedPrompt = {
  /**
   * Original prompt type. This is forwarded to the providers and can be used
   * to write send raw text to providers that support it.
   */
  type: &apos;prompt&apos; | &apos;messages&apos;;
  /**
   * System message.
   */
  system?: string;
  /**
   * Messages.
   */
  messages: CoreMessage[];
};
⋮----
/**
   * Original prompt type. This is forwarded to the providers and can be used
   * to write send raw text to providers that support it.
   */
⋮----
/**
   * System message.
   */
⋮----
/**
   * Messages.
   */
⋮----
export function standardizePrompt&lt;TOOLS extends ToolSet&gt;({
  prompt,
  tools,
}: {
  prompt: Prompt;
  tools: undefined | TOOLS;
}): StandardizedPrompt
⋮----
// validate that system is a string
⋮----
// type: prompt
⋮----
// validate that prompt is a string
⋮----
// type: messages
⋮----
function detectPromptType(
  prompt: Array&lt;any&gt;,
): &apos;ui-messages&apos; | &apos;messages&apos; | &apos;other&apos;
function detectSingleMessageCharacteristics(
  message: any,
): &apos;has-ui-specific-parts&apos; | &apos;has-core-specific-parts&apos; | &apos;message&apos; | &apos;other&apos;
⋮----
(message.role === &apos;function&apos; || // UI-only role
message.role === &apos;data&apos; || // UI-only role
&apos;toolInvocations&apos; in message || // UI-specific field
&apos;parts&apos; in message || // UI-specific field
⋮----
(Array.isArray(message.content) || // Core messages can have array content</file><file path="packages/ai/core/prompt/stringify-for-telemetry.test.ts">import { describe, expect, it } from &apos;vitest&apos;;
import { stringifyForTelemetry } from &apos;./stringify-for-telemetry&apos;;
import { LanguageModelV1Prompt } from &apos;@ai-sdk/provider&apos;;
⋮----
// https://cryptii.com/pipes/binary-to-base64 with 010203 in hex format input
⋮----
// https://cryptii.com/pipes/binary-to-base64 with 010203 in hex format input
⋮----
// We expect the URL to be preserved as is
⋮----
// https://cryptii.com/pipes/binary-to-base64 with 010203 in hex format input</file><file path="packages/ai/core/prompt/stringify-for-telemetry.ts">/**
 * Helper utility to serialize prompt content for OpenTelemetry tracing.
 * It is initially created because normalized LanguageModelV1Prompt carries
 * images as Uint8Arrays, on which JSON.stringify acts weirdly, converting
 * them to objects with stringified indices as keys, e.g. {&quot;0&quot;: 42, &quot;1&quot;: 69 }.
 */
import {
  LanguageModelV1ImagePart,
  LanguageModelV1Message,
  LanguageModelV1Prompt,
  LanguageModelV1ProviderMetadata,
} from &apos;@ai-sdk/provider&apos;;
import { convertDataContentToBase64String } from &apos;./data-content&apos;;
export function stringifyForTelemetry(prompt: LanguageModelV1Prompt): string
type MessageContentPart = Exclude&lt;
  LanguageModelV1Message[&apos;content&apos;],
  string
&gt;[number];
type ProcessedMessageContentPart =
  | Exclude&lt;MessageContentPart, LanguageModelV1ImagePart&gt;
  | {
      type: &apos;image&apos;;
      image: string | URL;
      mimeType?: string;
      providerMetadata?: LanguageModelV1ProviderMetadata;
    };
function processPart(part: MessageContentPart): ProcessedMessageContentPart</file><file path="packages/ai/core/prompt/tool-result-content.ts">import { z } from &apos;zod&apos;;
export type ToolResultContent = Array&lt;
  | {
      type: &apos;text&apos;;
      text: string;
    }
  | {
      type: &apos;image&apos;;
      data: string; // base64 encoded png image, e.g. screenshot
      mimeType?: string; // e.g. &apos;image/png&apos;;
    }
&gt;;
⋮----
data: string; // base64 encoded png image, e.g. screenshot
mimeType?: string; // e.g. &apos;image/png&apos;;
⋮----
export function isToolResultContent(
  value: unknown,
): value is ToolResultContent</file><file path="packages/ai/core/registry/custom-provider.test.ts">import { NoSuchModelError } from &apos;@ai-sdk/provider&apos;;
import { describe, expect, it, vi } from &apos;vitest&apos;;
import { MockEmbeddingModelV1 } from &apos;../test/mock-embedding-model-v1&apos;;
import { MockImageModelV1 } from &apos;../test/mock-image-model-v1&apos;;
import { MockLanguageModelV1 } from &apos;../test/mock-language-model-v1&apos;;
import { customProvider } from &apos;./custom-provider&apos;;</file><file path="packages/ai/core/registry/custom-provider.ts">import { NoSuchModelError, ProviderV1 } from &apos;@ai-sdk/provider&apos;;
import { EmbeddingModel, ImageModel, LanguageModel, Provider } from &apos;../types&apos;;
/**
 * Creates a custom provider with specified language models, text embedding models, and an optional fallback provider.
 *
 * @param {Object} options - The options for creating the custom provider.
 * @param {Record&lt;string, LanguageModel&gt;} [options.languageModels] - A record of language models, where keys are model IDs and values are LanguageModel instances.
 * @param {Record&lt;string, EmbeddingModel&lt;string&gt;&gt;} [options.textEmbeddingModels] - A record of text embedding models, where keys are model IDs and values are EmbeddingModel&lt;string&gt; instances.
 * @param {Record&lt;string, ImageModel&gt;} [options.imageModels] - A record of image models, where keys are model IDs and values are ImageModel instances.
 * @param {Provider} [options.fallbackProvider] - An optional fallback provider to use when a requested model is not found in the custom provider.
 * @returns {Provider} A Provider object with languageModel, textEmbeddingModel, and imageModel methods.
 *
 * @throws {NoSuchModelError} Throws when a requested model is not found and no fallback provider is available.
 */
export function customProvider&lt;
  LANGUAGE_MODELS extends Record&lt;string, LanguageModel&gt;,
  EMBEDDING_MODELS extends Record&lt;string, EmbeddingModel&lt;string&gt;&gt;,
  IMAGE_MODELS extends Record&lt;string, ImageModel&gt;,
&gt;({
  languageModels,
  textEmbeddingModels,
  imageModels,
  fallbackProvider,
}: {
  languageModels?: LANGUAGE_MODELS;
  textEmbeddingModels?: EMBEDDING_MODELS;
  imageModels?: IMAGE_MODELS;
  fallbackProvider?: ProviderV1;
}): Provider &amp;
⋮----
languageModel(modelId: ExtractModelId&lt;LANGUAGE_MODELS&gt;): LanguageModel;
textEmbeddingModel(
    modelId: ExtractModelId&lt;EMBEDDING_MODELS&gt;,
  ): EmbeddingModel&lt;string&gt;;
imageModel(modelId: ExtractModelId&lt;IMAGE_MODELS&gt;): ImageModel;
⋮----
languageModel(modelId: ExtractModelId&lt;LANGUAGE_MODELS&gt;): LanguageModel
textEmbeddingModel(
      modelId: ExtractModelId&lt;EMBEDDING_MODELS&gt;,
): EmbeddingModel&lt;string&gt;
imageModel(modelId: ExtractModelId&lt;IMAGE_MODELS&gt;): ImageModel
⋮----
/**
 * @deprecated Use `customProvider` instead.
 */
⋮----
type ExtractModelId&lt;MODELS extends Record&lt;string, unknown&gt;&gt; = Extract&lt;
  keyof MODELS,
  string
&gt;;</file><file path="packages/ai/core/registry/index.ts"></file><file path="packages/ai/core/registry/no-such-provider-error.ts">import { AISDKError, NoSuchModelError } from &apos;@ai-sdk/provider&apos;;
⋮----
export class NoSuchProviderError extends NoSuchModelError
⋮----
private readonly [symbol] = true; // used in isInstance
⋮----
constructor({
    modelId,
    modelType,
    providerId,
    availableProviders,
    message = `No such provider: ${providerId} (available providers: ${availableProviders.join()})`,
  }: {
    modelId: string;
    modelType: &apos;languageModel&apos; | &apos;textEmbeddingModel&apos;;
    providerId: string;
    availableProviders: string[];
    message?: string;
})
static isInstance(error: unknown): error is NoSuchProviderError</file><file path="packages/ai/core/registry/provider-registry.test.ts">import { NoSuchModelError } from &apos;@ai-sdk/provider&apos;;
import { MockEmbeddingModelV1 } from &apos;../test/mock-embedding-model-v1&apos;;
import { MockLanguageModelV1 } from &apos;../test/mock-language-model-v1&apos;;
import { NoSuchProviderError } from &apos;./no-such-provider-error&apos;;
import { createProviderRegistry } from &apos;./provider-registry&apos;;
import { MockImageModelV1 } from &apos;../test/mock-image-model-v1&apos;;
⋮----
// @ts-expect-error - should not accept arbitrary strings
⋮----
// @ts-expect-error - should not accept arbitrary strings
⋮----
// @ts-expect-error - should not accept arbitrary strings
⋮----
// @ts-expect-error - should not accept arbitrary strings
⋮----
// @ts-expect-error - should not accept arbitrary strings
⋮----
// @ts-expect-error - should not accept arbitrary strings</file><file path="packages/ai/core/registry/provider-registry.ts">import { NoSuchModelError, ProviderV1 } from &apos;@ai-sdk/provider&apos;;
import { EmbeddingModel, ImageModel, LanguageModel } from &apos;../types&apos;;
import { NoSuchProviderError } from &apos;./no-such-provider-error&apos;;
type ExtractLiteralUnion&lt;T&gt; = T extends string
  ? string extends T
    ? never
    : T
  : never;
export interface ProviderRegistryProvider&lt;
  PROVIDERS extends Record&lt;string, ProviderV1&gt; = Record&lt;string, ProviderV1&gt;,
  SEPARATOR extends string = &apos;:&apos;,
&gt; {
  languageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string
      ? `${KEY &amp; string}${SEPARATOR}${ExtractLiteralUnion&lt;Parameters&lt;NonNullable&lt;PROVIDERS[KEY][&apos;languageModel&apos;]&gt;&gt;[0]&gt;}`
      : never,
  ): LanguageModel;
  languageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string ? `${KEY &amp; string}${SEPARATOR}${string}` : never,
  ): LanguageModel;
  textEmbeddingModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string
      ? `${KEY &amp; string}${SEPARATOR}${ExtractLiteralUnion&lt;Parameters&lt;NonNullable&lt;PROVIDERS[KEY][&apos;textEmbeddingModel&apos;]&gt;&gt;[0]&gt;}`
      : never,
  ): EmbeddingModel&lt;string&gt;;
  textEmbeddingModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string ? `${KEY &amp; string}${SEPARATOR}${string}` : never,
  ): EmbeddingModel&lt;string&gt;;
  imageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string
      ? `${KEY &amp; string}${SEPARATOR}${ExtractLiteralUnion&lt;Parameters&lt;NonNullable&lt;PROVIDERS[KEY][&apos;imageModel&apos;]&gt;&gt;[0]&gt;}`
      : never,
  ): ImageModel;
  imageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string ? `${KEY &amp; string}${SEPARATOR}${string}` : never,
  ): ImageModel;
}
⋮----
languageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string
      ? `${KEY &amp; string}${SEPARATOR}${ExtractLiteralUnion&lt;Parameters&lt;NonNullable&lt;PROVIDERS[KEY][&apos;languageModel&apos;]&gt;&gt;[0]&gt;}`
      : never,
  ): LanguageModel;
languageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string ? `${KEY &amp; string}${SEPARATOR}${string}` : never,
  ): LanguageModel;
textEmbeddingModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string
      ? `${KEY &amp; string}${SEPARATOR}${ExtractLiteralUnion&lt;Parameters&lt;NonNullable&lt;PROVIDERS[KEY][&apos;textEmbeddingModel&apos;]&gt;&gt;[0]&gt;}`
      : never,
  ): EmbeddingModel&lt;string&gt;;
textEmbeddingModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string ? `${KEY &amp; string}${SEPARATOR}${string}` : never,
  ): EmbeddingModel&lt;string&gt;;
imageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string
      ? `${KEY &amp; string}${SEPARATOR}${ExtractLiteralUnion&lt;Parameters&lt;NonNullable&lt;PROVIDERS[KEY][&apos;imageModel&apos;]&gt;&gt;[0]&gt;}`
      : never,
  ): ImageModel;
imageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: KEY extends string ? `${KEY &amp; string}${SEPARATOR}${string}` : never,
  ): ImageModel;
⋮----
/**
 * Creates a registry for the given providers.
 */
export function createProviderRegistry&lt;
  PROVIDERS extends Record&lt;string, ProviderV1&gt;,
  SEPARATOR extends string = &apos;:&apos;,
&gt;(
  providers: PROVIDERS,
  {
    separator = &apos;:&apos; as SEPARATOR,
  }: {
    separator?: SEPARATOR;
  } = {},
): ProviderRegistryProvider&lt;PROVIDERS, SEPARATOR&gt;
/**
 * @deprecated Use `createProviderRegistry` instead.
 */
⋮----
class DefaultProviderRegistry&lt;
PROVIDERS extends Record&lt;string, ProviderV1&gt;,
⋮----
constructor(
registerProvider&lt;K extends keyof PROVIDERS&gt;({
    id,
    provider,
  }: {
    id: K;
    provider: PROVIDERS[K];
}): void
private getProvider(id: string): ProviderV1
private splitId(
    id: string,
    modelType: &apos;languageModel&apos; | &apos;textEmbeddingModel&apos; | &apos;imageModel&apos;,
): [string, string]
languageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: `${KEY &amp; string}${SEPARATOR}${string}`,
): LanguageModel
textEmbeddingModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: `${KEY &amp; string}${SEPARATOR}${string}`,
): EmbeddingModel&lt;string&gt;
imageModel&lt;KEY extends keyof PROVIDERS&gt;(
    id: `${KEY &amp; string}${SEPARATOR}${string}`,
): ImageModel</file><file path="packages/ai/core/telemetry/assemble-operation-name.ts">import { TelemetrySettings } from &apos;./telemetry-settings&apos;;
export function assembleOperationName({
  operationId,
  telemetry,
}: {
  operationId: string;
  telemetry?: TelemetrySettings;
})
⋮----
// standardized operation and resource name:
⋮----
// detailed, AI SDK specific data:</file><file path="packages/ai/core/telemetry/get-base-telemetry-attributes.ts">import { Attributes } from &apos;@opentelemetry/api&apos;;
import { CallSettings } from &apos;../prompt/call-settings&apos;;
import { TelemetrySettings } from &apos;./telemetry-settings&apos;;
export function getBaseTelemetryAttributes({
  model,
  settings,
  telemetry,
  headers,
}: {
  model: { modelId: string; provider: string };
  settings: Omit&lt;CallSettings, &apos;abortSignal&apos; | &apos;headers&apos;&gt;;
  telemetry: TelemetrySettings | undefined;
  headers: Record&lt;string, string | undefined&gt; | undefined;
}): Attributes
⋮----
// settings:
⋮----
// add metadata as attributes:
⋮----
// request headers</file><file path="packages/ai/core/telemetry/get-tracer.ts">import { Tracer, trace } from &apos;@opentelemetry/api&apos;;
import { noopTracer } from &apos;./noop-tracer&apos;;
export function getTracer({
  isEnabled = false,
  tracer,
}: {
  isEnabled?: boolean;
  tracer?: Tracer;
} =</file><file path="packages/ai/core/telemetry/noop-tracer.ts">import { Span, SpanContext, Tracer } from &apos;@opentelemetry/api&apos;;
/**
 * Tracer implementation that does nothing (null object).
 */
⋮----
startSpan(): Span
startActiveSpan&lt;F extends (span: Span) =&gt; unknown&gt;(
    name: unknown,
    arg1: unknown,
    arg2?: unknown,
    arg3?: F,
): ReturnType&lt;any&gt;
⋮----
spanContext()
setAttribute()
setAttributes()
addEvent()
addLink()
addLinks()
setStatus()
updateName()
end()
isRecording()
recordException()</file><file path="packages/ai/core/telemetry/record-span.ts">import { Attributes, Span, Tracer, SpanStatusCode } from &apos;@opentelemetry/api&apos;;
export function recordSpan&lt;T&gt;({
  name,
  tracer,
  attributes,
  fn,
  endWhenDone = true,
}: {
  name: string;
  tracer: Tracer;
  attributes: Attributes;
fn: (span: Span)
⋮----
// always stop the span when there is an error:</file><file path="packages/ai/core/telemetry/select-telemetry-attributes.ts">import type { Attributes, AttributeValue } from &apos;@opentelemetry/api&apos;;
import type { TelemetrySettings } from &apos;./telemetry-settings&apos;;
export function selectTelemetryAttributes({
  telemetry,
  attributes,
}: {
  telemetry?: TelemetrySettings;
  attributes: {
    [attributeKey: string]:
      | AttributeValue
      | { input: () =&gt; AttributeValue | undefined }
      | { output: () =&gt; AttributeValue | undefined }
      | undefined;
  };
}): Attributes
⋮----
// when telemetry is disabled, return an empty object to avoid serialization overhead:
⋮----
// input value, check if it should be recorded:
⋮----
// default to true:
⋮----
// output value, check if it should be recorded:
⋮----
// default to true:
⋮----
// value is an attribute value already:</file><file path="packages/ai/core/telemetry/select-temetry-attributes.test.ts">import { selectTelemetryAttributes } from &apos;./select-telemetry-attributes&apos;;</file><file path="packages/ai/core/telemetry/telemetry-settings.ts">import { AttributeValue, Tracer } from &apos;@opentelemetry/api&apos;;
/**
 * Telemetry configuration.
 */
// This is meant to be both flexible for custom app requirements (metadata)
// and extensible for standardization (example: functionId, more to come).
export type TelemetrySettings = {
  /**
   * Enable or disable telemetry. Disabled by default while experimental.
   */
  isEnabled?: boolean;
  /**
   * Enable or disable input recording. Enabled by default.
   *
   * You might want to disable input recording to avoid recording sensitive
   * information, to reduce data transfers, or to increase performance.
   */
  recordInputs?: boolean;
  /**
   * Enable or disable output recording. Enabled by default.
   *
   * You might want to disable output recording to avoid recording sensitive
   * information, to reduce data transfers, or to increase performance.
   */
  recordOutputs?: boolean;
  /**
   * Identifier for this function. Used to group telemetry data by function.
   */
  functionId?: string;
  /**
   * Additional information to include in the telemetry data.
   */
  metadata?: Record&lt;string, AttributeValue&gt;;
  /**
   * A custom tracer to use for the telemetry data.
   */
  tracer?: Tracer;
};
⋮----
/**
   * Enable or disable telemetry. Disabled by default while experimental.
   */
⋮----
/**
   * Enable or disable input recording. Enabled by default.
   *
   * You might want to disable input recording to avoid recording sensitive
   * information, to reduce data transfers, or to increase performance.
   */
⋮----
/**
   * Enable or disable output recording. Enabled by default.
   *
   * You might want to disable output recording to avoid recording sensitive
   * information, to reduce data transfers, or to increase performance.
   */
⋮----
/**
   * Identifier for this function. Used to group telemetry data by function.
   */
⋮----
/**
   * Additional information to include in the telemetry data.
   */
⋮----
/**
   * A custom tracer to use for the telemetry data.
   */</file><file path="packages/ai/core/test/mock-embedding-model-v1.ts">import { EmbeddingModelV1 } from &apos;@ai-sdk/provider&apos;;
import { Embedding } from &apos;../types&apos;;
import { EmbeddingModelUsage } from &apos;../types/usage&apos;;
import { notImplemented } from &apos;./not-implemented&apos;;
export class MockEmbeddingModelV1&lt;VALUE&gt; implements EmbeddingModelV1&lt;VALUE&gt;
⋮----
constructor({
    provider = &apos;mock-provider&apos;,
    modelId = &apos;mock-model-id&apos;,
    maxEmbeddingsPerCall = 1,
    supportsParallelCalls = false,
    doEmbed = notImplemented,
  }: {
    provider?: EmbeddingModelV1&lt;VALUE&gt;[&apos;provider&apos;];
    modelId?: EmbeddingModelV1&lt;VALUE&gt;[&apos;modelId&apos;];
    maxEmbeddingsPerCall?:
      | EmbeddingModelV1&lt;VALUE&gt;[&apos;maxEmbeddingsPerCall&apos;]
      | null;
    supportsParallelCalls?: EmbeddingModelV1&lt;VALUE&gt;[&apos;supportsParallelCalls&apos;];
    doEmbed?: EmbeddingModelV1&lt;VALUE&gt;[&apos;doEmbed&apos;];
} =
⋮----
export function mockEmbed&lt;VALUE&gt;(
  expectedValues: Array&lt;VALUE&gt;,
  embeddings: Array&lt;Embedding&gt;,
  usage?: EmbeddingModelUsage,
): EmbeddingModelV1&lt;VALUE&gt;[&apos;doEmbed&apos;]</file><file path="packages/ai/core/test/mock-image-model-v1.ts">import { ImageModelV1 } from &apos;@ai-sdk/provider&apos;;
import { notImplemented } from &apos;./not-implemented&apos;;
export class MockImageModelV1 implements ImageModelV1
⋮----
constructor({
    provider = &apos;mock-provider&apos;,
    modelId = &apos;mock-model-id&apos;,
    maxImagesPerCall = 1,
    doGenerate = notImplemented,
  }: {
    provider?: ImageModelV1[&apos;provider&apos;];
    modelId?: ImageModelV1[&apos;modelId&apos;];
    maxImagesPerCall?: ImageModelV1[&apos;maxImagesPerCall&apos;];
    doGenerate?: ImageModelV1[&apos;doGenerate&apos;];
} =</file><file path="packages/ai/core/test/mock-language-model-v1.ts">import { LanguageModelV1 } from &apos;@ai-sdk/provider&apos;;
import { notImplemented } from &apos;./not-implemented&apos;;
export class MockLanguageModelV1 implements LanguageModelV1
⋮----
constructor({
    provider = &apos;mock-provider&apos;,
    modelId = &apos;mock-model-id&apos;,
    supportsUrl = undefined,
    doGenerate = notImplemented,
    doStream = notImplemented,
    defaultObjectGenerationMode = undefined,
    supportsStructuredOutputs = undefined,
  }: {
    provider?: LanguageModelV1[&apos;provider&apos;];
    modelId?: LanguageModelV1[&apos;modelId&apos;];
    supportsUrl?: LanguageModelV1[&apos;supportsUrl&apos;];
    doGenerate?: LanguageModelV1[&apos;doGenerate&apos;];
    doStream?: LanguageModelV1[&apos;doStream&apos;];
    defaultObjectGenerationMode?: LanguageModelV1[&apos;defaultObjectGenerationMode&apos;];
    supportsStructuredOutputs?: LanguageModelV1[&apos;supportsStructuredOutputs&apos;];
} =</file><file path="packages/ai/core/test/mock-server-response.ts">import { ServerResponse } from &apos;node:http&apos;;
class MockServerResponse
⋮----
write(chunk: any): void
end(): void
⋮----
// You might want to mark the response as ended to simulate the real behavior
⋮----
writeHead(
    statusCode: number,
    statusMessage: string,
    headers: Record&lt;string, string&gt;,
): void
get body()
⋮----
// Combine all written chunks into a single string
⋮----
/**
   * Get the decoded chunks as strings.
   */
getDecodedChunks()
/**
   * Wait for the stream to finish writing to the mock response.
   */
async waitForEnd()
⋮----
const checkIfEnded = () =&gt;
⋮----
export function createMockServerResponse(): ServerResponse &amp;
  MockServerResponse {
  return new MockServerResponse() as ServerResponse &amp; MockServerResponse;</file><file path="packages/ai/core/test/mock-speech-model-v1.ts">import { SpeechModelV1 } from &apos;@ai-sdk/provider&apos;;
import { notImplemented } from &apos;./not-implemented&apos;;
export class MockSpeechModelV1 implements SpeechModelV1
⋮----
constructor({
    provider = &apos;mock-provider&apos;,
    modelId = &apos;mock-model-id&apos;,
    doGenerate = notImplemented,
  }: {
    provider?: SpeechModelV1[&apos;provider&apos;];
    modelId?: SpeechModelV1[&apos;modelId&apos;];
    doGenerate?: SpeechModelV1[&apos;doGenerate&apos;];
} =</file><file path="packages/ai/core/test/mock-tracer.ts">import {
  AttributeValue,
  Attributes,
  Context,
  Span,
  SpanContext,
  SpanOptions,
  Tracer,
} from &apos;@opentelemetry/api&apos;;
export class MockTracer implements Tracer
⋮----
get jsonSpans()
startSpan(name: string, options?: SpanOptions, context?: Context): Span
startActiveSpan&lt;F extends (span: Span) =&gt; unknown&gt;(
    name: string,
    arg1: unknown,
    arg2?: unknown,
    arg3?: F,
): ReturnType&lt;any&gt;
⋮----
class MockSpan implements Span
⋮----
constructor({
    name,
    options,
    context,
  }: {
    name: string;
    options?: SpanOptions;
    context?: Context;
})
spanContext(): SpanContext
setAttribute(key: string, value: AttributeValue): this
setAttributes(attributes: Attributes): this
addEvent(name: string, attributes?: Attributes): this
addLink()
addLinks()
setStatus()
updateName()
end()
isRecording()
recordException()
⋮----
class MockSpanContext implements SpanContext</file><file path="packages/ai/core/test/mock-transcription-model-v1.ts">import { TranscriptionModelV1 } from &apos;@ai-sdk/provider&apos;;
import { notImplemented } from &apos;./not-implemented&apos;;
export class MockTranscriptionModelV1 implements TranscriptionModelV1
⋮----
constructor({
    provider = &apos;mock-provider&apos;,
    modelId = &apos;mock-model-id&apos;,
    doGenerate = notImplemented,
  }: {
    provider?: TranscriptionModelV1[&apos;provider&apos;];
    modelId?: TranscriptionModelV1[&apos;modelId&apos;];
    doGenerate?: TranscriptionModelV1[&apos;doGenerate&apos;];
} =</file><file path="packages/ai/core/test/mock-values.ts">export function mockValues&lt;T&gt;(...values: T[]): () =&gt; T</file><file path="packages/ai/core/test/not-implemented.ts">export function notImplemented(): never</file><file path="packages/ai/core/tool/mcp/json-rpc-message.ts">import { z } from &apos;zod&apos;;
import { BaseParamsSchema, RequestSchema, ResultSchema } from &apos;./types&apos;;
⋮----
export type JSONRPCRequest = z.infer&lt;typeof JSONRPCRequestSchema&gt;;
⋮----
export type JSONRPCResponse = z.infer&lt;typeof JSONRPCResponseSchema&gt;;
⋮----
export type JSONRPCError = z.infer&lt;typeof JSONRPCErrorSchema&gt;;
⋮----
export type JSONRPCNotification = z.infer&lt;typeof JSONRPCNotificationSchema&gt;;
⋮----
export type JSONRPCMessage = z.infer&lt;typeof JSONRPCMessageSchema&gt;;</file><file path="packages/ai/core/tool/mcp/mcp-client.test.ts">import { z } from &apos;zod&apos;;
import { MCPClientError } from &apos;../../../errors&apos;;
import { createMCPClient } from &apos;./mcp-client&apos;;
import { MockMCPTransport } from &apos;./mock-mcp-transport&apos;;
import { CallToolResult } from &apos;./types&apos;;
⋮----
type ToolParams = Parameters&lt;typeof tool.execute&gt;[0];
⋮----
// Because isCustomMcpTransport will return false, the client will fallback to createMcpTransport, but it will throw because the transport is invalid:
⋮----
// @ts-expect-error - invalid transport
⋮----
// @ts-expect-error - invalid transport</file><file path="packages/ai/core/tool/mcp/mcp-client.ts">import { JSONSchema7 } from &apos;@ai-sdk/provider&apos;;
import { jsonSchema } from &apos;@ai-sdk/ui-utils&apos;;
import { z, ZodType } from &apos;zod&apos;;
import { MCPClientError } from &apos;../../../errors&apos;;
import { inferParameters, tool, Tool, ToolExecutionOptions } from &apos;../tool&apos;;
import {
  JSONRPCError,
  JSONRPCNotification,
  JSONRPCRequest,
  JSONRPCResponse,
} from &apos;./json-rpc-message&apos;;
import {
  createMcpTransport,
  isCustomMcpTransport,
  MCPTransport,
  MCPTransportConfig,
} from &apos;./mcp-transport&apos;;
import {
  CallToolResult,
  CallToolResultSchema,
  Configuration as ClientConfiguration,
  InitializeResultSchema,
  LATEST_PROTOCOL_VERSION,
  ListToolsResult,
  ListToolsResultSchema,
  McpToolSet,
  Notification,
  PaginatedRequest,
  Request,
  RequestOptions,
  ServerCapabilities,
  SUPPORTED_PROTOCOL_VERSIONS,
  ToolSchemas,
} from &apos;./types&apos;;
⋮----
interface MCPClientConfig {
  /** Transport configuration for connecting to the MCP server */
  transport: MCPTransportConfig | MCPTransport;
  /** Optional callback for uncaught errors */
  onUncaughtError?: (error: unknown) =&gt; void;
  /** Optional client name, defaults to &apos;ai-sdk-mcp-client&apos; */
  name?: string;
}
⋮----
/** Transport configuration for connecting to the MCP server */
⋮----
/** Optional callback for uncaught errors */
⋮----
/** Optional client name, defaults to &apos;ai-sdk-mcp-client&apos; */
⋮----
export async function createMCPClient(
  config: MCPClientConfig,
): Promise&lt;MCPClient&gt;
/**
 * A lightweight MCP Client implementation
 *
 * The primary purpose of this client is tool conversion between MCP&lt;&gt;AI SDK
 * but can later be extended to support other MCP features
 *
 * Tool parameters are automatically inferred from the server&apos;s JSON schema
 * if not explicitly provided in the tools configuration
 *
 * This client is meant to be used to communicate with a single server. To communicate and fetch tools across multiple servers, it&apos;s recommended to create a new client instance per server.
 *
 * Not supported:
 * - Client options (e.g. sampling, roots) as they are not needed for tool conversion
 * - Accepting notifications
 * - Session management (when passing a sessionId to an instance of the Streamable HTTP transport)
 * - Resumable SSE streams
 */
class MCPClient
⋮----
constructor({
    transport: transportConfig,
    name = &apos;ai-sdk-mcp-client&apos;,
    onUncaughtError,
}: MCPClientConfig)
⋮----
// This lightweight client implementation does not support
// receiving notifications or requests from server.
// If we get an unsupported message, we can safely ignore it and pass to the onError handler:
⋮----
async init(): Promise&lt;this&gt;
⋮----
// Complete initialization handshake:
⋮----
async close(): Promise&lt;void&gt;
private assertCapability(method: string): void
private async request&lt;T extends ZodType&lt;object&gt;&gt;({
    request,
    resultSchema,
    options,
  }: {
    request: Request;
    resultSchema: T;
    options?: RequestOptions;
}): Promise&lt;z.infer&lt;T&gt;&gt;
⋮----
const cleanup = () =&gt;
⋮----
private async listTools({
    params,
    options,
  }: {
    params?: PaginatedRequest[&apos;params&apos;];
    options?: RequestOptions;
} =
private async callTool({
    name,
    args,
    options,
  }: {
    name: string;
    args: Record&lt;string, unknown&gt;;
    options?: ToolExecutionOptions;
}): Promise&lt;CallToolResult&gt;
private async notification(notification: Notification): Promise&lt;void&gt;
/**
   * Returns a set of AI SDK tools from the MCP server
   * @returns A record of tool names to their implementations
   */
async tools&lt;TOOL_SCHEMAS extends ToolSchemas = &apos;automatic&apos;&gt;({
    schemas = &apos;automatic&apos;,
  }: {
    schemas?: TOOL_SCHEMAS;
} =
private onClose(): void
private onError(error: unknown): void
private onResponse(response: JSONRPCResponse | JSONRPCError): void</file><file path="packages/ai/core/tool/mcp/mcp-sse-transport.test.ts">import {
  createTestServer,
  TestResponseController,
} from &apos;@ai-sdk/provider-utils/test&apos;;
import { MCPClientError } from &apos;../../../errors&apos;;
import { SseMCPTransport } from &apos;./mcp-sse-transport&apos;;
⋮----
// Verify SSE connection headers
⋮----
// Verify POST request headers</file><file path="packages/ai/core/tool/mcp/mcp-sse-transport.ts">import { createEventSourceParserStream } from &apos;@ai-sdk/provider-utils&apos;;
import { MCPClientError } from &apos;../../../errors&apos;;
import { JSONRPCMessage, JSONRPCMessageSchema } from &apos;./json-rpc-message&apos;;
import { MCPTransport } from &apos;./mcp-transport&apos;;
export class SseMCPTransport implements MCPTransport
⋮----
constructor({
    url,
    headers,
  }: {
    url: string;
    headers?: Record&lt;string, string&gt;;
})
async start(): Promise&lt;void&gt;
⋮----
const establishConnection = async () =&gt;
⋮----
const processEvents = async () =&gt;
⋮----
// We do not throw here so we continue processing events after reporting the error
⋮----
async close(): Promise&lt;void&gt;
async send(message: JSONRPCMessage): Promise&lt;void&gt;
⋮----
export function deserializeMessage(line: string): JSONRPCMessage</file><file path="packages/ai/core/tool/mcp/mcp-transport.ts">import { MCPClientError } from &apos;../../../errors&apos;;
import { JSONRPCMessage } from &apos;./json-rpc-message&apos;;
import { SseMCPTransport } from &apos;./mcp-sse-transport&apos;;
/**
 * Transport interface for MCP (Model Context Protocol) communication.
 * Maps to the `Transport` interface in the MCP spec.
 */
export interface MCPTransport {
  /**
   * Initialize and start the transport
   */
  start(): Promise&lt;void&gt;;
  /**
   * Send a JSON-RPC message through the transport
   * @param message The JSON-RPC message to send
   */
  send(message: JSONRPCMessage): Promise&lt;void&gt;;
  /**
   * Clean up and close the transport
   */
  close(): Promise&lt;void&gt;;
  /**
   * Event handler for transport closure
   */
  onclose?: () =&gt; void;
  /**
   * Event handler for transport errors
   */
  onerror?: (error: Error) =&gt; void;
  /**
   * Event handler for received messages
   */
  onmessage?: (message: JSONRPCMessage) =&gt; void;
}
⋮----
/**
   * Initialize and start the transport
   */
start(): Promise&lt;void&gt;;
/**
   * Send a JSON-RPC message through the transport
   * @param message The JSON-RPC message to send
   */
send(message: JSONRPCMessage): Promise&lt;void&gt;;
/**
   * Clean up and close the transport
   */
close(): Promise&lt;void&gt;;
/**
   * Event handler for transport closure
   */
⋮----
/**
   * Event handler for transport errors
   */
⋮----
/**
   * Event handler for received messages
   */
⋮----
export type MCPTransportConfig = {
  type: &apos;sse&apos;;
  /**
   * The URL of the MCP server.
   */
  url: string;
  /**
   * Additional HTTP headers to be sent with requests.
   */
  headers?: Record&lt;string, string&gt;;
};
⋮----
/**
   * The URL of the MCP server.
   */
⋮----
/**
   * Additional HTTP headers to be sent with requests.
   */
⋮----
export function createMcpTransport(config: MCPTransportConfig): MCPTransport
export function isCustomMcpTransport(
  transport: MCPTransportConfig | MCPTransport,
): transport is MCPTransport</file><file path="packages/ai/core/tool/mcp/mock-mcp-transport.ts">import { delay } from &apos;@ai-sdk/provider-utils&apos;;
import { JSONRPCMessage } from &apos;./json-rpc-message&apos;;
import { MCPTransport } from &apos;./mcp-transport&apos;;
import { MCPTool } from &apos;./types&apos;;
⋮----
export class MockMCPTransport implements MCPTransport
⋮----
constructor({
    overrideTools = DEFAULT_TOOLS,
    failOnInvalidToolParams = false,
    initializeResult,
    sendError = false,
  }: {
    overrideTools?: MCPTool[];
    failOnInvalidToolParams?: boolean;
    initializeResult?: Record&lt;string, unknown&gt;;
    sendError?: boolean;
} =
async start(): Promise&lt;void&gt;
async send(message: JSONRPCMessage): Promise&lt;void&gt;
⋮----
// Mock server response implementation - extend as necessary:
⋮----
async close(): Promise&lt;void&gt;</file><file path="packages/ai/core/tool/mcp/types.ts">import { z } from &apos;zod&apos;;
import {
  inferParameters,
  Tool,
  ToolExecutionOptions,
  ToolParameters,
} from &apos;../tool&apos;;
⋮----
export type ToolSchemas =
  | Record&lt;string, { parameters: ToolParameters }&gt;
  | &apos;automatic&apos;
  | undefined;
export type McpToolSet&lt;TOOL_SCHEMAS extends ToolSchemas = &apos;automatic&apos;&gt; =
  TOOL_SCHEMAS extends Record&lt;string, { parameters: ToolParameters }&gt;
    ? {
        [K in keyof TOOL_SCHEMAS]: Tool&lt;
          TOOL_SCHEMAS[K][&apos;parameters&apos;],
          CallToolResult
        &gt; &amp; {
          execute: (
            args: inferParameters&lt;TOOL_SCHEMAS[K][&apos;parameters&apos;]&gt;,
            options: ToolExecutionOptions,
          ) =&gt; PromiseLike&lt;CallToolResult&gt;;
        };
      }
    : {
        [k: string]: Tool&lt;z.ZodUnknown, CallToolResult&gt; &amp; {
          execute: (
            args: unknown,
            options: ToolExecutionOptions,
          ) =&gt; PromiseLike&lt;CallToolResult&gt;;
        };
      };
⋮----
export type Configuration = z.infer&lt;typeof ClientOrServerImplementationSchema&gt;;
⋮----
type BaseParams = z.infer&lt;typeof BaseParamsSchema&gt;;
⋮----
export type Request = z.infer&lt;typeof RequestSchema&gt;;
export type RequestOptions = {
  signal?: AbortSignal;
  timeout?: number;
  maxTotalTimeout?: number;
};
export type Notification = z.infer&lt;typeof RequestSchema&gt;;
⋮----
export type ServerCapabilities = z.infer&lt;typeof ServerCapabilitiesSchema&gt;;
⋮----
export type InitializeResult = z.infer&lt;typeof InitializeResultSchema&gt;;
export type PaginatedRequest = Request &amp; {
  params?: BaseParams &amp; {
    cursor?: string;
  };
};
⋮----
export type MCPTool = z.infer&lt;typeof ToolSchema&gt;;
⋮----
export type ListToolsResult = z.infer&lt;typeof ListToolsResultSchema&gt;;
⋮----
/**
     * The URI of this resource.
     */
⋮----
/**
     * The MIME type of this resource, if known.
     */
⋮----
export type CallToolResult = z.infer&lt;typeof CallToolResultSchema&gt;;</file><file path="packages/ai/core/tool/index.ts"></file><file path="packages/ai/core/tool/tool.ts">import { Schema } from &apos;@ai-sdk/ui-utils&apos;;
import { z } from &apos;zod&apos;;
import { ToolResultContent } from &apos;../prompt/tool-result-content&apos;;
import { CoreMessage } from &apos;../prompt/message&apos;;
export type ToolParameters = z.ZodTypeAny | Schema&lt;any&gt;;
export type inferParameters&lt;PARAMETERS extends ToolParameters&gt; =
  PARAMETERS extends Schema&lt;any&gt;
    ? PARAMETERS[&apos;_type&apos;]
    : PARAMETERS extends z.ZodTypeAny
      ? z.infer&lt;PARAMETERS&gt;
      : never;
export interface ToolExecutionOptions {
  /**
   * The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.
   */
  toolCallId: string;
  /**
   * Messages that were sent to the language model to initiate the response that contained the tool call.
   * The messages **do not** include the system prompt nor the assistant response that contained the tool call.
   */
  messages: CoreMessage[];
  /**
   * An optional abort signal that indicates that the overall operation should be aborted.
   */
  abortSignal?: AbortSignal;
}
⋮----
/**
   * The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.
   */
⋮----
/**
   * Messages that were sent to the language model to initiate the response that contained the tool call.
   * The messages **do not** include the system prompt nor the assistant response that contained the tool call.
   */
⋮----
/**
   * An optional abort signal that indicates that the overall operation should be aborted.
   */
⋮----
/**
A tool contains the description and the schema of the input that the tool expects.
This enables the language model to generate the input.
The tool can also contain an optional execute function for the actual execution function of the tool.
 */
export type Tool&lt;PARAMETERS extends ToolParameters = any, RESULT = any&gt; = {
  /**
The schema of the input that the tool expects. The language model will use this to generate the input.
It is also used to validate the output of the language model.
Use descriptions to make the input understandable for the language model.
   */
  parameters: PARAMETERS;
  /**
An optional description of what the tool does.
Will be used by the language model to decide whether to use the tool.
Not used for provider-defined tools.
   */
  description?: string;
  /**
Optional conversion function that maps the tool result to multi-part tool content for LLMs.
   */
  experimental_toToolResultContent?: (result: RESULT) =&gt; ToolResultContent;
  /**
An async function that is called with the arguments from the tool call and produces a result.
If not provided, the tool will not be executed automatically.
@args is the input of the tool call.
@options.abortSignal is a signal that can be used to abort the tool call.
   */
  execute?: (
    args: inferParameters&lt;PARAMETERS&gt;,
    options: ToolExecutionOptions,
  ) =&gt; PromiseLike&lt;RESULT&gt;;
} &amp; (
  | {
      /**
Function tool.
       */
      type?: undefined | &apos;function&apos;;
    }
  | {
      /**
Provider-defined tool.
       */
      type: &apos;provider-defined&apos;;
      /**
The ID of the tool. Should follow the format `&lt;provider-name&gt;.&lt;tool-name&gt;`.
       */
      id: `${string}.${string}`;
      /**
The arguments for configuring the tool. Must match the expected arguments defined by the provider for this tool.
       */
      args: Record&lt;string, unknown&gt;;
    }
);
⋮----
/**
The schema of the input that the tool expects. The language model will use this to generate the input.
It is also used to validate the output of the language model.
Use descriptions to make the input understandable for the language model.
   */
⋮----
/**
An optional description of what the tool does.
Will be used by the language model to decide whether to use the tool.
Not used for provider-defined tools.
   */
⋮----
/**
Optional conversion function that maps the tool result to multi-part tool content for LLMs.
   */
⋮----
/**
An async function that is called with the arguments from the tool call and produces a result.
If not provided, the tool will not be executed automatically.
@args is the input of the tool call.
@options.abortSignal is a signal that can be used to abort the tool call.
   */
⋮----
/**
Function tool.
       */
⋮----
/**
Provider-defined tool.
       */
⋮----
/**
The ID of the tool. Should follow the format `&lt;provider-name&gt;.&lt;tool-name&gt;`.
       */
⋮----
/**
The arguments for configuring the tool. Must match the expected arguments defined by the provider for this tool.
       */
⋮----
/**
 * @deprecated Use `Tool` instead.
 */
// TODO remove in v5
export type CoreTool&lt;
  PARAMETERS extends ToolParameters = any,
  RESULT = any,
&gt; = Tool&lt;PARAMETERS, RESULT&gt;;
/**
Helper function for inferring the execute args of a tool.
 */
// Note: special type inference is needed for the execute function args to make sure they are inferred correctly.
export function tool&lt;PARAMETERS extends ToolParameters, RESULT&gt;(
  tool: Tool&lt;PARAMETERS, RESULT&gt; &amp; {
    execute: (
      args: inferParameters&lt;PARAMETERS&gt;,
      options: ToolExecutionOptions,
)
export function tool&lt;PARAMETERS extends ToolParameters, RESULT&gt;(
  tool: Tool&lt;PARAMETERS, RESULT&gt; &amp; {
    execute?: undefined;
  },
): Tool&lt;PARAMETERS, RESULT&gt; &amp;
export function tool&lt;PARAMETERS extends ToolParameters, RESULT = any&gt;(
  tool: Tool&lt;PARAMETERS, RESULT&gt;,
): Tool&lt;PARAMETERS, RESULT&gt;</file><file path="packages/ai/core/transcribe/index.ts"></file><file path="packages/ai/core/transcribe/transcribe-result.ts">import { JSONValue } from &apos;@ai-sdk/provider&apos;;
import { TranscriptionWarning } from &apos;../types/transcription-model&apos;;
import { TranscriptionModelResponseMetadata } from &apos;../types/transcription-model-response-metadata&apos;;
/**
The result of a `transcribe` call.
It contains the transcript and additional information.
 */
export interface TranscriptionResult {
  /**
   * The complete transcribed text from the audio.
   */
  readonly text: string;
  /**
   * Array of transcript segments with timing information.
   * Each segment represents a portion of the transcribed text with start and end times.
   */
  readonly segments: Array&lt;{
    /**
     * The text content of this segment.
     */
    readonly text: string;
    /**
     * The start time of this segment in seconds.
     */
    readonly startSecond: number;
    /**
     * The end time of this segment in seconds.
     */
    readonly endSecond: number;
  }&gt;;
  /**
   * The detected language of the audio content, as an ISO-639-1 code (e.g., &apos;en&apos; for English).
   * May be undefined if the language couldn&apos;t be detected.
   */
  readonly language: string | undefined;
  /**
   * The total duration of the audio file in seconds.
   * May be undefined if the duration couldn&apos;t be determined.
   */
  readonly durationInSeconds: number | undefined;
  /**
  Warnings for the call, e.g. unsupported settings.
     */
  readonly warnings: Array&lt;TranscriptionWarning&gt;;
  /**
  Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.
   */
  readonly responses: Array&lt;TranscriptionModelResponseMetadata&gt;;
  /**
  Provider metadata from the provider.
   */
  readonly providerMetadata: Record&lt;string, Record&lt;string, JSONValue&gt;&gt;;
}
⋮----
/**
   * The complete transcribed text from the audio.
   */
⋮----
/**
   * Array of transcript segments with timing information.
   * Each segment represents a portion of the transcribed text with start and end times.
   */
⋮----
/**
     * The text content of this segment.
     */
⋮----
/**
     * The start time of this segment in seconds.
     */
⋮----
/**
     * The end time of this segment in seconds.
     */
⋮----
/**
   * The detected language of the audio content, as an ISO-639-1 code (e.g., &apos;en&apos; for English).
   * May be undefined if the language couldn&apos;t be detected.
   */
⋮----
/**
   * The total duration of the audio file in seconds.
   * May be undefined if the duration couldn&apos;t be determined.
   */
⋮----
/**
  Warnings for the call, e.g. unsupported settings.
     */
⋮----
/**
  Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.
   */
⋮----
/**
  Provider metadata from the provider.
   */</file><file path="packages/ai/core/transcribe/transcribe.test.ts">import {
  JSONValue,
  TranscriptionModelV1,
  TranscriptionModelV1CallWarning,
} from &apos;@ai-sdk/provider&apos;;
import { MockTranscriptionModelV1 } from &apos;../test/mock-transcription-model-v1&apos;;
import { transcribe } from &apos;./transcribe&apos;;
const audioData = new Uint8Array([1, 2, 3, 4]); // Sample audio data
⋮----
const createMockResponse = (options: {
  text: string;
  segments: Array&lt;{
    text: string;
    startSecond: number;
    endSecond: number;
  }&gt;;
  language?: string;
  durationInSeconds?: number;
  warnings?: TranscriptionModelV1CallWarning[];
  timestamp?: Date;
  modelId?: string;
  headers?: Record&lt;string, string&gt;;
  providerMetadata?: Record&lt;string, Record&lt;string, JSONValue&gt;&gt;;
}) =&gt; (</file><file path="packages/ai/core/transcribe/transcribe.ts">import { JSONValue, TranscriptionModelV1 } from &apos;@ai-sdk/provider&apos;;
import { NoTranscriptGeneratedError } from &apos;../../errors/no-transcript-generated-error&apos;;
import { download } from &apos;../../util/download&apos;;
import { DataContent } from &apos;../prompt&apos;;
import { convertDataContentToUint8Array } from &apos;../prompt/data-content&apos;;
import { prepareRetries } from &apos;../prompt/prepare-retries&apos;;
import { ProviderOptions } from &apos;../types/provider-metadata&apos;;
import { TranscriptionWarning } from &apos;../types/transcription-model&apos;;
import { TranscriptionModelResponseMetadata } from &apos;../types/transcription-model-response-metadata&apos;;
import {
  audioMimeTypeSignatures,
  detectMimeType,
} from &apos;../util/detect-mimetype&apos;;
import { TranscriptionResult } from &apos;./transcribe-result&apos;;
/**
Generates transcripts using a transcription model.
@param model - The transcription model to use.
@param audio - The audio data to transcribe as DataContent (string | Uint8Array | ArrayBuffer | Buffer) or a URL.
@param providerOptions - Additional provider-specific options that are passed through to the provider
as body parameters.
@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.
@param abortSignal - An optional abort signal that can be used to cancel the call.
@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.
@returns A result object that contains the generated transcript.
 */
export async function transcribe({
  model,
  audio,
  providerOptions = {},
  maxRetries: maxRetriesArg,
  abortSignal,
  headers,
}: {
  /**
The transcription model to use.
     */
  model: TranscriptionModelV1;
  /**
The audio data to transcribe.
   */
  audio: DataContent | URL;
  /**
Additional provider-specific options that are passed through to the provider
as body parameters.
The outer record is keyed by the provider name, and the inner
record is keyed by the provider-specific metadata key.
```ts
{
  &quot;openai&quot;: {
    &quot;temperature&quot;: 0
  }
}
```
     */
  providerOptions?: ProviderOptions;
  /**
Maximum number of retries per transcript model call. Set to 0 to disable retries.
@default 2
   */
  maxRetries?: number;
  /**
Abort signal.
 */
  abortSignal?: AbortSignal;
  /**
Additional headers to include in the request.
Only applicable for HTTP-based providers.
 */
  headers?: Record&lt;string, string&gt;;
}): Promise&lt;TranscriptionResult&gt;
⋮----
/**
The transcription model to use.
     */
⋮----
/**
The audio data to transcribe.
   */
⋮----
/**
Additional provider-specific options that are passed through to the provider
as body parameters.
The outer record is keyed by the provider name, and the inner
record is keyed by the provider-specific metadata key.
```ts
{
  &quot;openai&quot;: {
    &quot;temperature&quot;: 0
  }
}
```
     */
⋮----
/**
Maximum number of retries per transcript model call. Set to 0 to disable retries.
@default 2
   */
⋮----
/**
Abort signal.
 */
⋮----
/**
Additional headers to include in the request.
Only applicable for HTTP-based providers.
 */
⋮----
class DefaultTranscriptionResult implements TranscriptionResult
⋮----
constructor(options: {
    text: string;
    segments: Array&lt;{
      text: string;
      startSecond: number;
      endSecond: number;
    }&gt;;
    language: string | undefined;
    durationInSeconds: number | undefined;
    warnings: Array&lt;TranscriptionWarning&gt;;
    responses: Array&lt;TranscriptionModelResponseMetadata&gt;;
    providerMetadata: Record&lt;string, Record&lt;string, JSONValue&gt;&gt; | undefined;
})</file><file path="packages/ai/core/types/embedding-model.ts">import { EmbeddingModelV1, EmbeddingModelV1Embedding } from &apos;@ai-sdk/provider&apos;;
/**
Embedding model that is used by the AI SDK Core functions.
*/
export type EmbeddingModel&lt;VALUE&gt; = EmbeddingModelV1&lt;VALUE&gt;;
/**
Embedding.
 */
export type Embedding = EmbeddingModelV1Embedding;</file><file path="packages/ai/core/types/image-model-response-metadata.ts">export type ImageModelResponseMetadata = {
  /**
Timestamp for the start of the generated response.
   */
  timestamp: Date;
  /**
The ID of the response model that was used to generate the response.
   */
  modelId: string;
  /**
Response headers.
   */
  headers?: Record&lt;string, string&gt;;
};
⋮----
/**
Timestamp for the start of the generated response.
   */
⋮----
/**
The ID of the response model that was used to generate the response.
   */
⋮----
/**
Response headers.
   */</file><file path="packages/ai/core/types/image-model.ts">import { ImageModelV1, ImageModelV1CallWarning } from &apos;@ai-sdk/provider&apos;;
/**
Image model that is used by the AI SDK Core functions.
  */
export type ImageModel = ImageModelV1;
/**
Warning from the model provider for this call. The call will proceed, but e.g.
some settings might not be supported, which can lead to suboptimal results.
  */
export type ImageGenerationWarning = ImageModelV1CallWarning;</file><file path="packages/ai/core/types/index.ts"></file><file path="packages/ai/core/types/json-value.ts">import { JSONValue } from &apos;@ai-sdk/provider&apos;;
import { z } from &apos;zod&apos;;</file><file path="packages/ai/core/types/language-model-request-metadata.ts">export type LanguageModelRequestMetadata = {
  /**
  Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).
     */
  body?: string;
};
⋮----
/**
  Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).
     */</file><file path="packages/ai/core/types/language-model-response-metadata.ts">export type LanguageModelResponseMetadata = {
  /**
  ID for the generated response.
     */
  id: string;
  /**
  Timestamp for the start of the generated response.
  */
  timestamp: Date;
  /**
  The ID of the response model that was used to generate the response.
  */
  modelId: string;
  /**
Response headers (available only for providers that use HTTP requests).
     */
  headers?: Record&lt;string, string&gt;;
};
⋮----
/**
  ID for the generated response.
     */
⋮----
/**
  Timestamp for the start of the generated response.
  */
⋮----
/**
  The ID of the response model that was used to generate the response.
  */
⋮----
/**
Response headers (available only for providers that use HTTP requests).
     */</file><file path="packages/ai/core/types/language-model.ts">import {
  LanguageModelV1,
  LanguageModelV1CallWarning,
  LanguageModelV1FinishReason,
  LanguageModelV1LogProbs,
  LanguageModelV1Source,
} from &apos;@ai-sdk/provider&apos;;
// Re-export LanguageModelV1 types for the middleware:
⋮----
/**
Language model that is used by the AI SDK Core functions.
*/
export type LanguageModel = LanguageModelV1;
/**
Reason why a language model finished generating a response.
Can be one of the following:
- `stop`: model generated stop sequence
- `length`: model generated maximum number of tokens
- `content-filter`: content filter violation stopped the model
- `tool-calls`: model triggered tool calls
- `error`: model stopped because of an error
- `other`: model stopped for other reasons
*/
export type FinishReason = LanguageModelV1FinishReason;
/**
Log probabilities for each token and its top log probabilities.
@deprecated Will become a provider extension in the future.
 */
export type LogProbs = LanguageModelV1LogProbs;
/**
Warning from the model provider for this call. The call will proceed, but e.g.
some settings might not be supported, which can lead to suboptimal results.
*/
export type CallWarning = LanguageModelV1CallWarning;
/**
A source that has been used as input to generate the response.
*/
export type Source = LanguageModelV1Source;
/**
Tool choice for the generation. It supports the following settings:
- `auto` (default): the model can choose whether and which tools to call.
- `required`: the model must call a tool. It can choose which tool to call.
- `none`: the model must not call tools
- `{ type: &apos;tool&apos;, toolName: string (typed) }`: the model must call the specified tool
 */
export type ToolChoice&lt;TOOLS extends Record&lt;string, unknown&gt;&gt; =
  | &apos;auto&apos;
  | &apos;none&apos;
  | &apos;required&apos;
  | { type: &apos;tool&apos;; toolName: keyof TOOLS };
/**
 * @deprecated Use `ToolChoice` instead.
 */
// TODO remove in v5
export type CoreToolChoice&lt;TOOLS extends Record&lt;string, unknown&gt;&gt; =
  ToolChoice&lt;TOOLS&gt;;</file><file path="packages/ai/core/types/provider-metadata.ts">import { LanguageModelV1ProviderMetadata } from &apos;@ai-sdk/provider&apos;;
import { z } from &apos;zod&apos;;
import { jsonValueSchema } from &apos;./json-value&apos;;
/**
Additional provider-specific metadata that is returned from the provider.
This is needed to enable provider-specific functionality that can be
fully encapsulated in the provider.
 */
export type ProviderMetadata = LanguageModelV1ProviderMetadata;
/**
Additional provider-specific options.
They are passed through to the provider from the AI SDK and enable
provider-specific functionality that can be fully encapsulated in the provider.
 */
// TODO change to LanguageModelV2ProviderOptions in language model v2
export type ProviderOptions = LanguageModelV1ProviderMetadata;</file><file path="packages/ai/core/types/provider.ts">import { EmbeddingModel } from &apos;./embedding-model&apos;;
import { LanguageModel } from &apos;./language-model&apos;;
import { ImageModel } from &apos;./image-model&apos;;
/**
 * Provider for language, text embedding, and image models.
 */
export type Provider = {
  /**
  Returns the language model with the given id.
  The model id is then passed to the provider function to get the model.
  @param {string} id - The id of the model to return.
  @returns {LanguageModel} The language model associated with the id
  @throws {NoSuchModelError} If no such model exists.
     */
  languageModel(modelId: string): LanguageModel;
  /**
  Returns the text embedding model with the given id.
  The model id is then passed to the provider function to get the model.
  @param {string} id - The id of the model to return.
  @returns {LanguageModel} The language model associated with the id
  @throws {NoSuchModelError} If no such model exists.
     */
  textEmbeddingModel(modelId: string): EmbeddingModel&lt;string&gt;;
  /**
  Returns the image model with the given id.
  The model id is then passed to the provider function to get the model.
  @param {string} id - The id of the model to return.
  @returns {ImageModel} The image model associated with the id
  */
  imageModel(modelId: string): ImageModel;
};
⋮----
/**
  Returns the language model with the given id.
  The model id is then passed to the provider function to get the model.
  @param {string} id - The id of the model to return.
  @returns {LanguageModel} The language model associated with the id
  @throws {NoSuchModelError} If no such model exists.
     */
languageModel(modelId: string): LanguageModel;
/**
  Returns the text embedding model with the given id.
  The model id is then passed to the provider function to get the model.
  @param {string} id - The id of the model to return.
  @returns {LanguageModel} The language model associated with the id
  @throws {NoSuchModelError} If no such model exists.
     */
textEmbeddingModel(modelId: string): EmbeddingModel&lt;string&gt;;
/**
  Returns the image model with the given id.
  The model id is then passed to the provider function to get the model.
  @param {string} id - The id of the model to return.
  @returns {ImageModel} The image model associated with the id
  */
imageModel(modelId: string): ImageModel;</file><file path="packages/ai/core/types/speech-model-response-metadata.ts">export type SpeechModelResponseMetadata = {
  /**
Timestamp for the start of the generated response.
   */
  timestamp: Date;
  /**
The ID of the response model that was used to generate the response.
   */
  modelId: string;
  /**
Response headers.
   */
  headers?: Record&lt;string, string&gt;;
};
⋮----
/**
Timestamp for the start of the generated response.
   */
⋮----
/**
The ID of the response model that was used to generate the response.
   */
⋮----
/**
Response headers.
   */</file><file path="packages/ai/core/types/speech-model.ts">import { SpeechModelV1, SpeechModelV1CallWarning } from &apos;@ai-sdk/provider&apos;;
/**
Speech model that is used by the AI SDK Core functions.
  */
export type SpeechModel = SpeechModelV1;
/**
Warning from the model provider for this call. The call will proceed, but e.g.
some settings might not be supported, which can lead to suboptimal results.
  */
export type SpeechWarning = SpeechModelV1CallWarning;</file><file path="packages/ai/core/types/transcription-model-response-metadata.ts">export type TranscriptionModelResponseMetadata = {
  /**
Timestamp for the start of the generated response.
   */
  timestamp: Date;
  /**
The ID of the response model that was used to generate the response.
   */
  modelId: string;
  /**
Response headers.
   */
  headers?: Record&lt;string, string&gt;;
};
⋮----
/**
Timestamp for the start of the generated response.
   */
⋮----
/**
The ID of the response model that was used to generate the response.
   */
⋮----
/**
Response headers.
   */</file><file path="packages/ai/core/types/transcription-model.ts">import {
  TranscriptionModelV1,
  TranscriptionModelV1CallWarning,
} from &apos;@ai-sdk/provider&apos;;
/**
Transcription model that is used by the AI SDK Core functions.
  */
export type TranscriptionModel = TranscriptionModelV1;
/**
Warning from the model provider for this call. The call will proceed, but e.g.
some settings might not be supported, which can lead to suboptimal results.
  */
export type TranscriptionWarning = TranscriptionModelV1CallWarning;</file><file path="packages/ai/core/types/usage.ts">/**
Represents the number of tokens used in a prompt and completion.
 */
export type LanguageModelUsage = {
  /**
The number of tokens used in the prompt.
   */
  promptTokens: number;
  /**
The number of tokens used in the completion.
 */
  completionTokens: number;
  /**
The total number of tokens used (promptTokens + completionTokens).
   */
  totalTokens: number;
};
⋮----
/**
The number of tokens used in the prompt.
   */
⋮----
/**
The number of tokens used in the completion.
 */
⋮----
/**
The total number of tokens used (promptTokens + completionTokens).
   */
⋮----
/**
Represents the number of tokens used in an embedding.
 */
export type EmbeddingModelUsage = {
  /**
The number of tokens used in the embedding.
   */
  tokens: number;
};
⋮----
/**
The number of tokens used in the embedding.
   */
⋮----
export function calculateLanguageModelUsage({
  promptTokens,
  completionTokens,
}: {
  promptTokens: number;
  completionTokens: number;
}): LanguageModelUsage
export function addLanguageModelUsage(
  usage1: LanguageModelUsage,
  usage2: LanguageModelUsage,
): LanguageModelUsage</file><file path="packages/ai/core/util/async-iterable-stream.test.ts">import {
  convertArrayToReadableStream,
  convertAsyncIterableToArray,
  convertReadableStreamToArray,
} from &apos;@ai-sdk/provider-utils/test&apos;;
import { describe, expect, it } from &apos;vitest&apos;;
import { createAsyncIterableStream } from &apos;./async-iterable-stream&apos;;</file><file path="packages/ai/core/util/async-iterable-stream.ts">export type AsyncIterableStream&lt;T&gt; = AsyncIterable&lt;T&gt; &amp; ReadableStream&lt;T&gt;;
export function createAsyncIterableStream&lt;T&gt;(
  source: ReadableStream&lt;T&gt;,
): AsyncIterableStream&lt;T&gt;
⋮----
async next(): Promise&lt;IteratorResult&lt;T&gt;&gt;</file><file path="packages/ai/core/util/cosine-similarity.test.ts">import { cosineSimilarity } from &apos;./cosine-similarity&apos;;
⋮----
// test against pre-calculated value:
⋮----
// test against pre-calculated value:</file><file path="packages/ai/core/util/cosine-similarity.ts">import { InvalidArgumentError } from &apos;../../errors/invalid-argument-error&apos;;
/**
 * Calculates the cosine similarity between two vectors. This is a useful metric for
 * comparing the similarity of two vectors such as embeddings.
 *
 * @param vector1 - The first vector.
 * @param vector2 - The second vector.
 * @param options - Optional configuration.
 * @param options.throwErrorForEmptyVectors - If true, throws an error for empty vectors. Default: false.
 *
 * @returns The cosine similarity between vector1 and vector2.
 * @returns 0 if either vector is the zero vector.
 *
 * @throws {InvalidArgumentError} If throwErrorForEmptyVectors is true and vectors are empty.
 * @throws {InvalidArgumentError} If the vectors do not have the same length.
 */
export function cosineSimilarity(
  vector1: number[],
  vector2: number[],
  // TODO remove throw option in 5.0
  options?: {
    /**
     * @deprecated will be removed in 5.0
     */
    throwErrorForEmptyVectors?: boolean;
  },
): number
⋮----
// TODO remove throw option in 5.0
⋮----
/**
     * @deprecated will be removed in 5.0
     */
⋮----
return 0; // Return 0 for empty vectors if no error is thrown</file><file path="packages/ai/core/util/create-stitchable-stream.test.ts">import {
  convertArrayToReadableStream,
  convertReadableStreamToArray,
} from &apos;@ai-sdk/provider-utils/test&apos;;
import { expect } from &apos;vitest&apos;;
import { createStitchableStream } from &apos;./create-stitchable-stream&apos;;
⋮----
// Start reading before any values are added
⋮----
// Add value with delay after starting read
⋮----
// Value should be returned once available
⋮----
// Stream should complete after value is read
⋮----
// read 5 values from the stream before they are added
// (added asynchronously)
⋮----
// wait for the stream to finish via await:
⋮----
start(controller)
⋮----
cancel()
⋮----
// Start reading from the stream
⋮----
// Should immediately close without reading remaining values</file><file path="packages/ai/core/util/create-stitchable-stream.ts">import { createResolvablePromise } from &apos;../../util/create-resolvable-promise&apos;;
/**
 * Creates a stitchable stream that can pipe one stream at a time.
 *
 * @template T - The type of values emitted by the streams.
 * @returns {Object} An object containing the stitchable stream and control methods.
 */
export function createStitchableStream&lt;T&gt;():
⋮----
const processPull = async () =&gt;
⋮----
// Case 1: Outer stream is closed and no more inner streams
⋮----
// Case 2: No inner streams available, but outer stream is open
// wait for a new inner stream to be added or the outer stream to close
⋮----
// Case 3: Current inner stream is done
innerStreamReaders.shift(); // Remove the finished stream
// Continue pulling from the next stream if available
⋮----
// Case 4: Current inner stream returns an item
⋮----
// Case 5: Current inner stream throws an error
⋮----
innerStreamReaders.shift(); // Remove the errored stream
⋮----
start(controllerParam)
⋮----
async cancel()
⋮----
/**
     * Gracefully close the outer stream. This will let the inner streams
     * finish processing and then close the outer stream.
     */
⋮----
/**
     * Immediately close the outer stream. This will cancel all inner streams
     * and close the outer stream.
     */</file><file path="packages/ai/core/util/detect-mimetype.test.ts">import { describe, it, expect } from &apos;vitest&apos;;
import {
  detectMimeType,
  imageMimeTypeSignatures,
  audioMimeTypeSignatures,
} from &apos;./detect-mimetype&apos;;
import { convertUint8ArrayToBase64 } from &apos;@ai-sdk/provider-utils&apos;;
⋮----
const gifBase64 = &apos;R0lGabc123&apos;; // Base64 string starting with GIF signature
⋮----
const pngBase64 = &apos;iVBORwabc123&apos;; // Base64 string starting with PNG signature
⋮----
const jpegBase64 = &apos;/9j/abc123&apos;; // Base64 string starting with JPEG signature
⋮----
const webpBase64 = &apos;UklGRgabc123&apos;; // Base64 string starting with WebP signature
⋮----
const bmpBase64 = &apos;Qkabc123&apos;; // Base64 string starting with BMP signature
⋮----
const tiffLEBase64 = &apos;SUkqAAabc123&apos;; // Base64 string starting with TIFF LE signature
⋮----
const tiffBEBase64 = &apos;TU0AKgabc123&apos;; // Base64 string starting with TIFF BE signature
⋮----
const avifBase64 = &apos;AAAAIGZ0eXBhdmlmabc123&apos;; // Base64 string starting with AVIF signature
⋮----
const heicBase64 = &apos;AAAAIGZ0eXBoZWljabc123&apos;; // Base64 string starting with HEIC signature
⋮----
const mp3Base64 = &apos;//s=&apos;; // Base64 string starting with MP3 signature
⋮----
0x33, // &apos;ID3&apos;
⋮----
0x00, // version
0x00, // flags
⋮----
0x0a, // size (10 bytes)
// 10 bytes of ID3 data
⋮----
// MP3 frame header
⋮----
0x33, // &apos;ID3&apos;
⋮----
0x00, // version
0x00, // flags
⋮----
0x0a, // size (10 bytes)
// 10 bytes of ID3 data
⋮----
// MP3 frame header
⋮----
const wavBase64 = &apos;UklGRiQ=&apos;; // Base64 string starting with WAV signature
⋮----
const oggBase64 = &apos;T2dnUw&apos;; // Base64 string starting with OGG signature
⋮----
const flacBase64 = &apos;ZkxhQw&apos;; // Base64 string starting with FLAC signature
⋮----
const aacBase64 = &apos;QBUA&apos;; // Base64 string starting with AAC signature
⋮----
const mp4Base64 = &apos;ZnR5cA&apos;; // Base64 string starting with MP4 signature
⋮----
const shortBytes = new Uint8Array([0x89, 0x50]); // Incomplete PNG signature
⋮----
const shortBytes = new Uint8Array([0x4f, 0x67]); // Incomplete OGG signature</file><file path="packages/ai/core/util/detect-mimetype.ts">import { convertBase64ToUint8Array } from &apos;@ai-sdk/provider-utils&apos;;
⋮----
const stripID3 = (data: Uint8Array | string) =&gt;
⋮----
// The raw MP3 starts here
⋮----
function stripID3TagsIfPresent(data: Uint8Array | string): Uint8Array | string
⋮----
data[0] === 0x49 &amp;&amp; // &apos;I&apos;
data[1] === 0x44 &amp;&amp; // &apos;D&apos;
data[2] === 0x33); // &apos;3&apos;
⋮----
export function detectMimeType({
  data,
  signatures,
}: {
  data: Uint8Array | string;
  signatures: typeof audioMimeTypeSignatures | typeof imageMimeTypeSignatures;
}): (typeof signatures)[number][&apos;mimeType&apos;] | undefined</file><file path="packages/ai/core/util/get-potential-start-index.test.ts">import { getPotentialStartIndex } from &apos;./get-potential-start-index&apos;;</file><file path="packages/ai/core/util/get-potential-start-index.ts">/**
 * Returns the index of the start of the searchedText in the text, or null if it
 * is not found.
 */
export function getPotentialStartIndex(
  text: string,
  searchedText: string,
): number | null
⋮----
// Return null immediately if searchedText is empty.
⋮----
// Check if the searchedText exists as a direct substring of text.
⋮----
// Otherwise, look for the largest suffix of &quot;text&quot; that matches
// a prefix of &quot;searchedText&quot;. We go from the end of text inward.</file><file path="packages/ai/core/util/is-non-empty-object.ts">export function isNonEmptyObject(
  object: Record&lt;string, unknown&gt; | undefined | null,
): object is Record&lt;string, unknown&gt;</file><file path="packages/ai/core/util/merge-objects.test.ts">import { describe, it, expect } from &apos;vitest&apos;;
import { mergeObjects } from &apos;./merge-objects&apos;;
⋮----
// Original objects should not be modified
⋮----
// Both inputs undefined
⋮----
// One input undefined</file><file path="packages/ai/core/util/merge-objects.ts">/**
 * Deeply merges two objects together.
 * - Properties from the second object override those in the first object with the same key
 * - For nested objects, the merge is performed recursively (deep merge)
 * - Arrays are replaced, not merged
 * - Primitive values are replaced
 * - If both inputs are undefined, returns undefined
 * - If one input is undefined, returns the other
 *
 * @param target The target object to merge into
 * @param source The source object to merge from
 * @returns A new object with the merged properties, or undefined if both inputs are undefined
 */
export function mergeObjects&lt;T extends object, U extends object&gt;(
  target: T | undefined,
  source: U | undefined,
): (T &amp; U) | T | U | undefined
⋮----
// If both inputs are undefined, return undefined
⋮----
// If target is undefined, return source
⋮----
// If source is undefined, return target
⋮----
// Create a new object to avoid mutating the inputs
⋮----
// Iterate through all keys in the source object
⋮----
// Skip if the source value is undefined
⋮----
// Get the target value if it exists
⋮----
// Check if both values are objects that can be deeply merged
⋮----
// If both values are mergeable objects, merge them recursively
⋮----
// For primitives, arrays, or when one value is not a mergeable object,
// simply override with the source value</file><file path="packages/ai/core/util/merge-streams.test.ts">import {
  convertArrayToReadableStream,
  convertReadableStreamToArray,
} from &apos;@ai-sdk/provider-utils/test&apos;;
import { expect, it } from &apos;vitest&apos;;
import { mergeStreams } from &apos;./merge-streams&apos;;
⋮----
start(controller)
⋮----
async function pull()
⋮----
stream2Controller!.enqueue(&apos;2d&apos;); // comes later
⋮----
stream2Controller!.enqueue(&apos;2e&apos;); // comes later</file><file path="packages/ai/core/util/merge-streams.ts">/**
 * Merges two readable streams into a single readable stream, emitting values
 * from each stream as they become available.
 *
 * The first stream is prioritized over the second stream. If both streams have
 * values available, the first stream&apos;s value is emitted first.
 *
 * @template VALUE1 - The type of values emitted by the first stream.
 * @template VALUE2 - The type of values emitted by the second stream.
 * @param {ReadableStream&lt;VALUE1&gt;} stream1 - The first readable stream.
 * @param {ReadableStream&lt;VALUE2&gt;} stream2 - The second readable stream.
 * @returns {ReadableStream&lt;VALUE1 | VALUE2&gt;} A new readable stream that emits values from both input streams.
 */
export function mergeStreams&lt;VALUE1, VALUE2&gt;(
  stream1: ReadableStream&lt;VALUE1&gt;,
  stream2: ReadableStream&lt;VALUE2&gt;,
): ReadableStream&lt;VALUE1 | VALUE2&gt;
⋮----
// only use when stream 2 is done:
async function readStream1(
    controller: ReadableStreamDefaultController&lt;VALUE1 | VALUE2&gt;,
)
// only use when stream 1 is done:
async function readStream2(
    controller: ReadableStreamDefaultController&lt;VALUE1 | VALUE2&gt;,
)
⋮----
async pull(controller)
⋮----
// stream 1 is done, we can only read from stream 2:
⋮----
// stream 2 is done, we can only read from stream 1:
⋮----
// pull the next value from the stream that was read last:
⋮----
// Note on Promise.race (prioritizing stream 1 over stream 2):
// If the iterable contains one or more non-promise values and/or an already settled promise,
// then Promise.race() will settle to the first of these values found in the iterable.
⋮----
// stream 1 is done, we can only read from stream 2:
⋮----
// stream 2 is done, we can only read from stream 1:
⋮----
cancel()</file><file path="packages/ai/core/util/now.ts">// Shim for performance.now() to support environments that don&apos;t have it:
export function now(): number</file><file path="packages/ai/core/util/prepare-outgoing-http-headers.test.ts">import { prepareOutgoingHttpHeaders } from &apos;./prepare-outgoing-http-headers&apos;;</file><file path="packages/ai/core/util/prepare-outgoing-http-headers.ts">export function prepareOutgoingHttpHeaders(
  headers: HeadersInit | undefined,
  {
    contentType,
    dataStreamVersion,
  }: { contentType: string; dataStreamVersion?: &apos;v1&apos; | undefined },
)</file><file path="packages/ai/core/util/prepare-response-headers.test.ts">import { prepareResponseHeaders } from &apos;./prepare-response-headers&apos;;</file><file path="packages/ai/core/util/prepare-response-headers.ts">export function prepareResponseHeaders(
  headers: HeadersInit | undefined,
  {
    contentType,
    dataStreamVersion,
  }: { contentType: string; dataStreamVersion?: &apos;v1&apos; | undefined },
)</file><file path="packages/ai/core/util/remove-text-after-last-whitespace.test.ts">import { removeTextAfterLastWhitespace } from &apos;./remove-text-after-last-whitespace&apos;;</file><file path="packages/ai/core/util/remove-text-after-last-whitespace.ts">import { splitOnLastWhitespace } from &apos;./split-on-last-whitespace&apos;;
export function removeTextAfterLastWhitespace(text: string): string</file><file path="packages/ai/core/util/simulate-readable-stream.test.ts">import { simulateReadableStream } from &apos;./simulate-readable-stream&apos;;
import { convertReadableStreamToArray } from &apos;@ai-sdk/provider-utils/test&apos;;
⋮----
const mockDelay = (ms: number | null) =&gt;
⋮----
await convertReadableStreamToArray(stream); // consume stream
⋮----
await convertReadableStreamToArray(stream); // consume stream
⋮----
await convertReadableStreamToArray(stream); // consume stream
⋮----
await convertReadableStreamToArray(stream); // consume stream</file><file path="packages/ai/core/util/simulate-readable-stream.ts">import { delay as delayFunction } from &apos;@ai-sdk/provider-utils&apos;;
/**
 * Creates a ReadableStream that emits the provided values with an optional delay between each value.
 *
 * @param options - The configuration options
 * @param options.chunks - Array of values to be emitted by the stream
 * @param options.initialDelayInMs - Optional initial delay in milliseconds before emitting the first value (default: 0). Can be set to `null` to skip the initial delay. The difference between `initialDelayInMs: null` and `initialDelayInMs: 0` is that `initialDelayInMs: null` will emit the values without any delay, while `initialDelayInMs: 0` will emit the values with a delay of 0 milliseconds.
 * @param options.chunkDelayInMs - Optional delay in milliseconds between emitting each value (default: 0). Can be set to `null` to skip the delay. The difference between `chunkDelayInMs: null` and `chunkDelayInMs: 0` is that `chunkDelayInMs: null` will emit the values without any delay, while `chunkDelayInMs: 0` will emit the values with a delay of 0 milliseconds.
 * @returns A ReadableStream that emits the provided values
 */
export function simulateReadableStream&lt;T&gt;({
  chunks,
  initialDelayInMs = 0,
  chunkDelayInMs = 0,
  _internal,
}: {
  chunks: T[];
  initialDelayInMs?: number | null;
  chunkDelayInMs?: number | null;
  _internal?: {
delay?: (ms: number | null)
⋮----
async pull(controller)</file><file path="packages/ai/core/util/split-array.test.ts">import { expect, it } from &apos;vitest&apos;;
import { splitArray } from &apos;./split-array&apos;;</file><file path="packages/ai/core/util/split-array.ts">/**
 * Splits an array into chunks of a specified size.
 *
 * @template T - The type of elements in the array.
 * @param {T[]} array - The array to split.
 * @param {number} chunkSize - The size of each chunk.
 * @returns {T[][]} - A new array containing the chunks.
 */
export function splitArray&lt;T&gt;(array: T[], chunkSize: number): T[][]</file><file path="packages/ai/core/util/split-on-last-whitespace.test.ts">import { splitOnLastWhitespace } from &apos;./split-on-last-whitespace&apos;;</file><file path="packages/ai/core/util/split-on-last-whitespace.ts">/**
 * Splits the text on the last whitespace.
 *
 * Whitespace is defined as one or more whitespace characters,
 * e.g. space, tab, newline, etc.
 *
 * @param text - The text to split.
 * @returns The prefix, whitespace, and suffix. Undefined if there is no whitespace.
 */
export function splitOnLastWhitespace(text: string):
  | {
      prefix: string;
      whitespace: string;
      suffix: string;
    }
  | undefined {
  const match = text.match(lastWhitespaceRegexp);</file><file path="packages/ai/core/util/value-of.ts">// License for this File only:
//
// MIT License
//
// Copyright (c) Sindre Sorhus &lt;sindresorhus@gmail.com&gt; (https://sindresorhus.com)
// Copyright (c) Vercel, Inc. (https://vercel.com)
//
// Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
// documentation files (the &quot;Software&quot;), to deal in the Software without restriction, including without limitation
// the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
// to permit persons to whom the Software is furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all copies or substantial portions
// of the Software.
//
// THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
// TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
// THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
// CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
// IN THE SOFTWARE.
/**
Create a union of the given object&apos;s values, and optionally specify which keys to get the values from.
Please upvote [this issue](https://github.com/microsoft/TypeScript/issues/31438) if you want to have this type as a built-in in TypeScript.
@example
```
// data.json
{
	&apos;foo&apos;: 1,
	&apos;bar&apos;: 2,
	&apos;biz&apos;: 3
}
// main.ts
import type {ValueOf} from &apos;type-fest&apos;;
import data = require(&apos;./data.json&apos;);
export function getData(name: string): ValueOf&lt;typeof data&gt; {
	return data[name];
}
export function onlyBar(name: string): ValueOf&lt;typeof data, &apos;bar&apos;&gt; {
	return data[name];
}
// file.ts
import {getData, onlyBar} from &apos;./main&apos;;
getData(&apos;foo&apos;);
//=&gt; 1
onlyBar(&apos;foo&apos;);
//=&gt; TypeError ...
onlyBar(&apos;bar&apos;);
//=&gt; 2
```
* @see https://github.com/sindresorhus/type-fest/blob/main/source/value-of.d.ts
*/
export type ValueOf&lt;
  ObjectType,
  ValueType extends keyof ObjectType = keyof ObjectType,
&gt; = ObjectType[ValueType];</file><file path="packages/ai/core/util/write-to-server-response.ts">import { ServerResponse } from &apos;node:http&apos;;
/**
 * Writes the content of a stream to a server response.
 */
export function writeToServerResponse({
  response,
  status,
  statusText,
  headers,
  stream,
}: {
  response: ServerResponse;
  status?: number;
  statusText?: string;
  headers?: Record&lt;string, string | number | string[]&gt;;
  stream: ReadableStream&lt;Uint8Array&gt;;
}): void
⋮----
const read = async () =&gt;</file><file path="packages/ai/core/index.ts">// re-exports:
⋮----
// directory exports:
⋮----
// telemetry types:
⋮----
// util exports:</file></files></repomix>